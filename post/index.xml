<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Kunal Kushwaha</title>
    <link>http://kunalkushwaha.github.io/post/</link>
    <description>Recent content in Posts on Kunal Kushwaha</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Kunal Kushwaha</copyright>
    <lastBuildDate>Fri, 14 Oct 2016 14:05:58 +0900</lastBuildDate>
    <atom:link href="http://kunalkushwaha.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Understanding Infrakit</title>
      <link>http://kunalkushwaha.github.io/2016/10/14/understanding-infrakit/</link>
      <pubDate>Fri, 14 Oct 2016 14:05:58 +0900</pubDate>
      
      <guid>http://kunalkushwaha.github.io/2016/10/14/understanding-infrakit/</guid>
      <description>

&lt;p&gt;Docker recently open sourced new project &lt;a href=&#34;https://github.com/docker/infrakit&#34;&gt;Infrakit&lt;/a&gt; at ContainerCon 2016, Berlin.&lt;/p&gt;

&lt;p&gt;As project describes itself as&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;A toolkit for creating and managing declarative, self-healing infrastructure&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Lets try to understand what it does and how it adds value to existing tools.&lt;/p&gt;

&lt;h3 id=&#34;infrakit:688c9f714106adece8c5c499d991286e&#34;&gt;Infrakit&lt;/h3&gt;

&lt;h5 id=&#34;problem:688c9f714106adece8c5c499d991286e&#34;&gt;Problem&lt;/h5&gt;

&lt;p&gt;Managing docker on different infrastructure is difficult and not portable.&lt;/p&gt;

&lt;h4 id=&#34;history:688c9f714106adece8c5c499d991286e&#34;&gt;History&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Initially Docker machine project was focused for building docker environment on various platforms, but there were lot of issues mainly due to unreliable third party drivers.

&lt;ul&gt;
&lt;li&gt;User experience was not great on developer platforms like Mac (VirtualBox issues)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Idea of declarative infrastructure was thought in Docker Machine Project also.&lt;/li&gt;
&lt;li&gt;Few &lt;a href=&#34;https://github.com/docker/machine/issues/773&#34;&gt;Proposals&lt;/a&gt; and &lt;a href=&#34;https://github.com/docker/machine/pull/2422&#34;&gt;PR were made for declarative infrastructure&lt;/a&gt; but never got merged due to user experience issues and Docker Inc&amp;rsquo;s long term Roadmap vision.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;NOTE:Docker Machine is still very much active project with great userbase.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Later during &lt;a href=&#34;http://2016.dockercon.com/&#34;&gt;Dockercon16&lt;/a&gt;, while interacting with docker developers &amp;amp; maintainer of Docker Machine, I understood, long term plan is to merge docker machine capabilities into docker engine.&lt;/li&gt;
&lt;li&gt;Meanwhile Docker had already released &lt;a href=&#34;https://docs.docker.com/engine/installation/mac/&#34;&gt;Docker For Mac&lt;/a&gt;. It is amazing product, as it just works and upgrade itself for new releases too.&lt;/li&gt;
&lt;li&gt;Docker for AWS/Azure were announced in &lt;a href=&#34;http://2016.dockercon.com/&#34;&gt;Dockercon16&lt;/a&gt;. Infrakit is born out of these projects.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;overview:688c9f714106adece8c5c499d991286e&#34;&gt;Overview&lt;/h4&gt;

&lt;p&gt;Unlike most of docker projects, Infrakit is not about containers, it is about preparing &amp;amp; managing Infrastructure for distributed computing.
Infrakit by design is build of active process(a.k.a. plugins) which collaborate with each other to analyze and take action to bring infrastructure in desired state. As other plugins in Docker Projects, Infrakit plugins also communicate over unix sockets and use HTTP protocol. This makes plugin implementation language agnostic and can be deployed separately as containers.&lt;/p&gt;

&lt;p&gt;These Plugins are also building block of Infrakit to manage infrastructure.&lt;/p&gt;

&lt;p&gt;Currently there are three kind of plugins supported by Infrakit which represents different layer of abstraction representing whole stack of infrastructure.
Lets go through each and try to understand their roles.&lt;/p&gt;

&lt;h4 id=&#34;groups:688c9f714106adece8c5c499d991286e&#34;&gt;Groups&lt;/h4&gt;

&lt;p&gt;Groups plugins are for managing cluster of machine. These clusters can be represented by two set of machines.
- Cattle or Replica : Machines with identical configuration.
- Pets or Quorum :  Machine with slightly different properties like Ordering and Identity.&lt;/p&gt;

&lt;p&gt;The size of group can be increased and decreased. While changing configuration of machines, rolling updates by swarmkit ensures, no downtime and smooth up-gradation.
Following actions can be done on group plugin.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;watch/ unwatch a group (start / stop managing a group)&lt;/li&gt;
&lt;li&gt;inspect a group&lt;/li&gt;
&lt;li&gt;trigger an update the configuration of a group - like changing its size or underlying properties of instances.&lt;/li&gt;
&lt;li&gt;stop an update&lt;/li&gt;
&lt;li&gt;destroy a group&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;instance:688c9f714106adece8c5c499d991286e&#34;&gt;Instance&lt;/h4&gt;

&lt;p&gt;Instance plugin represent, one instance of machine can be Cloud instance or just a VM. These Cloud instance or VMs have same specs.
Instances define spec of VM or Cloud instance and also we can associate some tags to identify etc.&lt;/p&gt;

&lt;h4 id=&#34;flavors:688c9f714106adece8c5c499d991286e&#34;&gt;Flavors&lt;/h4&gt;

&lt;p&gt;Flavor plugin give flavor to each instance. i.e. What exactly should one instance should contain and running. e.g. setting up software packages etc.&lt;/p&gt;

&lt;p&gt;With these plugins all kind of infrastructure from machine instance point of view can be configured.&lt;/p&gt;

&lt;p&gt;I won&amp;rsquo;t go into implementation details in this blog, but if you want to play with Infrakit, they have very good introductory  &lt;a href=&#34;https://github.com/docker/infrakit/blob/master/docs/tutorial.md&#34;&gt;tutorial&lt;/a&gt;. You must try.&lt;/p&gt;

&lt;h3 id=&#34;how-infrakit-is-different-from-existing-products-and-solutions-available:688c9f714106adece8c5c499d991286e&#34;&gt;How Infrakit is different from existing products and solutions available?&lt;/h3&gt;

&lt;h5 id=&#34;configuration-management-softwares-puppet-chef-etc:688c9f714106adece8c5c499d991286e&#34;&gt;Configuration Management softwares(Puppet/Chef etc).&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;All configuration management software works, once machine are up and running. They do not have capability to create and destroy the computing instances.&lt;/li&gt;
&lt;li&gt;Infrakit&amp;rsquo;s itself don&amp;rsquo;t have any defined syntax for configuring such as Puppet/Chef. So Puppet/Chef could to be used in Infakit&amp;rsquo;s flavor plugins, where computing node is configured.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;terraform-cloudformation-heat:688c9f714106adece8c5c499d991286e&#34;&gt;Terraform/CloudFormation/Heat.&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;Infrakit is similar to these, in terms of it also works on configuration plus state reconciliation feature i.e. Infrakit not only builds the infrastructure but also monitors them for changes and restore to expected state. i.e. If some instances in cluster stops, it will create new instances.&lt;/li&gt;
&lt;li&gt;Terraform/CloudFormation/Heat can be used as Instance Plugins. Example of &lt;a href=&#34;https://github.com/docker/infrakit/tree/master/example/instance/terraform&#34;&gt;Terraform plugin&lt;/a&gt; is provided by Infrakit&lt;/li&gt;
&lt;li&gt;Also, Infrakit is not polished product like Terraform, it is a framework which can be used by any existing/new product, to manage different infrastructure is similar fashion.
It abstracts the details of different infrastructure&amp;rsquo;s to end user.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;final-notes:688c9f714106adece8c5c499d991286e&#34;&gt;Final Notes&lt;/h4&gt;

&lt;p&gt;Infrakit is amazing piece of code &amp;amp; concept for build self healing infrastructure, like Swarmkit to building distributed softwares. Though it is very early in development, so community contribution in ideas and code may bring more changes(in good ways) in Project.&lt;/p&gt;

&lt;p&gt;Issues like preparing docker platform with third party services/framework enabled for Storage/Networking/Security plugins should be far more smoother and I am looking forward to work on same.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Migrating traditional apps to docker</title>
      <link>http://kunalkushwaha.github.io/2016/08/12/migrating-traditional-apps-to-docker/</link>
      <pubDate>Fri, 12 Aug 2016 17:10:24 +0900</pubDate>
      
      <guid>http://kunalkushwaha.github.io/2016/08/12/migrating-traditional-apps-to-docker/</guid>
      <description>

&lt;p&gt;If title of this blog, attracted you towards this blog, Most likely, you will be one of us, who want to migrate old traditional application&amp;rsquo;s in container environment. This blog covers my experience of Migrating old traditional application into Docker.&lt;/p&gt;

&lt;p&gt;&lt;img align=&#34;center&#34; src=http://kunalkushwaha.github.io/LegacyMigration.png&gt;&lt;/p&gt;

&lt;p&gt;Before beginning with migration procedure, Let me explain requirements I had to met with migrated system.&lt;/p&gt;

&lt;h3 id=&#34;problems:3b2a0f0135cb172f18bec86ce1483615&#34;&gt;Problems&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Old apps running on servers/VMs, resource utilization not optimal.&lt;/li&gt;
&lt;li&gt;High maintenance cost.

&lt;ul&gt;
&lt;li&gt;OS running is no longer supported by vendors, so in-house support required.&lt;/li&gt;
&lt;li&gt;Maintenance downtime high.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Platform to adopt DevOps practice for fast refactoring.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;challenges:3b2a0f0135cb172f18bec86ce1483615&#34;&gt;Challenges&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Migrate Application with least changes in existing platform and infrastructure use.

&lt;ul&gt;
&lt;li&gt;Use same OS distribution.&lt;/li&gt;
&lt;li&gt;No rewriting application for new OS.&lt;/li&gt;
&lt;li&gt;No changes in network infrastructure.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Behavioral changes for accessing the application should be none.

&lt;ul&gt;
&lt;li&gt;Use hardware load balancer.&lt;/li&gt;
&lt;li&gt;Use Fixed-IP with pre-approved MAC address.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;solution:3b2a0f0135cb172f18bec86ce1483615&#34;&gt;Solution&lt;/h3&gt;

&lt;p&gt;Migrating old applications into containers can be divided into two parts.
1. Application code/binary migration into image.
2. Application deployment, as per requirements.&lt;/p&gt;

&lt;p&gt;The first part is specific to each application, as every application have different challenges for migration.
I will not discuss details of that. Though few of things need be considered before migrating.&lt;/p&gt;

&lt;h4 id=&#34;docker-host:3b2a0f0135cb172f18bec86ce1483615&#34;&gt;Docker Host&lt;/h4&gt;

&lt;p&gt;Which OS/distribution to be chosen for docker host.
Ideally any distribution which have LTS should be fine.&lt;/p&gt;

&lt;p&gt;Though, sometime you may need to consider alternate from latest due compatibility with application.
To check application compatibility, follow the below Checklist.&lt;/p&gt;

&lt;h4 id=&#34;application-compatibility-checklist:3b2a0f0135cb172f18bec86ce1483615&#34;&gt;Application compatibility Checklist&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Check if any changes in ABI for linux kernel functionality and libraries used by application.

&lt;ul&gt;
&lt;li&gt;If ABI breaks, either we need to choose old kernel, where you may need to work little bit to make docker work properly.&lt;/li&gt;
&lt;li&gt;OR, recompile application with new libraries.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Any special Kernel settings required by application.

&lt;ul&gt;
&lt;li&gt;Host kernel parameters may need to be tweaked and/or cgroups/namespaces need to be adjusted accordingly.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After detailed analysis, we can decide and configure Docker Host.&lt;/p&gt;

&lt;h4 id=&#34;application-migration:3b2a0f0135cb172f18bec86ce1483615&#34;&gt;Application migration&lt;/h4&gt;

&lt;p&gt;Application migration in my case is simple. Just containerize the whole application.
i.e. Build an image/rootfs with old libraries and application code. Typically old application requires more then one process to run, using &lt;a href=&#34;https://docs.docker.com/engine/admin/using_supervisord/&#34;&gt;Supervisord&lt;/a&gt;, it can be achieved.&lt;/p&gt;

&lt;p&gt;Now once, you successfully able build &amp;amp; test image for application, Lets focus on how it can be deployed with least changes to current setup.&lt;/p&gt;

&lt;h3 id=&#34;deployment:3b2a0f0135cb172f18bec86ce1483615&#34;&gt;Deployment&lt;/h3&gt;

&lt;p&gt;As started in challenges section, we are using hardware load-balancer and want to continue the same.
Also, this application is bind to fixed-IP and specific MAC address.&lt;/p&gt;

&lt;p&gt;Since docker supports &lt;a href=&#34;https://github.com/docker/libnetwork/blob/master/docs/macvlan.md&#34;&gt;macvlan&lt;/a&gt; network driver, the above two goals can be easily achieved.
Using macvlan driver, we can put a container on same network as old servers. Also macvlan user less CPU and slightly better &lt;a href=&#34;http://events.linuxfoundation.org/sites/events/files/slides/LinuxConJapan2014_makita_0.pdf&#34;&gt;throughput&lt;/a&gt; then bridge network.&lt;/p&gt;

&lt;p&gt;Lets go through its details.&lt;/p&gt;

&lt;p&gt;On docker host, lets create a macvlan network.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker network create -d macvlan --subnet=172.20.20.0/18 --gateway=172.20.0.1 -o parent=enp1s0 -o macvlan_mode=bridge maclan
798b8a2ee7988dc0388c9985eaa7f0bc9373f11cf1be4a3b3a44abee162442fd


$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
9a9ff4851e71        bridge              bridge              local               
ddbc211bcf8f        docker_gwbridge     bridge              local               
a7211f1ae74d        host                host                local               
8aetfb2dtwmu        ingress             overlay             swarm               
798b8a2ee798        maclan              macvlan             local               
d43c2f84509b        none                null                local               
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;NOTE: macvlan network is local to host, so this cannot be used over swarm.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now, I need to create an network interfaces for my containers. I need to deploy two containers as replacement of two Application servers.
For security reason, I need to assign predefined MAC address to this interface. But with macvlan driver this is also possible.&lt;/p&gt;

&lt;p&gt;Now my network is ready, I can deploy my applications on same.
Lets create containers on these network.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --net=maclan --ip=172.20.20.51 --mac-address 08:00:27:0B:1C:FE -itd alpine sh                                                                                                     
1f0307c12ea7a666bffc4224665f304aae67056a7cbc6f037ec19d73b5a8a64d

$ docker run --net=maclan --ip=172.20.20.52 --mac-address 08:00:27:3F:FE:8E -itd alpine sh                                                                                                     
72849de4f1e23b2bd2394ef579c48045260b4f8f0026a601c5ac93514bc90e0d

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
72849de4f1e2        alpine              &amp;quot;sh&amp;quot;                5 seconds ago       Up 2 seconds                            grave_pike
1f0307c12ea7        alpine              &amp;quot;sh&amp;quot;                49 seconds ago      Up 46 seconds                           elegant_yalow
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Test it and containers are reachable from other machine on network.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ping 172.20.20.51
PING 172.20.20.51 (172.20.20.51): 56 data bytes
64 bytes from 172.20.20.51: seq=0 ttl=64 time=0.090 ms
64 bytes from 172.20.20.51: seq=1 ttl=64 time=0.057 ms
64 bytes from 172.20.20.51: seq=2 ttl=64 time=0.058 ms
^C
--- 172.20.20.51 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 0.057/0.068/0.090 ms

$ ping 172.20.20.52
PING 172.20.20.52 (172.20.20.52): 56 data bytes
64 bytes from 172.20.20.52: seq=0 ttl=64 time=0.075 ms
64 bytes from 172.20.20.52: seq=1 ttl=64 time=0.058 ms
64 bytes from 172.20.20.52: seq=2 ttl=64 time=0.066 ms
^C
--- 172.20.20.52 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 0.058/0.066/0.075 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now these containers have successfully replaced the two app servers, and for external load balancer, there is no changes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;NOTE: Alternative to macvlan is IPVlan, but since it is not yet stable in docker, I choose macvlan&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So what are the Benefits, I finally achieved through this migration.&lt;/p&gt;

&lt;h3 id=&#34;benefits:3b2a0f0135cb172f18bec86ce1483615&#34;&gt;Benefits&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Portability: Application,can be migrated to different machines easily.&lt;/li&gt;
&lt;li&gt;Flexibility: Image can be shared and experiment within team.&lt;/li&gt;
&lt;li&gt;Modularity: Provides a base to break application into further smaller services.&lt;/li&gt;
&lt;li&gt;Reuse: Reuse current infrastructure like network, load balancer. Least disturbance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hope this blog will help you to find some answers, if you are looking for migrating legacy/old/monolithic applications to docker.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>docker 1.12RC demo</title>
      <link>http://kunalkushwaha.github.io/2016/07/25/docker-1.12rc-demo/</link>
      <pubDate>Mon, 25 Jul 2016 13:39:38 +0900</pubDate>
      
      <guid>http://kunalkushwaha.github.io/2016/07/25/docker-1.12rc-demo/</guid>
      <description>

&lt;p&gt;Last friday (22nd July), I gave DockerCon16 Recap Talk and demo at &lt;a href=&#34;https://www.meetup.com/Docker-Tokyo/events/231737938/&#34;&gt;Docker Meetup Tokyo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This blog post gives a walk through my Docker Meetup demonstration, and you can follow these steps to try at your machines.&lt;/p&gt;

&lt;p&gt;In DockerCon16, docker 1.12 RC was announced. This release has an important &amp;amp; interesting feature called docker &lt;em&gt;swarm mode&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;get-docker-1-12-rc:6855b0581c631fde987ca5dc2ca3bf28&#34;&gt;Get Docker 1.12 RC.&lt;/h3&gt;

&lt;p&gt;If you are using &lt;a href=&#34;https://docs.docker.com/engine/installation/mac/#/docker-for-mac&#34;&gt;Docker for Mac&lt;/a&gt; or &lt;a href=&#34;https://docs.docker.com/engine/installation/windows/#/docker-for-windows&#34;&gt;Docker for Windows&lt;/a&gt;, You already have docker 1.12RC installed on your machine.&lt;/p&gt;

&lt;p&gt;You can check version&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker version
Client:
 Version:      1.12.0-rc4
 API version:  1.24
 Go version:   go1.6.2
 Git commit:   e4a0dbc
 Built:        Wed Jul 13 03:28:51 2016
 OS/Arch:      darwin/amd64
 Experimental: true

Server:
 Version:      1.12.0-rc4
 API version:  1.24
 Go version:   go1.6.2
 Git commit:   e4a0dbc
 Built:        Wed Jul 13 03:28:51 2016
 OS/Arch:      linux/amd64
 Experimental: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In case you are using linux or not using above, you can get latest releases from &lt;a href=&#34;https://github.com/docker/docker/releases&#34;&gt;https://github.com/docker/docker/releases&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;create-demo-docker-hosts:6855b0581c631fde987ca5dc2ca3bf28&#34;&gt;Create demo docker hosts.&lt;/h3&gt;

&lt;p&gt;To create a swarm of docker hosts, you require more then one docker-host. I use &lt;a href=&#34;https://docs.docker.com/machine/&#34;&gt;docker-machine&lt;/a&gt; to create my docker hosts.&lt;/p&gt;

&lt;p&gt;With following commands, my three docker hosts will be ready with docker 1.12 RC installed with default &lt;a href=&#34;http://boot2docker.io/&#34;&gt;boot2docker&lt;/a&gt; image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$docker-machine -d virtualbox manager

$docker-machine -d virtualbox worker1

$docker-machine -d virtualbox worker2

$ docker-machine ls
NAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER        ERRORS
manager   *        virtualbox   Running   tcp://192.168.99.100:2376           v1.12.0-rc4
worker1   -        virtualbox   Running   tcp://192.168.99.101:2376           v1.12.0-rc4
worker2   -        virtualbox   Running   tcp://192.168.99.102:2376           v1.12.0-rc4

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Note: If you work behind proxy, use command as &lt;code&gt;docker-machine -d virtualbox -engine-env http_proxy=&amp;quot;example.com:port&amp;quot; manager&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Once, all machines are up and running, we can create swarm.&lt;/p&gt;

&lt;h3 id=&#34;initialize-swarm:6855b0581c631fde987ca5dc2ca3bf28&#34;&gt;Initialize swarm.&lt;/h3&gt;

&lt;p&gt;To run command on my &lt;em&gt;manager&lt;/em&gt; node, I need to set my environment variables, so all docker commands gets executed at manager.
This can be achieved by single command as follows.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval $(docker-machine env manager)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now initialize docker swarm mode.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker swarm init --listen-addr $(docker-machine ip manager):2377
No --secret provided. Generated random secret:
	aj92ivakk282slwal0ujwaloj

Swarm initialized: current node (2ksrqgk2x3vbjh3bw0aly5dwr) is now a manager.

To add a worker to this swarm, run the following command:
	docker swarm join --secret aj92ivakk282slwal0ujwaloj \
	--ca-hash sha256:1f7176d2474cf8dd3fa7a29e46ce42250c5a0aecaf07e40e014f039a7bf1e5ba \
	192.168.99.100:2377
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above command initializes and generates relevant secrets and CA Key for swarm.&lt;/p&gt;

&lt;p&gt;Currently swarm is created with only one node &amp;ldquo;master&amp;rdquo;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker node ls
ID                           HOSTNAME  MEMBERSHIP  STATUS  AVAILABILITY  MANAGER STATUS
2ksrqgk2x3vbjh3bw0aly5dwr *  manager   Accepted    Ready   Active        Leader
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;adding-workers-node:6855b0581c631fde987ca5dc2ca3bf28&#34;&gt;Adding Workers node&lt;/h3&gt;

&lt;p&gt;To add swarm workers &lt;code&gt;docker swarm join&lt;/code&gt; command will be used.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval $(docker-machine env worker1)

$ docker swarm join --secret aj92ivakk282slwal0ujwaloj \
&amp;gt;     --ca-hash sha256:1f7176d2474cf8dd3fa7a29e46ce42250c5a0aecaf07e40e014f039a7bf1e5ba \
&amp;gt;     192.168.99.100:2377
This node joined a Swarm as a worker.

$ eval $(docker-machine env worker2)

$ docker swarm join --secret aj92ivakk282slwal0ujwaloj \
    --ca-hash sha256:1f7176d2474cf8dd3fa7a29e46ce42250c5a0aecaf07e40e014f039a7bf1e5ba \
    192.168.99.100:2377
This node joined a Swarm as a worker.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To view complete list of nodes in swarm, we need to query swarm master.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;NOTE: In case of multiple masters, query can be made to any master&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval $(docker-machine env manager)

$ docker node ls
ID                           HOSTNAME  MEMBERSHIP  STATUS  AVAILABILITY  MANAGER STATUS
0cs0e5phve5onp41pxfe9c1kj    worker1   Accepted    Ready   Active
2ksrqgk2x3vbjh3bw0aly5dwr *  manager   Accepted    Ready   Active        Leader
c6nvb1ljj9mntt1v8qlmxl2my    worker2   Accepted    Ready   Active
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Thats it! two commands create whole docker swarm. No external KV Store, No tricky CA Key generation, It all just two commands&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;monitoring-events-of-swarm:6855b0581c631fde987ca5dc2ca3bf28&#34;&gt;Monitoring events of swarm&lt;/h3&gt;

&lt;p&gt;In swarm mode all managers have consistent view that enables, the monitoring of whole swarm through any  manager node.
All you need to listen at /var/run/docker.sock of any manager node.&lt;/p&gt;

&lt;h3 id=&#34;creating-visualizer-for-swarm:6855b0581c631fde987ca5dc2ca3bf28&#34;&gt;Creating Visualizer for Swarm.&lt;/h3&gt;

&lt;p&gt;Visualizer is a simple nodejs app, which listen on &lt;code&gt;/var/run/docker.sock&lt;/code&gt; and show nodes present in swarm and containers within as boxes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -it -d -p 8080:8080 -e HOST=$(docker-machine ip manager) -v /var/run/docker.sock:/var/run/docker.sock manomarks/visualizer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open the visualizer in browser with &lt;a href=&#34;http://192.168.99.100:8080&#34;&gt;http://192.168.99.100:8080&lt;/a&gt;, here &amp;ldquo;192.168.99.100&amp;rdquo; is manager node IP.&lt;/p&gt;

&lt;p&gt;&lt;img align=&#34;center&#34; src=http://kunalkushwaha.github.io/SwarmVisualizer-1.jpg&gt;&lt;/p&gt;

&lt;h3 id=&#34;lets-deploy-some-application-in-swarm:6855b0581c631fde987ca5dc2ca3bf28&#34;&gt;Lets deploy some Application in swarm&lt;/h3&gt;

&lt;p&gt;Till now applications can be deployed using &lt;code&gt;docker run&lt;/code&gt; command. To support orchestration, docker 1.12 have introduced &lt;code&gt;docker service&lt;/code&gt; command. Using &lt;code&gt;service&lt;/code&gt; commands, we can define properties of an application, which docker swarm mode tries to reconcile in case of any application errors or node failure.&lt;/p&gt;

&lt;p&gt;For demo, I have a simple application &lt;a href=&#34;https://github.com/kunalkushwaha/slides/tree/master/examples/webserver&#34;&gt;demoapp&lt;/a&gt;, which is a simple web-server listen on port &lt;code&gt;5000&lt;/code&gt;, prints the some message. Lets try to deploy this application.&lt;/p&gt;

&lt;p&gt;Since, I want my application to be spread all over swarm, and all instance should be discoverable, at first, I will create an &lt;a href=&#34;https://en.wikipedia.org/wiki/Overlay_network&#34;&gt;overlay network&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Lets create overlay network using &lt;code&gt;docker network create&lt;/code&gt; command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker network create -d overlay mynet
2xjpxwugr0glog7sqywdsnbn6

$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
01e7e286803b        bridge              bridge              local
7a512b485351        docker_gwbridge     bridge              local
6712d943241e        host                host                local
dyk8o6w5cddo        ingress             overlay             swarm
2xjpxwugr0gl        mynet               overlay             swarm
ae968fa64609        none                null                local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In above list of docker networks, there are two overlay networks.
- ingress is default overlay network which uses &lt;a href=&#34;http://www.linuxvirtualserver.org/software/ipvs.html&#34;&gt;IPVS&lt;/a&gt; and blazing fast due to kernel only data path. This is used for exposing services to external &lt;a href=&#34;https://docs.docker.com/engine/swarm/key-concepts/#/load-balancing&#34;&gt;load balancers&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;create-service:6855b0581c631fde987ca5dc2ca3bf28&#34;&gt;Create service&lt;/h3&gt;

&lt;p&gt;Lets create a service with 2 replicas(instance) attached to &lt;code&gt;mynet&lt;/code&gt; from image &lt;a href=&#34;https://hub.docker.com/r/kunalkushwaha/demoapp_image/&#34;&gt;kunalkushwaha/demoapp_image&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker service create \
&amp;gt; --name demoapp \
&amp;gt;     --replicas 2 \
&amp;gt;     --network mynet \
&amp;gt; -p 5000:5000 \
&amp;gt; kunalkushwaha/demoapp_image:v1
8fwyujxnc1rm0jwewx5tfeai1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;Service create&lt;/code&gt; commands defines that service and docker swarm manager, picks up the definition to schedule it.
You can monitor, service state by following commands.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker service ls&lt;/code&gt; just lists the service and shows how many instance running.
&lt;code&gt;docker service tasks&lt;/code&gt; command give details like on which node service instance is running.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker service ls
ID            NAME     REPLICAS  IMAGE                           COMMAND
8fwyujxnc1rm  demoapp  0/2       kunalkushwaha/demoapp_image:v1

$ docker service tasks demoapp
ID                         NAME       SERVICE  IMAGE                           LAST STATE              DESIRED STATE  NODE
a2yczjfyhyryg4pkg8m27ztd6  demoapp.1  demoapp  kunalkushwaha/demoapp_image:v1  Running 21 seconds ago  Running        manager
aj080rydyghazbhco6h9dhaaw  demoapp.2  demoapp  kunalkushwaha/demoapp_image:v1  Running 21 seconds ago  Running        worker2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To read configuration of service, you can use &lt;code&gt;docker service inspect&lt;/code&gt; command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker service inspect  demoapp --pretty
ID:		8fwyujxnc1rm0jwewx5tfeai1
Name:		demoapp
Mode:		Replicated
 Replicas:	2
Placement:
 Strategy:	Spread
UpdateConfig:
 Parallelism:	0
ContainerSpec:
 Image:		kunalkushwaha/demoapp_image:v1
Resources:
Reservations:
Limits:
Networks: 2xjpxwugr0glog7sqywdsnbn6Ports:
 Name =
 Protocol = tcp
 TargetPort = 5000
 PublishedPort = 5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img align=&#34;center&#34; src=http://kunalkushwaha.github.io/serviceInstance.jpg&gt;&lt;/p&gt;

&lt;p&gt;Now our service is deployed successfully, lets try to access and see if, it works properly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl 192.168.99.100:5000
This is DemoApp v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So far all good! Now lets try to scale this service by 6 instances.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker service scale demoapp=6
demoapp scaled to 6

$ docker service ls
ID            NAME     REPLICAS  IMAGE                           COMMAND
8fwyujxnc1rm  demoapp  4/6       kunalkushwaha/demoapp_image:v1

$ docker service ps demoapp
ID                         NAME       SERVICE  IMAGE                           LAST STATE                DESIRED STATE  NODE
a2yczjfyhyryg4pkg8m27ztd6  demoapp.1  demoapp  kunalkushwaha/demoapp_image:v1  Running 13 minutes ago    Running        manager
aj080rydyghazbhco6h9dhaaw  demoapp.2  demoapp  kunalkushwaha/demoapp_image:v1  Running 13 minutes ago    Running        worker2
7c0dbomrbljlqdq9gitthjg1f  demoapp.3  demoapp  kunalkushwaha/demoapp_image:v1  Running 10 seconds ago    Running        manager
dzooellh3a2vsbsr2soesfhir  demoapp.4  demoapp  kunalkushwaha/demoapp_image:v1  Running 10 seconds ago    Running        worker2
5r114sf8i0n5o53s0hwv911g4  demoapp.5  demoapp  kunalkushwaha/demoapp_image:v1  Preparing 10 seconds ago  Running        worker1
85fbs7z0ntazachrxw98xnjfo  demoapp.6  demoapp  kunalkushwaha/demoapp_image:v1  Preparing 10 seconds ago  Running        worker1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you observe carefully, column of &lt;code&gt;LAST STATE&lt;/code&gt; &amp;amp; &lt;code&gt;DESIRED STATE&lt;/code&gt; shows &lt;code&gt;demoapp.5&lt;/code&gt; and &lt;code&gt;demoapp.6&lt;/code&gt; as &amp;ldquo;Preparing&amp;rdquo; and &amp;ldquo;Running&amp;rdquo;
This is same as explained above i.e. &lt;code&gt;docker scale&lt;/code&gt; command changes the configuration of service. Now docker swarm mode is trying to achieve desired state.&lt;/p&gt;

&lt;p&gt;&lt;img align=&#34;center&#34; src=http://kunalkushwaha.github.io/swarm-scale.jpg&gt;&lt;/p&gt;

&lt;h3 id=&#34;node-failure:6855b0581c631fde987ca5dc2ca3bf28&#34;&gt;Node failure.&lt;/h3&gt;

&lt;p&gt;Let try to delete one of worker node and see how swarm it tries to reconcile the configuration of service.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine rm worker2
About to remove worker2
Are you sure? (y/n): y
Successfully removed worker2

$ docker service ps demoapp
ID                         NAME       SERVICE  IMAGE                           LAST STATE              DESIRED STATE  NODE
a2yczjfyhyryg4pkg8m27ztd6  demoapp.1  demoapp  kunalkushwaha/demoapp_image:v1  Running 27 minutes ago  Running        manager
5afbnsfl5p3xyygbk1b6aawc6  demoapp.2  demoapp  kunalkushwaha/demoapp_image:v1  Accepted 3 seconds ago  Accepted       worker1
7c0dbomrbljlqdq9gitthjg1f  demoapp.3  demoapp  kunalkushwaha/demoapp_image:v1  Running 13 minutes ago  Running        manager
8lgqtsewc0vbgh318fx27sm0m  demoapp.4  demoapp  kunalkushwaha/demoapp_image:v1  Accepted 3 seconds ago  Accepted       manager
5r114sf8i0n5o53s0hwv911g4  demoapp.5  demoapp  kunalkushwaha/demoapp_image:v1  Running 13 minutes ago  Running        worker1
85fbs7z0ntazachrxw98xnjfo  demoapp.6  demoapp  kunalkushwaha/demoapp_image:v1  Running 13 minutes ago  Running        worker1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see now, worker2&amp;rsquo;s instances (&lt;code&gt;demoapp.2&lt;/code&gt; &amp;amp; &lt;code&gt;demoapp.4&lt;/code&gt;) got rescheduled on manager and worker1. All by its own :).&lt;/p&gt;

&lt;p&gt;&lt;img align=&#34;center&#34; src=http://kunalkushwaha.github.io/node-failure.jpg&gt;&lt;/p&gt;

&lt;h3 id=&#34;rolling-updates:6855b0581c631fde987ca5dc2ca3bf28&#34;&gt;Rolling updates.&lt;/h3&gt;

&lt;p&gt;I have v2 of demoapp, which appends IP address of container to message. Lets try to upgrade the application.
Here we can define how many instances should get upgraded at one point &lt;code&gt;--update-parallelism&lt;/code&gt; and also delay between two updates &lt;code&gt;--update-delay&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;These both are important feature, which helps to upgrade the application without any downtime. Also, while upgrading if some error occurs, you can rollback the service.&lt;/p&gt;

&lt;p&gt;In this example, I will try to upgrade &lt;code&gt;2&lt;/code&gt; instances at a time and with delay of &lt;code&gt;30s&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker service update demoapp --update-parallelism=2 --update-delay 10s --image kunalkushwaha/demoapp_image:v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now try to get the output of app.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl  192.168.99.100:5000
This is DemoApp v2 @  IP: 10.255.0.13

$ curl 192.168.99.100:5000
This is DemoApp v2 @  IP: 10.255.0.9

$ curl 192.168.99.100:5000
This is DemoApp v1

$ curl 192.168.99.100:5000
This is DemoApp v1

$ curl 192.168.99.100:5000
This is DemoApp v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see output is mixed from v1 and v2 application instance. This is due to docker&amp;rsquo;s internal load-balancer. Docker has embedded DNS, which is used for service discovery and load balancing. By default its round-robin.&lt;/p&gt;

&lt;h3 id=&#34;routing-mesh:6855b0581c631fde987ca5dc2ca3bf28&#34;&gt;Routing Mesh.&lt;/h3&gt;

&lt;p&gt;With routing mesh, service discovery of services in swarm can be done through any node of swarm. i.e. even if service instance not running on any particular node, still if request comes of application/service port, it will be redirected to one of running instance of service. This is achieved by &lt;code&gt;ingress&lt;/code&gt; network.&lt;/p&gt;

&lt;p&gt;To demonstrate, lets add one more node into swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox worker3

$ eval $(docker-machine env worker3)

$ docker swarm join --secret aj92ivakk282slwal0ujwaloj \
    --ca-hash sha256:1f7176d2474cf8dd3fa7a29e46ce42250c5a0aecaf07e40e014f039a7bf1e5ba \
    192.168.99.100:2377
This node joined a Swarm as a worker.

$ eval $(docker-machine env manager)

$ docker node ls
ID                           HOSTNAME  MEMBERSHIP  STATUS  AVAILABILITY  MANAGER STATUS
0cs0e5phve5onp41pxfe9c1kj    worker1   Accepted    Ready   Active
2ksrqgk2x3vbjh3bw0aly5dwr *  manager   Accepted    Ready   Active        Leader
5gfj06su4l885zg88qyze7imu    worker3   Accepted    Ready   Active
c6nvb1ljj9mntt1v8qlmxl2my    worker2   Accepted    Down    Active

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, demoapp is running only on worker1 and manager nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker service ps demoapp
ID                         NAME       SERVICE  IMAGE                           LAST STATE              DESIRED STATE  NODE
9ucohwuw91e8uuquhxidhplsu  demoapp.1  demoapp  kunalkushwaha/demoapp_image:v2  Running 15 minutes ago  Running        manager
a1odej6m4pz1k4lowzt2l48rz  demoapp.2  demoapp  kunalkushwaha/demoapp_image:v2  Running 16 minutes ago  Running        worker1
crm4zjjb1hjxdr59zraef4yj9  demoapp.3  demoapp  kunalkushwaha/demoapp_image:v2  Running 15 minutes ago  Running        worker1
9q84itlob65mbzzvpb1dyjtij  demoapp.4  demoapp  kunalkushwaha/demoapp_image:v2  Running 15 minutes ago  Running        worker1
7bg8r15bq617ghupei8yegqdi  demoapp.5  demoapp  kunalkushwaha/demoapp_image:v2  Running 16 minutes ago  Running        worker1
5uu9qppsh7ohopc7td1tjs9lg  demoapp.6  demoapp  kunalkushwaha/demoapp_image:v2  Running 15 minutes ago  Running        manager
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If I try to send request on worker3, still I will be able to get demoapp output and that too with load balancing.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine ip worker3
192.168.99.103

$ curl --noproxy 192.168.99.103 192.168.99.103:5000
This is DemoApp v2 @  IP: 10.255.0.13

$ curl --noproxy 192.168.99.103 192.168.99.103:5000
This is DemoApp v2 @  IP: 10.255.0.14

$ curl --noproxy 192.168.99.103 192.168.99.103:5000
This is DemoApp v2 @  IP: 10.0.0.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Isnt&amp;rsquo;t this cool!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Docker swarm mode is about making orchestration simple, so anyone without deep understanding of distributed computing, clustering, security should be able to create a robust, scalable and super secure cluster and still focus on his main work.&lt;/p&gt;

&lt;p&gt;Also Docker swarm mode do no expect you to change your workflow and application deployment. It just adapts to you.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you find docker swarm mode interesting, You should look at &lt;a href=&#34;https://github.com/docker/swarmkit&#34;&gt;SwarmKit&lt;/a&gt;. This project does all magic for docker swarm mode and you can use to build your own distributed application.&lt;/p&gt;

&lt;p&gt;Hope this blog will help you to explore docker 1.12 RC.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tutorial setting up docker-swarm with multihost networking using docker-machine.</title>
      <link>http://kunalkushwaha.github.io/2016/01/14/tutorial-docker-swarm-with-multihost-networking/</link>
      <pubDate>Thu, 14 Jan 2016 12:45:10 +0900</pubDate>
      
      <guid>http://kunalkushwaha.github.io/2016/01/14/tutorial-docker-swarm-with-multihost-networking/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/docker/machine&#34;&gt;Docker-machine&lt;/a&gt; is tool to create Docker hosts on computer, on cloud providers, and inside data center. It creates &lt;a href=&#34;https://docs.docker.com/machine/drivers/os-base/&#34;&gt;Linux based&lt;/a&gt; server, and installs and configures docker. It is also capable of configuring docker-swarm nodes.&lt;/p&gt;

&lt;p&gt;This blog, will explain stepwise walkthrough for docker host creation using docker-machine.&lt;/p&gt;

&lt;h2 id=&#34;installation:b7bef7e2cdd7681f0182f26cab562e4a&#34;&gt;Installation.&lt;/h2&gt;

&lt;p&gt;If you use Windows or Mac, Docker has already made awesome packaged installer for you &lt;a href=&#34;https://www.docker.com/products/docker-toolbox&#34;&gt;Docker-toolbox&lt;/a&gt;. Download and Install it.
For Linux users, you can download docker-machine binaries from &lt;a href=&#34;https://github.com/docker/machine/releases&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;NOTE: &lt;i&gt; Linux users, do remember, you need to install docker client also on your machine.&lt;/i&gt;&lt;/p&gt;

&lt;h2 id=&#34;creation-of-docker-host-with-machine:b7bef7e2cdd7681f0182f26cab562e4a&#34;&gt;Creation of docker host with machine.&lt;/h2&gt;

&lt;p&gt;I will be using VirtualBox in this demo, but you can explore cloud options too.&lt;/p&gt;

&lt;p&gt;Docker host can be created with command &lt;code&gt;docker-machine create&lt;/code&gt;. Here &lt;code&gt;-d&lt;/code&gt; flags specifies driver-name. So command looks like as follows.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ docker-machine create -d &amp;lt;driver-name&amp;gt; &amp;lt;name-of-machine&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This command does couple of things at backend such as
 - It downloads latest boot2docker image, if not locally available.
 - Create a machine with &lt;code&gt;name-of-machine&lt;/code&gt;
 - Creates ssh keys and copies to machine.
 - Installs &lt;code&gt;docker&lt;/code&gt;
 - Configures docker daemon at port 2376, so that docker daemon accessible at  &lt;code&gt;tcp://&amp;lt;HostIP&amp;gt;:2376&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Lets create our first host &lt;code&gt;testhost&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ docker-machine create -d virtualbox testhost
 Running pre-create checks...
Creating machine...
(testhost) Copying C:\Users\kunal\.docker\machine\cache\boot2docker.iso to C:\Users\kunal\.docker\machine\machines\testhost\boot2docker.iso...
(testhost) Creating VirtualBox VM...
(testhost) Creating SSH key...
(testhost) Starting VM...
Waiting for machine to be running, this may take a few minutes...
Machine is running, waiting for SSH to be available...
Detecting operating system of created instance...
Detecting the provisioner...
Provisioning with boot2docker...
Copying certs to the local machine directory...
Copying certs to the remote machine...
Setting Docker configuration on the remote daemon...
Checking connection to Docker...
Docker is up and running!
To see how to connect Docker to this machine, run: C:\Program Files\Docker Toolbox\docker-machine.exe env testhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here our docker host is ready for use. But wait! host is not same machine as your machine right? It is running inside Virtual Machine.
So how docker daemon is accessed remotely?&lt;/p&gt;

&lt;p&gt;Docker works on client-server model. Docker daemon is server and it can communicates though &lt;a href=&#34;https://docs.docker.com/engine/reference/api/docker_remote_api/&#34;&gt;REST API&amp;rsquo;s&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;i&gt; NOTE: &lt;code&gt;docker-machine create&lt;/code&gt; is most important command of docker-machine. Understanding various flags help you to get best of docker-machine. You must spend some time in understanding all &lt;a href=&#34;https://docs.docker.com/machine/reference/create/&#34;&gt;flags&lt;/a&gt; &lt;/i&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To access docker host with docker client binary remotely, we need to export some environment variables.
Docker-machine provides a handy command for printing these variables and their values for us as shown below.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine env testhost
  export DOCKER_TLS_VERIFY=&amp;quot;1&amp;quot;
  export DOCKER_HOST=&amp;quot;tcp://192.168.99.110:2376&amp;quot;
  export DOCKER_CERT_PATH=&amp;quot;C:\Users\kunal\.docker\machine\machines\testhost&amp;quot;
  export DOCKER_MACHINE_NAME=&amp;quot;testhost&amp;quot;
  # Run this command to configure your shell:
  # eval $(&amp;quot;C:\Program Files\Docker Toolbox\docker-machine.exe&amp;quot; env testhost)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;To export all environment variable at once, simply run
&lt;code&gt;
$ eval $(docker-machine env testhost)
&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Now all docker commands will communicates with docker daemon of &lt;code&gt;testhost&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  $ docker info
  Containers: 0
  Images: 0
  Server Version: 1.9.1
  Storage Driver: aufs
   Root Dir: /mnt/sda1/var/lib/docker/aufs
   Backing Filesystem: extfs
   Dirs: 0
   Dirperm1 Supported: true
  Execution Driver: native-0.2
  Logging Driver: json-file
  Kernel Version: 4.1.13-boot2docker
  Operating System: Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015
  CPUs: 1
  Total Memory: 996.2 MiB
  Name: testhost
  ID: 26MF:TVKB:JI7Q:TL4S:7RXM:U5CD:VHIR:W2NH:CUNZ:RGB3:C6GC:POBS
  Debug mode (server): true
   File Descriptors: 14
   Goroutines: 21
   System Time: 2016-01-07T07:09:51.081394779Z
   EventsListeners: 0
   Init SHA1:
   Init Path: /usr/local/bin/docker
   Docker Root Dir: /mnt/sda1/var/lib/docker
  Labels:
   provider=virtualbox
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets create our first docker container on this host.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker run busybox sh
  Unable to find image &#39;busybox:latest&#39; locally
  latest: Pulling from library/busybox
  c00ef186408b: Pulling fs layer
  ac6a7980c6c2: Pulling fs layer
  ac6a7980c6c2: Verifying Checksum
  ac6a7980c6c2: Download complete
  c00ef186408b: Verifying Checksum
  c00ef186408b: Download complete
  c00ef186408b: Pull complete
  ac6a7980c6c2: Pull complete
  Digest: sha256:e4f93f6ed15a0cdd342f5aae387886fba0ab98af0a102da6276eaf24d6e6ade0
  Status: Downloaded newer image for busybox:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;i&gt; NOTE: If you are one of user, who works behind proxy, then you may face problem like, your newly created docker host may not communicate with &lt;a href=&#34;https://hub.docker.com/&#34;&gt;docker-hub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To Fix that, you need to pass few environment flags at creation time as below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine create -d virtualbox \
  --engine-env HTTP_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
  --engine-env HTTPS_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
  testhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;So now we learnt how to create a docker host and run docker containers using docker-machine.
But Docker-machine not only lets you to create independent hosts, but also can put these host in one cluster!&lt;/p&gt;

&lt;p&gt;Isn&amp;rsquo;t it interesting?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how we can build whole cluster of docker-hosts using docker-machine.&lt;/p&gt;

&lt;p&gt;Docker &lt;a href=&#34;https://www.docker.com/products/docker-swarm&#34;&gt;Swarm&lt;/a&gt; is native clustering solution for docker. Here with native means, Swarm understands and exports &lt;a href=&#34;https://docs.docker.com/swarm/swarm-api/&#34;&gt;all(almost)&lt;/a&gt; docker API&amp;rsquo;s. i.e. API for docker and docker swarm are same.
If one product/scripts works with docker, it will automatically work with swarm :)&lt;/p&gt;

&lt;h2 id=&#34;docker-swarm:b7bef7e2cdd7681f0182f26cab562e4a&#34;&gt;Docker Swarm&lt;/h2&gt;

&lt;p&gt;Docker swarm has three components.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Discovery Backend : Swarm requires a discovery backend, which is used by each node(agent) to discover the master.

&lt;ul&gt;
&lt;li&gt;Default discovery is provided by Docker Hub. (Not for production usage)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Master : Swarm master takes care of all scheduling logic and HA.&lt;/li&gt;
&lt;li&gt;Agent : These runs on each node and communicates with Swarm master.

&lt;ul&gt;
&lt;li&gt;All agent node must listen to the same network interface (TCP port).&lt;/li&gt;
&lt;li&gt;Each node runs a node agent that registers the referenced Docker daemon, monitors it, and updates the discovery backend with the nodeâ€™s status&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this demo, We will create a docker swarm consisting one Master and three agents including master.
Also, Consul will be used for discovery backend.&lt;/p&gt;

&lt;h3 id=&#34;discovery-backend-using-consul:b7bef7e2cdd7681f0182f26cab562e4a&#34;&gt;Discovery Backend using consul.&lt;/h3&gt;

&lt;p&gt;We will create a dedicated machine for consul, but running &lt;a href=&#34;https://www.consul.io/&#34;&gt;consul&lt;/a&gt; using docker is just one command task.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine create -d virtualbox \
     --engine-env HTTP_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
     --engine-env HTTPS_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
     swl-consul

$ docker $(docker-machine config swl-consul) run \
     -e &amp;quot;http_proxy=http://example.com:8080&amp;quot; \
     -e &amp;quot;https_proxy=http://example.com:8080&amp;quot; \
     --restart=&amp;quot;always&amp;quot; \
     -d -p &amp;quot;8500:8500&amp;quot; \
     -h &amp;quot;consul&amp;quot; \
     progrium/consul -server -bootstrap
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check if consul container is created as expected.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine ls
  NAME         ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER   ERRORS
  swl-consul   -        virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1

  $ eval $(docker-machine env swl-consul)

  $ docker ps
  CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                            NAMES
  5357e85abcb5        progrium/consul     &amp;quot;/bin/start -server -&amp;quot;   16 minutes ago      Up 16 minutes       53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 0.0.0.0:8500-&amp;gt;8500/tcp   goofy_aryabhata
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;creation-of-swarm-master:b7bef7e2cdd7681f0182f26cab562e4a&#34;&gt;Creation of Swarm Master.&lt;/h3&gt;

&lt;p&gt;In cluster, we will have one master and 2 agents. But master will also have agent, so practically we will have 3 agents.&lt;/p&gt;

&lt;p&gt;Docker machine helps to setup docker host with swarm in single command.&lt;/p&gt;

&lt;p&gt;Few highlights of docker machine provisioned swarm enabled host.
- Swarm master and agent runs as docker containers.
- Swarm manager bind on &amp;ldquo;3376&amp;rdquo; port, So to communicate with master &lt;code&gt;tcp://&amp;lt;IP:3376&amp;gt;&amp;gt;&lt;/code&gt; should be used.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine create \
  &amp;gt;    -d virtualbox \
  &amp;gt;    --swarm \
  &amp;gt;    --swarm-master \
  &amp;gt;    --swarm-discovery=consul://$(docker-machine ip swl-consul):8500 \
  &amp;gt;    --engine-env HTTP_PROXY=http://example.com:8080 \
  &amp;gt;    --engine-env HTTPS_PROXY=http://example.com:8080 \
  &amp;gt;    --engine-env NO_PROXY=192.168.99.100 \
  &amp;gt;    --engine-opt=&amp;quot;cluster-store=consul://$(docker-machine ip swl-consul):8500&amp;quot; \
  &amp;gt;    --engine-opt=&amp;quot;cluster-advertise=eth1:3376&amp;quot; \
  &amp;gt;    node1
  Running pre-create checks...
   .
   .
  (node1) Starting VM...
  Waiting for machine to be running, this may take a few minutes...
  Machine is running, waiting for SSH to be available...
  Detecting operating system of created instance...
  Detecting the provisioner...
  Provisioning with boot2docker...
  Copying certs to the local machine directory...
  Copying certs to the remote machine...
  Setting Docker configuration on the remote daemon...
  Configuring swarm...
  Checking connection to Docker...
  Docker is up and running!
  To see how to connect Docker to this machine, run: C:\Program Files\Docker Toolbox\docker-machine.exe env node1

  $ eval $(docker-machine env node1)

  $ docker ps
  CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
  7e644b71f606        swarm:latest        &amp;quot;/swarm join --advert&amp;quot;   19 seconds ago      Up 16 seconds                           swarm-agent
  58f2a4da438b        swarm:latest        &amp;quot;/swarm manage --tlsv&amp;quot;   22 seconds ago      Up 19 seconds                           swarm-agent-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;i&gt; NOTE: You may have seen &lt;code&gt;NO_PROXY&lt;/code&gt; is also set as environment variable.&lt;/p&gt;

&lt;p&gt;This is to skip proxy for communicating with consul server &lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Here,
- &lt;code&gt;--swarm&lt;/code&gt; indicates, swarm agent will be configured.
- &lt;code&gt;--swarm-master&lt;/code&gt; indicates, swarm master will be configured.
&lt;code&gt;--swarm-discovery&lt;/code&gt; sets the backend discovery for swarm.&lt;/p&gt;

&lt;p&gt;Here you go, we have our swarm master node up and running. Before communicating to swarm, let me remind you again.
- Swarm and docker talks in same API signature. But on our host, we have both docker daemon and swarm running.
- Our docker client can communicate with both, but we need to choose whom it has to talk.
  - To talk with docker daemon use &lt;code&gt;eval $(docker-machine env &amp;lt;host-name&amp;gt;)&lt;/code&gt;
  - To talk with swarm use &lt;code&gt;eval $(docker-machine env --swarm &amp;lt;host-name&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;So, since we want our docker client should communicate with swarm, we will add &lt;code&gt;--swarm&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ eval $(docker-machine env --swarm node1)

  $ docker info
  Containers: 2
  Images: 1
  Role: primary
  Strategy: spread
  Filters: health, port, dependency, affinity, constraint
  Nodes: 1
   node1: 192.168.99.102:2376
    â”” Status: Healthy
    â”” Containers: 2
    â”” Reserved CPUs: 0 / 1
    â”” Reserved Memory: 0 B / 1.021 GiB
    â”” Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs
  CPUs: 1
  Total Memory: 1.021 GiB
  Name: node1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now add two nodes with only agent.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine create \
  &amp;gt;    -d virtualbox \
  &amp;gt;    --swarm \
  &amp;gt;    --swarm-discovery=&amp;quot;consul://$(docker-machine ip swl-consul):8500&amp;quot; \
  &amp;gt;    --engine-env HTTP_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
  &amp;gt;    --engine-env HTTPS_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
  &amp;gt;    --engine-env NO_PROXY=&amp;quot;192.168.99.100&amp;quot; \
  &amp;gt;    --engine-opt=&amp;quot;cluster-store=consul://$(docker-machine ip swl-consul):8500&amp;quot; \
  &amp;gt;    --engine-opt=&amp;quot;cluster-advertise=eth1:3376&amp;quot; \  
  &amp;gt;    node3
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Command for adding agent doesn&amp;rsquo;t requires &lt;code&gt;--swarm-master&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cluster-advertise&lt;/code&gt; and &lt;code&gt;cluster-store&lt;/code&gt; are required for overlay networking also, Will be explained later.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So after adding two more machines with swarm agent, we have 4 machines running as below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine ls
  NAME         ACTIVE   DRIVER       STATE     URL                         SWARM            DOCKER   ERRORS
  node1        -        virtualbox   Running   tcp://192.168.99.102:2376   node1 (master)   v1.9.1
  node2        -        virtualbox   Running   tcp://192.168.99.103:2376   node1            v1.9.1
  node3        -        virtualbox   Running   tcp://192.168.99.104:2376   node1            v1.9.1
  swl-consul   -        virtualbox   Running   tcp://192.168.99.100:2376                    v1.9.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets see information of our cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ eval $(docker-machine env --swarm node1)

  $ docker info
  Containers: 4
  Images: 3
  Role: primary
  Strategy: spread
  Filters: health, port, dependency, affinity, constraint
  Nodes: 3
   node1: 192.168.99.102:2376
    â”” Status: Healthy
    â”” Containers: 2
    â”” Reserved CPUs: 0 / 1
    â”” Reserved Memory: 0 B / 1.021 GiB
    â”” Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs
   node2: 192.168.99.103:2376
    â”” Status: Healthy
    â”” Containers: 1
    â”” Reserved CPUs: 0 / 1
    â”” Reserved Memory: 0 B / 1.021 GiB
    â”” Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs
   node3: 192.168.99.104:2376
    â”” Status: Healthy
    â”” Containers: 1
    â”” Reserved CPUs: 0 / 1
    â”” Reserved Memory: 0 B / 1.021 GiB
    â”” Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs
  CPUs: 3
  Total Memory: 3.064 GiB
  Name: node1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Superb! we have three nodes running in our cluster.
This completes the swarm setup!!&lt;/p&gt;

&lt;h2 id=&#34;understanding-few-more-swarm-functionality:b7bef7e2cdd7681f0182f26cab562e4a&#34;&gt;Understanding few more Swarm functionality.&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Strategy&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Currently &amp;ldquo;Spread&amp;rdquo; strategy is set. So the containers will be spread over all the hosts in cluster.&lt;/p&gt;

&lt;p&gt;e.g.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker run -d -P -m 1G -e MYSQL_ROOT_PASSWORD=test123 --name db mysql
  57a1af46d72117306b833a28a219d3523e1531c98a19ef25cba00a7bc94c9645

  $ docker run -d -P -m 1G --name frontend nginx
  f40776d7c9f7583965263340c1542a09eb7cb881c53bcf18a290df0d6ce2fd51

  $ docker ps
  CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                         NAMES
  f40776d7c9f7        nginx               &amp;quot;nginx -g &#39;daemon off&amp;quot;   4 seconds ago       Up 4 seconds        192.168.99.103:32770-&amp;gt;80/tcp, 192.168.99.103:32769-&amp;gt;443/tcp   node2/frontend
  57a1af46d721        mysql               &amp;quot;/entrypoint.sh mysql&amp;quot;   3 minutes ago       Up 3 minutes        192.168.99.104:32769-&amp;gt;3306/tcp                                node3/db
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Here node3 have &lt;code&gt;db&lt;/code&gt; and node2 have &lt;code&gt;frontend&lt;/code&gt; container.&lt;/li&gt;
&lt;li&gt;If two nodes have the same amount of available RAM and CPUs, the spread strategy prefers the node with least containers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other available strategies are BinPack and Random.
- BinPack tries to pack all possible containers on single node first and so on.
- If two nodes have the same amount of available RAM and CPUs, the binpack strategy prefers the node with most containers.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Things to Try:
- Create swarm with BinPack and create multiple containers to see the behavior. &lt;/i&gt;&lt;/p&gt;

&lt;h3 id=&#34;filters:b7bef7e2cdd7681f0182f26cab562e4a&#34;&gt;Filters&lt;/h3&gt;

&lt;p&gt;Filters tell Docker Swarm scheduler which nodes to use when creating and running a container.&lt;/p&gt;

&lt;p&gt;Filters are divided into two categories, node filters and container configuration filters.
- Node filters operate on characteristics of the Docker host or on the configuration of the Docker daemon.
- Container configuration filters operate on characteristics of containers, or on the availability of images on a host.
- Each filter has a name that identifies it.&lt;/p&gt;

&lt;p&gt;The node filters are:
  - constraint
  - health&lt;/p&gt;

&lt;p&gt;The container configuration filters are:
  - affinity
  - dependency
  - port&lt;/p&gt;

&lt;p&gt;When you start a Swarm manager with the swarm manage command, all the filters are enabled.
If you want to limit the filters available to your Swarm, specify a subset of filters by passing the &lt;code&gt;--filter&lt;/code&gt; flag and the name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm manage --filter=health --filter=dependency
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In case of docker machine, while provisioning &lt;code&gt;--swarm-opt&lt;/code&gt; can be used to set filters.&lt;/p&gt;

&lt;h4 id=&#34;use-a-constraint-filter:b7bef7e2cdd7681f0182f26cab562e4a&#34;&gt;Use a constraint filter&lt;/h4&gt;

&lt;p&gt;Node constraints can refer to Dockerâ€™s default tags or to custom labels. Default tags are sourced from docker info. Often, they relate to properties of the Docker host. Currently, the dafult tags include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;node to refer to the node by ID or name&lt;/li&gt;
&lt;li&gt;storagedriver&lt;/li&gt;
&lt;li&gt;executiondriver&lt;/li&gt;
&lt;li&gt;kernelversion&lt;/li&gt;
&lt;li&gt;operatingsystem&lt;/li&gt;
&lt;li&gt;Custom node labels can be applied while provisioning docker machine.

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--engine-label&lt;/code&gt; can be used for setting custom label.&lt;/li&gt;
&lt;li&gt;custom label like &lt;code&gt;environment&lt;/code&gt;, &lt;code&gt;storage&lt;/code&gt; etc are helpful.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since we have all labels same except node name, we will try creating new container using node-name constraint.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -itd -e constraint:node==node3 --name test1 ubuntu
ae4013d014931e43cd5342a625f6b549a8c920cb146b1371a7226359a4bcf014

$ docker run -itd -e constraint:node==node3 --name test2 ubuntu
a6b371773e1a2609122ca68df00d1e03ee274ba3db504d3942c301f8e2c45504

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
a6b371773e1a        ubuntu              &amp;quot;/bin/bash&amp;quot;         3 seconds ago       Up 2 seconds                            node3/test2
ae4013d01493        ubuntu              &amp;quot;/bin/bash&amp;quot;         19 seconds ago      Up 18 seconds                           node3/test1
934501aed30f        ubuntu              &amp;quot;/bin/bash&amp;quot;         19 hours ago        Up 2 hours                              node3/n4
30358292be89        ubuntu              &amp;quot;/bin/bash&amp;quot;         19 hours ago        Up 19 hours                             node1/n3
6a13aa3eca8b        ubuntu              &amp;quot;/bin/bash&amp;quot;         21 hours ago        Up 21 hours                             node2/n1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similarly other filters can be used. While using filters the syntax to use in &lt;code&gt;docker run&lt;/code&gt; is as below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- ``-e &amp;lt;filter-name&amp;gt;:&amp;lt;key&amp;gt;&amp;lt;operator&amp;gt;&amp;lt;value&amp;gt;``
- Here operator used are  ``==`` , ``!=`` &amp;amp; ``~``
- For ``==`` &amp;amp; ``!=``, exact match is found.
  - If nothing satisfies the condition, it does schedule container.
- ``~`` is for soft condition. i.e. if nothing matches, it ignores the condition and use default scheduler.
- Here value can contain any valid regex (https://github.com/google/re2/wiki/Syntax)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hope this article will be helpful in getting started with docker-machine and swarm :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tutorial: Getting started with docker machine and Swarm</title>
      <link>http://kunalkushwaha.github.io/2016/01/14/tutorial-getting-started-with-docker-machine-and-swarm/</link>
      <pubDate>Thu, 14 Jan 2016 12:45:10 +0900</pubDate>
      
      <guid>http://kunalkushwaha.github.io/2016/01/14/tutorial-getting-started-with-docker-machine-and-swarm/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/docker/machine&#34;&gt;Docker-machine&lt;/a&gt; is tool to create Docker hosts on computer, on cloud providers, and inside data center. It creates &lt;a href=&#34;https://docs.docker.com/machine/drivers/os-base/&#34;&gt;Linux based&lt;/a&gt; server, and installs and configures docker. It is also capable of configuring docker-swarm nodes.&lt;/p&gt;

&lt;p&gt;This blog, will explain stepwise walkthrough for docker host creation using docker-machine.&lt;/p&gt;

&lt;h2 id=&#34;installation:fce0df6d9ab4aa14b09a0b333f488e03&#34;&gt;Installation.&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you use Windows or Mac, Docker has already made awesome packaged installer for you &lt;a href=&#34;https://www.docker.com/products/docker-toolbox&#34;&gt;Docker-toolbox&lt;/a&gt;. Download and Install it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For Linux users, you can download docker-machine binaries from &lt;a href=&#34;https://github.com/docker/machine/releases&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NOTE: &lt;i&gt; Linux users, do remember, you need to install docker client also on your machine.&lt;/i&gt;&lt;/p&gt;

&lt;h2 id=&#34;creation-of-docker-host-with-machine:fce0df6d9ab4aa14b09a0b333f488e03&#34;&gt;Creation of docker host with machine.&lt;/h2&gt;

&lt;p&gt;I will be using VirtualBox in this demo, but you can explore cloud options too.&lt;/p&gt;

&lt;p&gt;Docker host can be created with command &lt;code&gt;docker-machine create&lt;/code&gt;. Here &lt;code&gt;-d&lt;/code&gt; flags specifies driver-name. So command looks like as follows.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ docker-machine create -d &amp;lt;driver-name&amp;gt; &amp;lt;name-of-machine&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This command does couple of things at backend such as
 - It downloads latest boot2docker image, if not locally available.
 - Create a machine with &lt;code&gt;name-of-machine&lt;/code&gt;
 - Creates ssh keys and copies to machine.
 - Installs &lt;code&gt;docker&lt;/code&gt;
 - Configures docker daemon at port 2376, so that docker daemon accessible at  &lt;code&gt;tcp://&amp;lt;HostIP&amp;gt;:2376&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Lets create our first host &lt;code&gt;testhost&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ docker-machine create -d virtualbox testhost
 Running pre-create checks...
Creating machine...
(testhost) Copying C:\Users\kunal\.docker\machine\cache\boot2docker.iso to C:\Users\kunal\.docker\machine\machines\testhost\boot2docker.iso...
(testhost) Creating VirtualBox VM...
(testhost) Creating SSH key...
(testhost) Starting VM...
Waiting for machine to be running, this may take a few minutes...
Machine is running, waiting for SSH to be available...
Detecting operating system of created instance...
Detecting the provisioner...
Provisioning with boot2docker...
Copying certs to the local machine directory...
Copying certs to the remote machine...
Setting Docker configuration on the remote daemon...
Checking connection to Docker...
Docker is up and running!
To see how to connect Docker to this machine, run: C:\Program Files\Docker Toolbox\docker-machine.exe env testhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here our docker host is ready for use. But wait! host is not same machine as your machine right? It is running inside Virtual Machine.
So how docker daemon is accessed remotely?&lt;/p&gt;

&lt;p&gt;Docker works on client-server model. Docker daemon is server and it can communicates though &lt;a href=&#34;https://docs.docker.com/engine/reference/api/docker_remote_api/&#34;&gt;REST API&amp;rsquo;s&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;i&gt; NOTE: &lt;code&gt;docker-machine create&lt;/code&gt; is most important command of docker-machine. Understanding various flags help you to get best of docker-machine. You must spend some time in understanding all &lt;a href=&#34;https://docs.docker.com/machine/reference/create/&#34;&gt;flags&lt;/a&gt; &lt;/i&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To access docker host with docker client binary remotely, we need to export some environment variables.
Docker-machine provides a handy command for printing these variables and their values for us as shown below.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine env testhost
  export DOCKER_TLS_VERIFY=&amp;quot;1&amp;quot;
  export DOCKER_HOST=&amp;quot;tcp://192.168.99.110:2376&amp;quot;
  export DOCKER_CERT_PATH=&amp;quot;C:\Users\kunal\.docker\machine\machines\testhost&amp;quot;
  export DOCKER_MACHINE_NAME=&amp;quot;testhost&amp;quot;
  # Run this command to configure your shell:
  # eval $(&amp;quot;C:\Program Files\Docker Toolbox\docker-machine.exe&amp;quot; env testhost)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;To export all environment variable at once, simply run
&lt;code&gt;
$ eval $(docker-machine env testhost)
&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Now all docker commands will communicates with docker daemon of &lt;code&gt;testhost&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  $ docker info
  Containers: 0
  Images: 0
  Server Version: 1.9.1
  Storage Driver: aufs
   Root Dir: /mnt/sda1/var/lib/docker/aufs
   Backing Filesystem: extfs
   Dirs: 0
   Dirperm1 Supported: true
  Execution Driver: native-0.2
  Logging Driver: json-file
  Kernel Version: 4.1.13-boot2docker
  Operating System: Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015
  CPUs: 1
  Total Memory: 996.2 MiB
  Name: testhost
  ID: 26MF:TVKB:JI7Q:TL4S:7RXM:U5CD:VHIR:W2NH:CUNZ:RGB3:C6GC:POBS
  Debug mode (server): true
   File Descriptors: 14
   Goroutines: 21
   System Time: 2016-01-07T07:09:51.081394779Z
   EventsListeners: 0
   Init SHA1:
   Init Path: /usr/local/bin/docker
   Docker Root Dir: /mnt/sda1/var/lib/docker
  Labels:
   provider=virtualbox
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets create our first docker container on this host.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker run busybox sh
  Unable to find image &#39;busybox:latest&#39; locally
  latest: Pulling from library/busybox
  c00ef186408b: Pulling fs layer
  ac6a7980c6c2: Pulling fs layer
  ac6a7980c6c2: Verifying Checksum
  ac6a7980c6c2: Download complete
  c00ef186408b: Verifying Checksum
  c00ef186408b: Download complete
  c00ef186408b: Pull complete
  ac6a7980c6c2: Pull complete
  Digest: sha256:e4f93f6ed15a0cdd342f5aae387886fba0ab98af0a102da6276eaf24d6e6ade0
  Status: Downloaded newer image for busybox:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;i&gt; NOTE: If you are one of user, who works behind proxy, then you may face problem like, your newly created docker host may not communicate with &lt;a href=&#34;https://hub.docker.com/&#34;&gt;docker-hub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To Fix that, you need to pass few environment flags at creation time as below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine create -d virtualbox \
  --engine-env HTTP_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
  --engine-env HTTPS_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
  testhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;So now we learnt how to create a docker host and run docker containers using docker-machine.
But Docker-machine not only lets you to create independent hosts, but also can put these host in one cluster!&lt;/p&gt;

&lt;p&gt;Isn&amp;rsquo;t it interesting?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how we can build whole cluster of docker-hosts using docker-machine.&lt;/p&gt;

&lt;p&gt;Docker &lt;a href=&#34;https://www.docker.com/products/docker-swarm&#34;&gt;Swarm&lt;/a&gt; is native clustering solution for docker. Here with native means, Swarm understands and exports &lt;a href=&#34;https://docs.docker.com/swarm/swarm-api/&#34;&gt;all(almost)&lt;/a&gt; docker API&amp;rsquo;s. i.e. API for docker and docker swarm are same.
If one product/scripts works with docker, it will automatically work with swarm :)&lt;/p&gt;

&lt;h2 id=&#34;docker-swarm:fce0df6d9ab4aa14b09a0b333f488e03&#34;&gt;Docker Swarm&lt;/h2&gt;

&lt;p&gt;Docker swarm has three components.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Discovery Backend : Swarm requires a discovery backend, which is used by each node(agent) to discover the master.

&lt;ul&gt;
&lt;li&gt;Default discovery is provided by Docker Hub. (Not for production usage)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Master : Swarm master takes care of all scheduling logic and HA.&lt;/li&gt;
&lt;li&gt;Agent : These runs on each node and communicates with Swarm master.

&lt;ul&gt;
&lt;li&gt;All agent node must listen to the same network interface (TCP port).&lt;/li&gt;
&lt;li&gt;Each node runs a node agent that registers the referenced Docker daemon, monitors it, and updates the discovery backend with the nodeâ€™s status&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this demo, We will create a docker swarm consisting one Master and three agents including master.
Also, Consul will be used for discovery backend.&lt;/p&gt;

&lt;h3 id=&#34;discovery-backend-using-consul:fce0df6d9ab4aa14b09a0b333f488e03&#34;&gt;Discovery Backend using consul.&lt;/h3&gt;

&lt;p&gt;We will create a dedicated machine for consul, but running &lt;a href=&#34;https://www.consul.io/&#34;&gt;consul&lt;/a&gt; using docker is just one command task.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine create -d virtualbox \
     --engine-env HTTP_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
     --engine-env HTTPS_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
     swl-consul

$ docker $(docker-machine config swl-consul) run \
     -e &amp;quot;http_proxy=http://example.com:8080&amp;quot; \
     -e &amp;quot;https_proxy=http://example.com:8080&amp;quot; \
     --restart=&amp;quot;always&amp;quot; \
     -d -p &amp;quot;8500:8500&amp;quot; \
     -h &amp;quot;consul&amp;quot; \
     progrium/consul -server -bootstrap
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check if consul container is created as expected.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine ls
  NAME         ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER   ERRORS
  swl-consul   -        virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1

  $ eval $(docker-machine env swl-consul)

  $ docker ps
  CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                            NAMES
  5357e85abcb5        progrium/consul     &amp;quot;/bin/start -server -&amp;quot;   16 minutes ago      Up 16 minutes       53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 0.0.0.0:8500-&amp;gt;8500/tcp   goofy_aryabhata
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;creation-of-swarm-master:fce0df6d9ab4aa14b09a0b333f488e03&#34;&gt;Creation of Swarm Master.&lt;/h3&gt;

&lt;p&gt;In cluster, we will have one master and 2 agents. But master will also have agent, so practically we will have 3 agents.&lt;/p&gt;

&lt;p&gt;Docker machine helps to setup docker host with swarm in single command.&lt;/p&gt;

&lt;p&gt;Few highlights of docker machine provisioned swarm enabled host.
- Swarm master and agent runs as docker containers.
- Swarm manager bind on &amp;ldquo;3376&amp;rdquo; port, So to communicate with master &lt;code&gt;tcp://&amp;lt;IP:3376&amp;gt;&amp;gt;&lt;/code&gt; should be used.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine create \
  &amp;gt;    -d virtualbox \
  &amp;gt;    --swarm \
  &amp;gt;    --swarm-master \
  &amp;gt;    --swarm-discovery=consul://$(docker-machine ip swl-consul):8500 \
  &amp;gt;    --engine-env HTTP_PROXY=http://example.com:8080 \
  &amp;gt;    --engine-env HTTPS_PROXY=http://example.com:8080 \
  &amp;gt;    --engine-env NO_PROXY=192.168.99.100 \
  &amp;gt;    --engine-opt=&amp;quot;cluster-store=consul://$(docker-machine ip swl-consul):8500&amp;quot; \
  &amp;gt;    --engine-opt=&amp;quot;cluster-advertise=eth1:3376&amp;quot; \
  &amp;gt;    node1
  Running pre-create checks...
   .
   .
  (node1) Starting VM...
  Waiting for machine to be running, this may take a few minutes...
  Machine is running, waiting for SSH to be available...
  Detecting operating system of created instance...
  Detecting the provisioner...
  Provisioning with boot2docker...
  Copying certs to the local machine directory...
  Copying certs to the remote machine...
  Setting Docker configuration on the remote daemon...
  Configuring swarm...
  Checking connection to Docker...
  Docker is up and running!
  To see how to connect Docker to this machine, run: C:\Program Files\Docker Toolbox\docker-machine.exe env node1

  $ eval $(docker-machine env node1)

  $ docker ps
  CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
  7e644b71f606        swarm:latest        &amp;quot;/swarm join --advert&amp;quot;   19 seconds ago      Up 16 seconds                           swarm-agent
  58f2a4da438b        swarm:latest        &amp;quot;/swarm manage --tlsv&amp;quot;   22 seconds ago      Up 19 seconds                           swarm-agent-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;i&gt; NOTE: You may have seen &lt;code&gt;NO_PROXY&lt;/code&gt; is also set as environment variable.
This is to skip proxy for communicating with consul server &lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Here,
- &lt;code&gt;--swarm&lt;/code&gt; indicates, swarm agent will be configured.
- &lt;code&gt;--swarm-master&lt;/code&gt; indicates, swarm master will be configured.
&lt;code&gt;--swarm-discovery&lt;/code&gt; sets the backend discovery for swarm.&lt;/p&gt;

&lt;p&gt;Here you go, we have our swarm master node up and running. Before communicating to swarm, let me remind you again.
- Swarm and docker talks in same API signature. But on our host, we have both docker daemon and swarm running.
- Our docker client can communicate with both, but we need to choose whom it has to talk.
  - To talk with docker daemon use &lt;code&gt;eval $(docker-machine env &amp;lt;host-name&amp;gt;)&lt;/code&gt;
  - To talk with swarm use &lt;code&gt;eval $(docker-machine env --swarm &amp;lt;host-name&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;So, since we want our docker client should communicate with swarm, we will add &lt;code&gt;--swarm&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ eval $(docker-machine env --swarm node1)

  $ docker info
  Containers: 2
  Images: 1
  Role: primary
  Strategy: spread
  Filters: health, port, dependency, affinity, constraint
  Nodes: 1
   node1: 192.168.99.102:2376
    â”” Status: Healthy
    â”” Containers: 2
    â”” Reserved CPUs: 0 / 1
    â”” Reserved Memory: 0 B / 1.021 GiB
    â”” Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs
  CPUs: 1
  Total Memory: 1.021 GiB
  Name: node1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now add two nodes with only agent.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine create \
  &amp;gt;    -d virtualbox \
  &amp;gt;    --swarm \
  &amp;gt;    --swarm-discovery=&amp;quot;consul://$(docker-machine ip swl-consul):8500&amp;quot; \
  &amp;gt;    --engine-env HTTP_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
  &amp;gt;    --engine-env HTTPS_PROXY=&amp;quot;http://example.com:8080&amp;quot; \
  &amp;gt;    --engine-env NO_PROXY=&amp;quot;192.168.99.100&amp;quot; \
  &amp;gt;    --engine-opt=&amp;quot;cluster-store=consul://$(docker-machine ip swl-consul):8500&amp;quot; \
  &amp;gt;    --engine-opt=&amp;quot;cluster-advertise=eth1:3376&amp;quot; \  
  &amp;gt;    node3
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Command for adding agent doesn&amp;rsquo;t requires &lt;code&gt;--swarm-master&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cluster-advertise&lt;/code&gt; and &lt;code&gt;cluster-store&lt;/code&gt; are required for overlay networking also, Will be explained later.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So after adding two more machines with swarm agent, we have 4 machines running as below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker-machine ls
  NAME         ACTIVE   DRIVER       STATE     URL                         SWARM            DOCKER   ERRORS
  node1        -        virtualbox   Running   tcp://192.168.99.102:2376   node1 (master)   v1.9.1
  node2        -        virtualbox   Running   tcp://192.168.99.103:2376   node1            v1.9.1
  node3        -        virtualbox   Running   tcp://192.168.99.104:2376   node1            v1.9.1
  swl-consul   -        virtualbox   Running   tcp://192.168.99.100:2376                    v1.9.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets see information of our cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ eval $(docker-machine env --swarm node1)

  $ docker info
  Containers: 4
  Images: 3
  Role: primary
  Strategy: spread
  Filters: health, port, dependency, affinity, constraint
  Nodes: 3
   node1: 192.168.99.102:2376
    â”” Status: Healthy
    â”” Containers: 2
    â”” Reserved CPUs: 0 / 1
    â”” Reserved Memory: 0 B / 1.021 GiB
    â”” Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs
   node2: 192.168.99.103:2376
    â”” Status: Healthy
    â”” Containers: 1
    â”” Reserved CPUs: 0 / 1
    â”” Reserved Memory: 0 B / 1.021 GiB
    â”” Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs
   node3: 192.168.99.104:2376
    â”” Status: Healthy
    â”” Containers: 1
    â”” Reserved CPUs: 0 / 1
    â”” Reserved Memory: 0 B / 1.021 GiB
    â”” Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs
  CPUs: 3
  Total Memory: 3.064 GiB
  Name: node1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Superb! we have three nodes running in our cluster.
This completes the swarm setup!!&lt;/p&gt;

&lt;h2 id=&#34;understanding-few-more-swarm-functionality:fce0df6d9ab4aa14b09a0b333f488e03&#34;&gt;Understanding few more Swarm functionality.&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Strategy&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Currently &amp;ldquo;Spread&amp;rdquo; strategy is set. So the containers will be spread over all the hosts in cluster.&lt;/p&gt;

&lt;p&gt;e.g.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ docker run -d -P -m 1G -e MYSQL_ROOT_PASSWORD=test123 --name db mysql
  57a1af46d72117306b833a28a219d3523e1531c98a19ef25cba00a7bc94c9645

  $ docker run -d -P -m 1G --name frontend nginx
  f40776d7c9f7583965263340c1542a09eb7cb881c53bcf18a290df0d6ce2fd51

  $ docker ps
  CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                         NAMES
  f40776d7c9f7        nginx               &amp;quot;nginx -g &#39;daemon off&amp;quot;   4 seconds ago       Up 4 seconds        192.168.99.103:32770-&amp;gt;80/tcp, 192.168.99.103:32769-&amp;gt;443/tcp   node2/frontend
  57a1af46d721        mysql               &amp;quot;/entrypoint.sh mysql&amp;quot;   3 minutes ago       Up 3 minutes        192.168.99.104:32769-&amp;gt;3306/tcp                                node3/db
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Here node3 have &lt;code&gt;db&lt;/code&gt; and node2 have &lt;code&gt;frontend&lt;/code&gt; container.&lt;/li&gt;
&lt;li&gt;If two nodes have the same amount of available RAM and CPUs, the spread strategy prefers the node with least containers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other available strategies are BinPack and Random.
- BinPack tries to pack all possible containers on single node first and so on.
- If two nodes have the same amount of available RAM and CPUs, the binpack strategy prefers the node with most containers.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Things to Try:
- Create swarm with BinPack and create multiple containers to see the behavior. &lt;/i&gt;&lt;/p&gt;

&lt;h3 id=&#34;filters:fce0df6d9ab4aa14b09a0b333f488e03&#34;&gt;Filters&lt;/h3&gt;

&lt;p&gt;Filters tell Docker Swarm scheduler which nodes to use when creating and running a container.&lt;/p&gt;

&lt;p&gt;Filters are divided into two categories, node filters and container configuration filters.
- Node filters operate on characteristics of the Docker host or on the configuration of the Docker daemon.
- Container configuration filters operate on characteristics of containers, or on the availability of images on a host.
- Each filter has a name that identifies it.&lt;/p&gt;

&lt;p&gt;The node filters are:
  - constraint
  - health&lt;/p&gt;

&lt;p&gt;The container configuration filters are:
  - affinity
  - dependency
  - port&lt;/p&gt;

&lt;p&gt;When you start a Swarm manager with the swarm manage command, all the filters are enabled.
If you want to limit the filters available to your Swarm, specify a subset of filters by passing the &lt;code&gt;--filter&lt;/code&gt; flag and the name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm manage --filter=health --filter=dependency
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In case of docker machine, while provisioning &lt;code&gt;--swarm-opt&lt;/code&gt; can be used to set filters.&lt;/p&gt;

&lt;h4 id=&#34;use-a-constraint-filter:fce0df6d9ab4aa14b09a0b333f488e03&#34;&gt;Use a constraint filter&lt;/h4&gt;

&lt;p&gt;Node constraints can refer to Dockerâ€™s default tags or to custom labels. Default tags are sourced from docker info. Often, they relate to properties of the Docker host. Currently, the dafult tags include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;node to refer to the node by ID or name&lt;/li&gt;
&lt;li&gt;storagedriver&lt;/li&gt;
&lt;li&gt;executiondriver&lt;/li&gt;
&lt;li&gt;kernelversion&lt;/li&gt;
&lt;li&gt;operatingsystem&lt;/li&gt;
&lt;li&gt;Custom node labels can be applied while provisioning docker machine.

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--engine-label&lt;/code&gt; can be used for setting custom label.&lt;/li&gt;
&lt;li&gt;custom label like &lt;code&gt;environment&lt;/code&gt;, &lt;code&gt;storage&lt;/code&gt; etc are helpful.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since we have all labels same except node name, we will try creating new container using node-name constraint.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -itd -e constraint:node==node3 --name test1 ubuntu
ae4013d014931e43cd5342a625f6b549a8c920cb146b1371a7226359a4bcf014

$ docker run -itd -e constraint:node==node3 --name test2 ubuntu
a6b371773e1a2609122ca68df00d1e03ee274ba3db504d3942c301f8e2c45504

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
a6b371773e1a        ubuntu              &amp;quot;/bin/bash&amp;quot;         3 seconds ago       Up 2 seconds                            node3/test2
ae4013d01493        ubuntu              &amp;quot;/bin/bash&amp;quot;         19 seconds ago      Up 18 seconds                           node3/test1
934501aed30f        ubuntu              &amp;quot;/bin/bash&amp;quot;         19 hours ago        Up 2 hours                              node3/n4
30358292be89        ubuntu              &amp;quot;/bin/bash&amp;quot;         19 hours ago        Up 19 hours                             node1/n3
6a13aa3eca8b        ubuntu              &amp;quot;/bin/bash&amp;quot;         21 hours ago        Up 21 hours                             node2/n1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similarly other filters can be used. While using filters the syntax to use in &lt;code&gt;docker run&lt;/code&gt; is as below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- ``-e &amp;lt;filter-name&amp;gt;:&amp;lt;key&amp;gt;&amp;lt;operator&amp;gt;&amp;lt;value&amp;gt;``
- Here operator used are  ``==`` , ``!=`` &amp;amp; ``~``
- For ``==`` &amp;amp; ``!=``, exact match is found.
  - If nothing satisfies the condition, it does schedule container.
- ``~`` is for soft condition. i.e. if nothing matches, it ignores the condition and use default scheduler.
- Here value can contain any valid regex (https://github.com/google/re2/wiki/Syntax)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hope this article will be helpful in getting started with docker-machine and swarm :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Golang Interfaces</title>
      <link>http://kunalkushwaha.github.io/2015/09/11/understanding-golang-interfaces/</link>
      <pubDate>Fri, 11 Sep 2015 15:14:46 +0900</pubDate>
      
      <guid>http://kunalkushwaha.github.io/2015/09/11/understanding-golang-interfaces/</guid>
      <description>

&lt;p&gt;Even after writing Go code for a while, there have been couple of time, when I get confused about Interfaces in golang.
So I think, it may help people, who have started with Golang, or don&amp;rsquo;t use much Interfaces in there code.&lt;/p&gt;

&lt;h3 id=&#34;interfaces:8e8f150f0261927c23fb2a377a40072b&#34;&gt;Interfaces&lt;/h3&gt;

&lt;p&gt;By definition, Interfaces are named collections of method signatures. But usage of Interfaces in go is little confusing as it is used a in context of data type also and defining the behavior of methods also.&lt;/p&gt;

&lt;p&gt;Lets try to understand each of them.&lt;/p&gt;

&lt;h3 id=&#34;1-interface-interface-as-data-type:8e8f150f0261927c23fb2a377a40072b&#34;&gt;1. interface{} , interface as data-type&lt;/h3&gt;

&lt;p&gt;This is commonly mistaken as ( void type of C/C++), But it&amp;rsquo;s not! It still holds its property from the above definition.
Since, &lt;code&gt;interface{}&lt;/code&gt; is empty interface i.e. No methods associated with it, all data type satisfy the behavior and hence, all data types can be passed/assigned to &lt;code&gt;interface{}&lt;/code&gt; type.&lt;/p&gt;

&lt;p&gt;Lets take a example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;
type Stack struct {
       data []interface{}
}

func NewStack() *Stack {
	s := Stack{data: make([]interface{},0)}
	return &amp;amp;s
}

func (s *Stack) Push(data interface{}) {
	s.data = append(s.data, data)
}

func (s *Stack) Pop() interface{} {
	if len(s.data) == 0 {
		return nil
	}
	data := s.data[len(s.data) -1]
	s.data = s.data[0:len(s.data)-1]
	return data
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To use the above stack, a simple example shows retrieval of values from interface type.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;s := NewStack()
s.Push(10)
s.Push(&amp;quot;Hello&amp;quot;)
obj1 := s.Pop()
obj2 = s.Pop()
// To retrieve int value, use type assertion.
elem1 := obj.(string)
elem2 := obj.(int)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here are few things needs to be understood about interfaces to be used as type.
 - Whenever variables of any datatype is assigned to interface type, it is converted into interface type and stored.
     - So properties of original data-type cannot be retrieved until, it is converted again back to original data-type.
 - conversion to data-type from interfaces cannot be achieved using typecasting, Here it required, &lt;a href=&#34;http://golang.org/ref/spec#Type_assertions&#34;&gt;type assertion&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So use interface{}, wherever you wish to add generic data-type and remember at retrieval time, use type assertion!&lt;/p&gt;

&lt;h3 id=&#34;2-interface-as-set-of-methods:8e8f150f0261927c23fb2a377a40072b&#34;&gt;2. Interface as set of methods.&lt;/h3&gt;

&lt;p&gt;Unlike above type, here interface have some function prototype.&lt;/p&gt;

&lt;p&gt;If you come from &amp;ldquo;C/C++&amp;rdquo; background, you can relate it with function pointers or abstract functions/pure virtual functions. These functions don&amp;rsquo;t have any implementation.&lt;/p&gt;

&lt;p&gt;But, to implement these functions, no need to inherit in structs or assign to function pointers.&lt;/p&gt;

&lt;p&gt;In Go, if some structs or functions have same behavior as of interface, it automatically can be used in interface. Here behavior mean, all functions in interface should be implemented with same signature of functions as of interface.&lt;/p&gt;

&lt;p&gt;Lets take a simple example.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;type Plugin interface {
    Execute(string, int) bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, a Plugin interface is defined with one function. Any Structure, which have this one function, will automatically implements the interface.&lt;/p&gt;

&lt;p&gt;i.e. Any structure, which has same behavior means the Plugin interface can represent that structure. No explicitly need to show via any keyword like implement or inherit.&lt;/p&gt;

&lt;p&gt;I feel, it is very simple approach to build OOP design. Such simple approaches, to create design patterns, make Go even more interesting.&lt;/p&gt;

&lt;p&gt;Lets see examples which explains the behavior of interfaces&lt;/p&gt;

&lt;h3 id=&#34;1-example-1:8e8f150f0261927c23fb2a377a40072b&#34;&gt;1. Example #1&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt; type aPlugin struct {
     name string
 }

 func (p aPlugin) Execute(name string, count int) bool {
     fmt.Println(&amp;quot;aPlugin executing&amp;quot;, name, count)
     return 0
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, structure &lt;code&gt;aPlugin&lt;/code&gt; have exactly same function as of &lt;code&gt;Plugin&lt;/code&gt; interface, so this &lt;code&gt;aPlugin&lt;/code&gt; can be assigned to Plugin interface.&lt;/p&gt;

&lt;p&gt;So main function will look like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt; func main() {
    // Create a Plugin array
    obj := []Plugin{aPlugin{}, bPlugin{}}
    obj[0].Execute(&amp;quot;Hello&amp;quot;, 1)
    obj[1].Execute(&amp;quot;World&amp;quot;, 2)
 }

 $ go run main.go
 aPlugin executing Hello 1
 bPLugin executing World 2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-example-2:8e8f150f0261927c23fb2a377a40072b&#34;&gt;2. Example #2&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt; type bPlugin struct {
     name string
 }

 func (p bPlugin) Execute(name string, count int) bool {
     fmt.Println(&amp;quot;bPlugin executing&amp;quot;, name, count)
     return 0
 }

 func (p bPlugin) DumpInfo() {
     fmt.Println(p.name)
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, another structure &lt;code&gt;bPlugin&lt;/code&gt; have one additional function then of &lt;code&gt;Plugin&lt;/code&gt; interface. So this &lt;code&gt;bPlugin&lt;/code&gt; can be assigned to Plugin interface.&lt;/p&gt;

&lt;p&gt;Only, if it is assigned to Plugin object, the additional function DumpInfo() cannot be invoked from Plugin object.
    - So a Structure can have additional functions, data members and still it can assigned to interface object.&lt;/p&gt;

&lt;h3 id=&#34;3-example-3:8e8f150f0261927c23fb2a377a40072b&#34;&gt;3. Example #3&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt; type cPlugin struct {
     name string
 }

 func (p cPlugin) Execute() bool {
     fmt.Println(&amp;quot;cPlugin executing&amp;quot;)
     return 0
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, structure &lt;code&gt;cPlugin&lt;/code&gt; have function with same name &lt;code&gt;Execute&lt;/code&gt;, but with different signatures. So cPlugin object cannot be assigned to &lt;code&gt;Plugin&lt;/code&gt; object.&lt;/p&gt;

&lt;p&gt;go compiler will raise error if it is tried to do so.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-golang&#34;&gt;obj := []Plugin{aPlugin{}, bPlugin{}, cPlugin{}}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt; $ go run main.go
 ./main.go:22: cannot use cPlugin literal (type cPlugin) as type Plugin in array element:
        cPlugin does not implement Plugin (wrong type for Execute method)
                have Execute() bool
                want Execute(string, int) bool

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;how-should-these-interfaces-should-be-used:8e8f150f0261927c23fb2a377a40072b&#34;&gt;How should these interfaces should be used.&lt;/h2&gt;

&lt;p&gt;Interfaces are one of most important building blocks of golang. Also, its usage make golang as language very simpler to use.
for example, you can look into &lt;a href=&#34;http://golang.org/src/io/io.go&#34;&gt;&amp;ldquo;io&amp;rdquo; package&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you implement any kind of framework or as simple as muti-platform program, where you wish functionalities, differ for each OS, interfaces are perfect candidates to use.&lt;/p&gt;

&lt;p&gt;I hope this blog would have helped to understand the interfaces in Golang.
Please comment in case you have any feedback.&lt;/p&gt;

&lt;p&gt;Happy Coding :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conditional compilation in golang</title>
      <link>http://kunalkushwaha.github.io/2015/07/24/conditional-compilation-in-golang/</link>
      <pubDate>Fri, 24 Jul 2015 22:21:27 +0900</pubDate>
      
      <guid>http://kunalkushwaha.github.io/2015/07/24/conditional-compilation-in-golang/</guid>
      <description>

&lt;h2 id=&#34;conditional-compiling-in-golang:2d62b9b37d02354b93f9e4e4f4d7ca80&#34;&gt;Conditional compiling in golang.&lt;/h2&gt;

&lt;p&gt;While hacking around experimental builds of docker, I learned about conditional compiling in golang.
As any project grew or ported to multiple architecture/platforms  the need of conditional compiling is obvious.&lt;/p&gt;

&lt;p&gt;Unlike C golang, do not have macros. Golang make use of build tags to achieve this.&lt;/p&gt;

&lt;h4 id=&#34;build-tags:2d62b9b37d02354b93f9e4e4f4d7ca80&#34;&gt;Build Tags:&lt;/h4&gt;

&lt;p&gt;Build tags are basically annotation in source code. generally it is first line of file, where build tags are defined.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;// +build &amp;lt;tags&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;These tags are searched by &lt;code&gt;go build&lt;/code&gt; tool before passing to go compiler. These tags can be passed while go build command like&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ go build -tags &amp;lt;tags&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Here are few points to remember regarding tags.
- Tags specifies architecture and platforms are special. i.e. No need to pass such tags, &lt;code&gt;go build&lt;/code&gt; is smart enough to pass them.
    - e.g. linux, windows, darwin, 386, arch etc
- Tags can be passed with negation i.e. !&lt;tag&gt;
- A blank line must be present between build tags and code, else that line will be ignored.
- OR condition for tags can be specified with space.
- AND condition can be specified with &lt;code&gt;,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If some tag is defined in file and is not passed in &lt;code&gt;go build&lt;/code&gt;` command, it will be ignored.&lt;/p&gt;

&lt;p&gt;Lets see this with example with custom tag.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Here we will take three files&lt;/li&gt;
&lt;li&gt;main.go&lt;/li&gt;
&lt;li&gt;include.go&lt;/li&gt;
&lt;li&gt;exclude.go&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;â—‹ cat main.go
package main

import (
&amp;quot;fmt&amp;quot;
)

func init() {
        fmt.Println(&amp;quot;Hello from Main init()&amp;quot;)
}


func main() {
        fmt.Println(&amp;quot;Hello from main&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;
â—‹ cat include.go

package main

import &amp;quot;fmt&amp;quot;

func init() {
        fmt.Println(&amp;quot;Hello from IncludeInit()&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;â—‹ cat exclude.go
// +build exclude

package main

import &amp;quot;fmt&amp;quot;

func init(){
        fmt.Println(&amp;quot;Hello from exclude&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, here exclude file has build tag of &lt;code&gt;exclude&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now if do &lt;code&gt;go build&lt;/code&gt;, It will not compile exclude.go.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;â—‹ go build

â—‹ ./temp
Hello from IncludeInit()
Hello from Main init()
Hello from main
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let try using buildtag exclude.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;â—‹ go build -tags exclude

â—‹ ./temp
Hello from exclude
Hello from IncludeInit()
Hello from Main init()
Hello from main
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see now, the exclude.go is part of binary.&lt;/p&gt;

&lt;p&gt;One more interesting thing with golang is &lt;code&gt;go list&lt;/code&gt; tool.
Using the &lt;code&gt;go list&lt;/code&gt;, without compiling, you can see which files will be compiled, with given gotags or default.&lt;/p&gt;

&lt;p&gt;I found &lt;code&gt;go list --help&lt;/code&gt; not great, but the summary of this tool is a go template can be used to check the list of files that will be included in the build.
A simple example as below will help you to understand better.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;â—‹ go list -f &#39;{{.GoFiles}}&#39; --tags exclude
[exclude.go include.go main.go]

â—‹ go list -f &#39;{{.GoFiles}}&#39;
[include.go main.go]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, if exclude tag is not used, only include.go and main.go files are used.&lt;/p&gt;

&lt;h4 id=&#34;so-it-that-all-about-conditional-compiling-in-golang:2d62b9b37d02354b93f9e4e4f4d7ca80&#34;&gt;So it that all about conditional compiling in golang?&lt;/h4&gt;

&lt;p&gt;No, There is even simpler method available too :-) and it is file-name suffixes.&lt;/p&gt;

&lt;p&gt;In golang, there are few go environment variables defined.
GOOS and GOARCH are two of such.
- GOOS defines the current OS
- GOARCH defines the current machine architecture.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;go build&lt;/code&gt; matches value of these two environment variables in filename and if it matches then only include for compiling.&lt;/p&gt;

&lt;p&gt;So by naming files like &lt;code&gt;exclude_linux.go&lt;/code&gt;, it will be included only if compiled in linux system.
&lt;code&gt;exclude_linux_amd64.go&lt;/code&gt; will ensure, it will be inlcuded only if OS is linux and architecture of system in 64 bit.&lt;/p&gt;

&lt;p&gt;You may try these combination.&lt;/p&gt;

&lt;h4 id=&#34;which-one-should-be-used:2d62b9b37d02354b93f9e4e4f4d7ca80&#34;&gt;Which one should be used?&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;For custom tags build tags are the only options.&lt;/li&gt;
&lt;li&gt;For platform and architecture specific, if you plan to use cross build on one systems, build tags will be preferable.&lt;/li&gt;
&lt;li&gt;Also, if same file does work for multiple platforms, build tags will be better choice.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For rest, file sufixes can be used.&lt;/p&gt;

&lt;p&gt;Hope this info will be useful.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Container Virtualization and its building blocks</title>
      <link>http://kunalkushwaha.github.io/2015/07/04/container-virtualization/</link>
      <pubDate>Sat, 04 Jul 2015 20:25:44 +0900</pubDate>
      
      <guid>http://kunalkushwaha.github.io/2015/07/04/container-virtualization/</guid>
      <description>

&lt;p&gt;Since 2014, Linux containers have become buzz word in Cloud Infrastructure. Almost all, from Big corporations to startups, all have started using it. Huge credit goes to &lt;a href=&#34;www.docker.com&#34;&gt;Docker&lt;/a&gt; for making using containers so easy to use.&lt;/p&gt;

&lt;p&gt;Linux Containers are there in Linux systems for alomst decade old, But making them work, was not so easy, and generally required linux admin experts for doing same. Few Solution as linux containers like FreeBSD Jails, LXC, openVZ, Solaris Zones etc exists for quite some time.&lt;/p&gt;

&lt;p&gt;These are also known as OS level Virtualization.
To understand other type of virtualization please read &lt;a href=&#34;http://kunalkushwaha.github.io/post/Layman-guide-to-Platform-Virtualization&#34;&gt;Layman guide to Platform virtualization&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;operating-system-level-virtualization:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;Operating System level Virtualization&lt;/h2&gt;

&lt;p&gt;Quoting below from Wikipedia, I it explains beautifully in technical and yet not too complex.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;OS level Virtualization is a server virtualization method where the kernel of an operating system allows for multiple isolated user space instances, instead of just one. Such instances (often called containers, virtualization engines (VE), virtual private servers (VPS), or jails) may look and feel like a real server from the point of view of its owners and users.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In simple words, it allows to run multiple rootfs (user-space) simultaneously and all running rootfs have their own view of filesystem and devices. So they are not aware of each others and resource usage can be configured.&lt;/p&gt;

&lt;p&gt;Sounds similar to virtual Machines? Yes it is!&lt;/p&gt;

&lt;h4 id=&#34;how-this-isolation-is-achieved:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;How this isolation is achieved?&lt;/h4&gt;

&lt;p&gt;This isolation is achieved using linux features like namespaces, cgroups and chroot. To understand details we need to first understand each of them.&lt;/p&gt;

&lt;h4 id=&#34;namespaces:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;Namespaces&lt;/h4&gt;

&lt;p&gt;Namespace wraps a particular global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource.&lt;/p&gt;

&lt;p&gt;Currently, Linux implements six different types of namespaces&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;h5 id=&#34;mount-namespaces-clone-newns:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;Mount namespaces (CLONE_NEWNS)&lt;/h5&gt;

&lt;p&gt;This isolate the set of filesystem mount points seen by a group of processes. Thus, processes in different mount namespaces can have different views of the filesystem hierarchy. With the addition of mount namespaces, the mount() and umount() system calls ceased operating on a global set of mount points visible to all processes on the system and instead performed operations that affected just the mount namespace associated with the calling process.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This is also an alternative to chroot system call.&lt;/li&gt;
&lt;li&gt;This is supported since Linux 2.4.19&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;h5 id=&#34;uts-namespaces-clone-newuts:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;UTS namespaces (CLONE_NEWUTS)&lt;/h5&gt;

&lt;p&gt;This isolate two system identifiersâ€”nodename and domainnameâ€”returned by the uname() system call; the names are set using the sethostname() and setdomainname() system calls.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the context of containers, the UTS namespaces feature allows each container to have its own hostname and NIS domain name. This can be useful for initialization and configuration scripts that tailor their actions based on these names.&lt;/li&gt;
&lt;li&gt;This is supported in Linux kernel since  Linux 2.6.19.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;h5 id=&#34;ipc-namespaces-clone-newipc:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;IPC namespaces (CLONE_NEWIPC)&lt;/h5&gt;

&lt;p&gt;This isolate certain interprocess communication (IPC) resources, namely, System V IPC objects and (since Linux 2.6.30) POSIX message queues.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The common characteristic of these IPC mechanisms is that IPC objects are identified by mechanisms other than filesystem pathnames. Each IPC namespace has its own set of System V IPC identifiers and its own POSIX message queue filesystem.&lt;/li&gt;
&lt;li&gt;This is supported in Linux kernel since Linux 2.6.19&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;h5 id=&#34;pid-namespaces-clone-newpid:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;PID namespaces (CLONE_NEWPID)&lt;/h5&gt;

&lt;p&gt;This isolate the process ID number space. In other words, processes in different PID namespaces can have the same PID.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This helps migrating containers between hosts while keeping the same process IDs for the processes inside the container.&lt;/li&gt;
&lt;li&gt;PID namespaces also allow each container to have its own init (PID 1), the &amp;ldquo;ancestor of all processes&amp;rdquo; that manages various system initialization tasks and reaps orphaned child processes when they terminate.&lt;/li&gt;
&lt;li&gt;This is supported since Linux 2.6.24&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;h5 id=&#34;network-namespaces-clone-newnet:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;Network namespaces (CLONE_NEWNET)&lt;/h5&gt;

&lt;p&gt;This provide isolation of the system resources associated with networking. Thus, each network namespace has its own network devices, IP addresses, IP routing tables, /proc/net directory, port numbers, and so on.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Network namespaces make containers useful from a networking perspective: each container can have its own (virtual) network device and its own applications that bind to the per-namespace port number space.&lt;/li&gt;
&lt;li&gt;started in Linux 2.4.19 2.6.24 and largely completed by about Linux 2.6.29&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;h5 id=&#34;user-namespaces-clone-newuser:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;User namespaces (CLONE_NEWUSER)&lt;/h5&gt;

&lt;p&gt;This isolate the user and group ID number spaces. In other words, a process&amp;rsquo;s user and group IDs can be different inside and outside a user namespace.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The most interesting case here is that a process can have a normal unprivileged user ID outside a user namespace while at the same time having a user ID of 0 inside the namespace. This means that the process has full root privileges for operations inside the user namespace, but is unprivileged for operations outside the namespace.&lt;/li&gt;
&lt;li&gt;This was partially supported since Linux 2.6.23 and completed in Linux 3.8.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;control-groups-a-k-a-cgroups:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;Control groups a.k.a. cgroups&lt;/h4&gt;

&lt;p&gt;Cgroups allow you to allocate resourcesâ€”such as CPU time, system memory, network bandwidth, or combinations of these resourcesâ€”among user-defined groups of tasks (processes) running on a system.
  - One can configure cgroups, deny cgroups access to certain resources, and even reconfigure cgroups dynamically on a running system.
  - The cgconfig (control group config) service can be configured to start up at boot time and reestablish your predefined cgroups, thus making them persistent across reboots.
  - By using cgroups, we gain fine-grained control over allocating, prioritizing, denying, managing, and monitoring system resources. Hardware resources can be appropriately divided up among tasks and users, increasing overall efficiency.
  - These are like process, hierarchical in nature i.e. child cgroups inherit certain attributes from their parent cgroup.&lt;/p&gt;

&lt;p&gt;Follwing resources are supported currently in cgroups.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;blkio â€” this subsystem sets limits on input/output access to and from block devices such as physical drives (disk, solid state, USB, etc.).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;cpu â€” this subsystem uses the scheduler to provide cgroup tasks access to the CPU.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;cpuacct â€” this subsystem generates automatic reports on CPU resources used by tasks in a cgroup.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;cpuset â€” this subsystem assigns individual CPUs (on a multicore system) and memory nodes to tasks in a cgroup.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;devices â€” this subsystem allows or denies access to devices by tasks in a cgroup.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;freezer â€” this subsystem suspends or resumes tasks in a cgroup.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;memory â€” this subsystem sets limits on memory use by tasks in a cgroup, and generates automatic reports on memory resources used by those tasks.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;net_cls â€” this subsystem tags network packets with a class identifier (classid) that allows the Linux traffic controller (tc) to identify packets originating from a particular cgroup task.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;net_prio â€” this subsystem provides a way to dynamically set the priority of network traffic per network interface.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ns â€” the namespace subsystem.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For details you may refer : &lt;a href=&#34;https://www.kernel.org/doc/Documentation/cgroups/cgroups.txt&#34;&gt;https://www.kernel.org/doc/Documentation/cgroups/cgroups.txt&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;so-how-these-helps-in-containers:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;So how these helps in containers?&lt;/h2&gt;

&lt;p&gt;By now, you must have understood, namespaces and cgroups help to create isolated environment.
- Namespaces provides isolation of filesystem view, devices, network and processes.
- Cgroups helps to allocate, devices accessibility, and allocate quota to use the devices.&lt;/p&gt;

&lt;p&gt;&lt;img align=&#34;center&#34; src=http://kunalkushwaha.github.io/images/containers.png&gt;&lt;/p&gt;

&lt;h2 id=&#34;is-this-all-sufficient-for-virtualization:b9f02760c1e2afe4297fc7674428ef9f&#34;&gt;Is this all sufficient for Virtualization?&lt;/h2&gt;

&lt;p&gt;No, still security is left. To create secure containers features like Capablities, secomp , SELinux and Apparmor are used for that. These all are integrated with new container solutions like Docker, CoreOS rocket etc.&lt;/p&gt;

&lt;p&gt;Few Container projects worth wating are
- LXC - Linux containers : This is default container hypervisior on all linux based systems. &lt;a href=&#34;https://linuxcontainers.org/&#34;&gt;https://linuxcontainers.org/&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Docker libcontainer:
This is now donated by Docker to Linux foundation and new name will is OpenContainers. &lt;a href=&#34;http://www.opencontainers.org/&#34;&gt;http://www.opencontainers.org/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CoreOS - Rocket - &lt;a href=&#34;https://github.com/coreos/rkt&#34;&gt;https://github.com/coreos/rkt&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Redhat&amp;rsquo;s systemd-nspawn. &lt;a href=&#34;http://www.freedesktop.org/software/systemd/man/systemd-nspawn.html&#34;&gt;http://www.freedesktop.org/software/systemd/man/systemd-nspawn.html&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope this blog would have help you to understand Linux Containers.&lt;/p&gt;

&lt;p&gt;I will be writing more on current status of linux containers projects in my next blog so stay tuned :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Golang Development Environment</title>
      <link>http://kunalkushwaha.github.io/2015/04/27/golang-dev-environment/</link>
      <pubDate>Mon, 27 Apr 2015 22:29:36 +0900</pubDate>
      
      <guid>http://kunalkushwaha.github.io/2015/04/27/golang-dev-environment/</guid>
      <description>

&lt;p&gt;Setting up golang environment is quite simple good docs are already present in golang.org.
&lt;br&gt;But I couldn&amp;rsquo;t find any simple doc, where complete setup with GOROOT and GOPATH along with github is explained.&lt;/p&gt;

&lt;p&gt;So I thought it might be helpful to others too.
&lt;br&gt;My dev environment is ubuntu based &lt;a href=&#34;https://elementary.io/&#34;&gt;ElementryOS &amp;ldquo;Freya&amp;rdquo;&lt;/a&gt;, So it would work on all Ubuntu based distros.&lt;/p&gt;

&lt;h3 id=&#34;golang-installer:22b90498ad19ed8bfadbfc3c440b6be8&#34;&gt;Golang Installer&lt;/h3&gt;

&lt;p&gt;Golang comes with single tar file setup can be downloaded from &lt;a href=&#34;https://golang.org/dl/&#34;&gt;here&lt;/a&gt;
&lt;br&gt;extract the tar file to your /usr/local using below command&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;tar -C /usr/local -xzf go$VERSION.$OS-$ARCH.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;this will install your all go binaries under following dir structure.&lt;/p&gt;

&lt;p&gt;base folder where go is installed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/usr/local/go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All standard go binaries are in&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/usr/local/go/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;golang-environment-setup:22b90498ad19ed8bfadbfc3c440b6be8&#34;&gt;Golang environment setup.&lt;/h3&gt;

&lt;p&gt;Now next step is to define golang related environment variable.
&lt;br&gt;There are two important golang Env variables that need to be defined.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GOROOT : This variable have value, where golang is installed.&lt;/li&gt;
&lt;li&gt;GOPATH : This is like workspace. This folder will be root of all go getable packages and projects.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br&gt;So you need to create a folder for GOPATH. My fav is ~/go folder.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kunal@kunal-Aspire-5670:~$ mkdir ~/go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now define GOROOT, GOPATH and append PATH variables.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export GOROOT=/usr/local/go
export GOPATH=$HOME/go
export PATH=$PATH:$GOROOT/bin:$GOPATH/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Its better to append your ~/.profile file with these 3 lines. So no need to export these variables every time you restart machine.&lt;/p&gt;

&lt;p&gt;After setting environment variables check if everything is set as per expected or not!
&lt;br&gt;Use &amp;ldquo;go env&amp;rdquo; command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kunal@kunal-Aspire-5670:~$ go env
GOARCH=&amp;quot;386&amp;quot;
GOBIN=&amp;quot;&amp;quot;
GOCHAR=&amp;quot;8&amp;quot;
GOEXE=&amp;quot;&amp;quot;
GOHOSTARCH=&amp;quot;386&amp;quot;
GOHOSTOS=&amp;quot;linux&amp;quot;
GOOS=&amp;quot;linux&amp;quot;
GOPATH=&amp;quot;/home/kunal/go&amp;quot;
GORACE=&amp;quot;&amp;quot;
GOROOT=&amp;quot;/usr/local/go&amp;quot;
GOTOOLDIR=&amp;quot;/usr/local/go/pkg/tool/linux_386&amp;quot;
CC=&amp;quot;gcc&amp;quot;
GOGCCFLAGS=&amp;quot;-fPIC -m32 -pthread -fmessage-length=0&amp;quot;
CXX=&amp;quot;g++&amp;quot;
CGO_ENABLED=&amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;verify GOPATH and GOROOT.&lt;/p&gt;

&lt;h3 id=&#34;go-getable:22b90498ad19ed8bfadbfc3c440b6be8&#34;&gt;go getable.&lt;/h3&gt;

&lt;p&gt;One of beauty of golang is &amp;ldquo;go get&amp;rdquo; command.
&lt;br&gt;It automatically downloads the all required packages along with your program from git repo or mercurial.&lt;/p&gt;

&lt;p&gt;Prerequisite to this is you should install git and mercurial both on your machine.
mercurial is required as lot of official libraries of golang is still hosted at google code and it hosted with mercurial.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kunal@kunal-Aspire-5670:~$ sudo apt-get install mercurial
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we are set with all basics, now ready for go getable your code!&lt;/p&gt;

&lt;p&gt;after your go get github.com/&lt;some-go-project&gt;, You will see your ~/go folder have sum folder structure created.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kunal@kunal-Aspire-5670:~$ ll go
total 32
drwxrwxr-x  6 kunal kunal  4096 Apr 20 20:28 ./
drwx------ 24 kunal kunal 12288 Apr 27 22:45 ../
drwxrwxr-x  2 kunal kunal  4096 Apr 16 03:27 bin/
drwxrwxr-x  3 kunal kunal  4096 Apr 12 21:26 pkg/
drwxrwxr-x  8 kunal kunal  4096 Apr 16 03:24 src/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are folder which will have all your go getable code/packages and binaries.
*bin : folder will have binaries build after you build go project using &amp;ldquo;go build&amp;rdquo;
*pkg : this will have all pakages downloaded due to dependencies of your program. These packages compiled packages.
*src : this will have your source code under folder of source of code like github.com , bitbucket.com, code.google.com etc
e.g&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kunal@kunal-Aspire-5670:~$ ll go/src/
total 32
drwxrwxr-x  8 kunal kunal 4096 Apr 16 03:24 ./
drwxrwxr-x  6 kunal kunal 4096 Apr 20 20:28 ../
drwxrwxr-x  3 kunal kunal 4096 Apr 16 03:24 bitbucket.org/
drwxrwxr-x  3 kunal kunal 4096 Apr 12 21:28 code.google.com/
drwxrwxr-x 38 kunal kunal 4096 Apr 20 20:05 github.com/
drwxrwxr-x  3 kunal kunal 4096 Apr 12 21:29 golang.org/
drwxrwxr-x  3 kunal kunal 4096 Apr 12 21:29 google.golang.org/
drwxrwxr-x  6 kunal kunal 4096 Apr 16 03:26 gopkg.in/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So you are now all set for code/test/ship in golang. Happy Coding :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Layman guide to Platform Virtualization</title>
      <link>http://kunalkushwaha.github.io/2015/04/25/layman-guide-to-platform-virtualization/</link>
      <pubDate>Sat, 25 Apr 2015 23:20:04 +0900</pubDate>
      
      <guid>http://kunalkushwaha.github.io/2015/04/25/layman-guide-to-platform-virtualization/</guid>
      <description>

&lt;p&gt;&lt;img align=&#34;center&#34; src=http://kunalkushwaha.github.io/VirtualMachineCartoon.jpg&gt;&lt;/p&gt;

&lt;h3 id=&#34;platform-virtualization:a88864bb2e02cbfffb44963e73e3622a&#34;&gt;Platform Virtualization!&lt;/h3&gt;

&lt;p&gt;Virtualization is not new in Computer World today.
But when I am asked what I am working on, I say &amp;ldquo;Cloud Infrastructure and trying to build a private Cloud Platform!&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Explaining cloud service is easy but Cloud Infrastructure becomes little difficult.
So I am writing a blog series to explain Cloud Infrastructure and it building blocks.
Hope it will help many others too :)&lt;/p&gt;

&lt;h3 id=&#34;virtualization:a88864bb2e02cbfffb44963e73e3622a&#34;&gt;Virtualization.&lt;/h3&gt;

&lt;p&gt;Though simple meaning of Virtualization is creating virtual version of anything.
But in computer software world everything is already Virtual :).
But computer hardware devices, Network cable etc are real!&lt;/p&gt;

&lt;p&gt;When all these things are created virtually i.e. software simulation of all such hardware its called virtualization!&lt;/p&gt;

&lt;p&gt;But why that is required? I heard Hardware is becoming cheap day by day :-|
These are genuine doubts! indeed hardware is becoming cheap and also performance of capacity wise is improving at much faster rate then ever.&lt;/p&gt;

&lt;p&gt;Also, with increase of hardware capacity, most of hardware are underutilized :O
Yes and this becomes huge concern for business owner, How to utilize best of your hardware.
Second Management of hardware resources is also a addition of cost.&lt;/p&gt;

&lt;p&gt;Using Virtualization both issues can be addressed.
Multiple systems workload is executed on same hardware which result into better utilization,
and Managing multiple systems with software also makes more efficient and easy.&lt;/p&gt;

&lt;h3 id=&#34;how-is-virtualization-is-achieved:a88864bb2e02cbfffb44963e73e3622a&#34;&gt;How is Virtualization is achieved?&lt;/h3&gt;

&lt;p&gt;There are broadly three types of Virtualization Techniques.
&lt;a href=&#34;http://en.wikipedia.org/wiki/Virtualization#Hardware_virtualization&#34;&gt;Wikipedia&lt;/a&gt; has short and crisp definition for all three.&lt;/p&gt;

&lt;p&gt;Here Host OS is the OS which runs on Actual Hardware and Guest OS is running on Virtual Hardware/Machine.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Full virtualization â€“ almost complete simulation of the actual hardware to allow software, which typically consists of a guest operating system, to run unmodified.
&lt;br&gt;e.g. VirtualBox, VMWare Workstations, Parallel for MAC works on full virtualization, No change is required in guest OS&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Partial virtualization â€“ some but not all of the target environment attributes are simulated. As a result, some guest programs may need modifications to run in such virtual environments.
&lt;br&gt;Probably this is first approach to virtualization which lead to full virtualization.
&lt;br&gt;e.g. IBM mainfraim system &lt;a href=&#34;http://en.wikipedia.org/wiki/IBM_M44/44X&#34;&gt;IBM M44/44X&lt;/a&gt; was one of such experimental machine.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Paravirtualization â€“ a hardware environment is not simulated; however, the guest programs are executed in their own isolated domains, as if they are running on a separate system. Guest programs need to be specifically modified to run in this environment.
&lt;br&gt;Paravirtulization is lightweight as compared to full virtualization.
&lt;br&gt;e.g. XEN is based on paravirtualization.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;so-what-we-understood:a88864bb2e02cbfffb44963e73e3622a&#34;&gt;So what we understood!&lt;/h3&gt;

&lt;p&gt;Well, We must have realized with all these techniques, all that is achieved here is Isolation!
Isolation of Hardware, so OS running in Virtual Machine gets a feel all hardware is available for its use.
That is the way all OS are implemented! Exclusive access to hardware.&lt;/p&gt;

&lt;p&gt;But above three methods are not that efficient with Hardware support for Virtualization.
So, CPU also have extra core and support to run the Virtualization code efficently. More you can find on &lt;a href=&#34;http://en.wikipedia.org/wiki/X86_virtualization&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since Isolation is the key to virtulization, Unix &amp;amp; Linux based OS have few features, which provides Isolation to process.
Features like namespaces, cgroups and chroot.
Utilzing these features, OS level virtualization can be achieved. In linux these are called Containers.&lt;/p&gt;

&lt;p&gt;I will be writing details of containers in my next blog. So stay tuned :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Moving to Hugo &amp; github</title>
      <link>http://kunalkushwaha.github.io/2015/03/04/moved-to-github/</link>
      <pubDate>Wed, 04 Mar 2015 14:53:05 +0530</pubDate>
      
      <guid>http://kunalkushwaha.github.io/2015/03/04/moved-to-github/</guid>
      <description>

&lt;p&gt;Started blogging today after few years of gap. Had stopped blogging few years back on &lt;a href=&#34;http://kunalkushwaha.wordpress.com&#34;&gt;wordpress&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;Blogging is always fun and great way to share your experiences and findings with others.&lt;/p&gt;

&lt;p&gt;Lot of things have changed in last 3-5 years in world of Web Technology, So is the blogging platform too.
While exploring Web Technologies and Golang, I came across the static site genrator and Hugo.&lt;/p&gt;

&lt;p&gt;I found it really powerful tool for blogging as well as Product Documentations.&lt;/p&gt;

&lt;h2 id=&#34;why-not-to-continue-with-wordpress:1dd8e767a925214d802c5a963c76c86a&#34;&gt;Why not to continue with wordpress!&lt;/h2&gt;

&lt;p&gt;I work most of times on linux terminal and &lt;a href=&#34;http://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; with github is seamlessly integrated with  git workflow.
Also, I have complete control over theme and customization. This learning is helpful to me for product documentation too :)&lt;/p&gt;

&lt;p&gt;Going ahead I am hopeful, will write regularly and share intresting findings from work and side projects.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>