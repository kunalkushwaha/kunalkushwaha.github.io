<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Go on Kunal Kushwaha</title>
    <link>https://kunalkushwaha.github.io/tags/go/</link>
    <description>Recent content in Go on Kunal Kushwaha</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Kunal Kushwaha</copyright>
    <lastBuildDate>Thu, 06 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://kunalkushwaha.github.io/tags/go/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building AI Agents That Actually Do Things: Tools and MCP Made Simple in Go</title>
      <link>https://kunalkushwaha.github.io/2025/11/06/mcp-tools-with-agenticgokit/</link>
      <pubDate>Thu, 06 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://kunalkushwaha.github.io/2025/11/06/mcp-tools-with-agenticgokit/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;&#xA;&lt;p&gt;If you&amp;rsquo;re new to building AI agents in Go, you might think integrating tools and external services is complex. &lt;strong&gt;It&amp;rsquo;s not.&lt;/strong&gt; AgenticGoKit makes tools—including Model Context Protocol (MCP) tools—incredibly simple to use.&lt;/p&gt;&#xA;&lt;p&gt;In this hands-on guide, you&amp;rsquo;ll discover how AgenticGoKit treats tools as first-class citizens, making your AI agents genuinely useful by connecting them to real-world capabilities. Whether you want to call APIs, run system commands, or integrate with external services, it&amp;rsquo;s just a few lines of Go code.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building Context-Aware AI Agents with Memory in AgenticGoKit</title>
      <link>https://kunalkushwaha.github.io/2025/10/30/context-aware-ai-agents-with-memory/</link>
      <pubDate>Thu, 30 Oct 2025 02:18:55 +0900</pubDate>
      <guid>https://kunalkushwaha.github.io/2025/10/30/context-aware-ai-agents-with-memory/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://kunalkushwaha.github.io/memory-quote.PNG&#34; alt=&#34;Simple streaming (terminal)&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;One of the biggest challenges in building AI agents is making them &lt;strong&gt;remember&lt;/strong&gt;. Users expect conversational agents to recall previous interactions, maintain context across multiple turns, and provide personalized responses based on conversation history.&lt;/p&gt;&#xA;&lt;p&gt;AgenticGoKit&amp;rsquo;s memory system solves this elegantly with a unified interface that supports:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Conversation history - Sequential chat memory&lt;/li&gt;&#xA;&lt;li&gt;RAG (Retrieval Augmented Generation) - Semantic search over memories&lt;/li&gt;&#xA;&lt;li&gt;Memory tracking - Monitor memory usage and query performance&lt;/li&gt;&#xA;&lt;li&gt;Session management - Scope memories to conversation sessions&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;In this post, we&amp;rsquo;ll build a real interactive chat agent that demonstrates these features.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming AI Responses in Go with AgenticGoKit</title>
      <link>https://kunalkushwaha.github.io/2025/10/26/streaming-agenticgokit/</link>
      <pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://kunalkushwaha.github.io/2025/10/26/streaming-agenticgokit/</guid>
      <description>&lt;p&gt;Streaming makes AI feel alive—tokens show up instantly, long tasks feel responsive, and multi‑step workflows become explainable as they run. In this post, we&amp;rsquo;ll build two streaming experiences with AgenticGoKit:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A minimal &amp;ldquo;simple-streaming&amp;rdquo; chat&lt;/li&gt;&#xA;&lt;li&gt;A sequential multi‑agent &amp;ldquo;streaming_workflow&amp;rdquo; with step-by-step progress&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;We&amp;rsquo;ll also cover when to use streaming, why it helps, and a few gotchas and tips.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-streaming-and-why-it-matters&#34;&gt;What is streaming and why it matters&lt;/h2&gt;&#xA;&lt;p&gt;Instead of waiting for the full response, streaming lets you consume output as it&amp;rsquo;s generated (token‑by‑token or chunk‑by‑chunk). That enables:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
