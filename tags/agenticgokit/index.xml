<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Agenticgokit on Kunal Kushwaha</title>
    <link>https://kunalkushwaha.github.io/tags/agenticgokit/</link>
    <description>Recent content in Agenticgokit on Kunal Kushwaha</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Kunal Kushwaha</copyright>
    <lastBuildDate>Thu, 06 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://kunalkushwaha.github.io/tags/agenticgokit/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Use MCP and Tools with AgenticGoKit</title>
      <link>https://kunalkushwaha.github.io/2025/11/06/mcp-tools-with-agenticgokit/</link>
      <pubDate>Thu, 06 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://kunalkushwaha.github.io/2025/11/06/mcp-tools-with-agenticgokit/</guid>
      <description>&lt;p&gt;AgenticGoKit makes tools (and MCP tools) first-class citizens. This post shows exactly how to enable them, call them directly, and let your LLM trigger them—with copy‑pasteable snippets and a runnable example.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-youll-build&#34;&gt;What you&amp;rsquo;ll build&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;An agent with tools enabled&lt;/li&gt;&#xA;&lt;li&gt;MCP tools via explicit server or automatic discovery&lt;/li&gt;&#xA;&lt;li&gt;Direct tool calls from Go&lt;/li&gt;&#xA;&lt;li&gt;LLM‑driven tool calls using a simple TOOL_CALL JSON envelope&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Go 1.24+&lt;/li&gt;&#xA;&lt;li&gt;LLM provider plugin (examples use Ollama with &lt;code&gt;gemma3:1b&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Optional: An MCP server (e.g., HTTP SSE on &lt;code&gt;localhost:8812&lt;/code&gt;) or use discovery&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;quick-mental-model&#34;&gt;Quick mental model&lt;/h2&gt;&#xA;&lt;p&gt;AgenticGoKit supports two tool sources:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building Context-Aware AI Agents with Memory in AgenticGoKit</title>
      <link>https://kunalkushwaha.github.io/2025/10/30/context-aware-ai-agents-with-memory/</link>
      <pubDate>Thu, 30 Oct 2025 02:18:55 +0900</pubDate>
      <guid>https://kunalkushwaha.github.io/2025/10/30/context-aware-ai-agents-with-memory/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://kunalkushwaha.github.io/memory-quote.PNG&#34; alt=&#34;Simple streaming (terminal)&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;One of the biggest challenges in building AI agents is making them &lt;strong&gt;remember&lt;/strong&gt;. Users expect conversational agents to recall previous interactions, maintain context across multiple turns, and provide personalized responses based on conversation history.&lt;/p&gt;&#xA;&lt;p&gt;AgenticGoKit&amp;rsquo;s memory system solves this elegantly with a unified interface that supports:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Conversation history - Sequential chat memory&lt;/li&gt;&#xA;&lt;li&gt;RAG (Retrieval Augmented Generation) - Semantic search over memories&lt;/li&gt;&#xA;&lt;li&gt;Memory tracking - Monitor memory usage and query performance&lt;/li&gt;&#xA;&lt;li&gt;Session management - Scope memories to conversation sessions&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;In this post, we&amp;rsquo;ll build a real interactive chat agent that demonstrates these features.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming AI Responses in Go with AgenticGoKit</title>
      <link>https://kunalkushwaha.github.io/2025/10/26/streaming-agenticgokit/</link>
      <pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://kunalkushwaha.github.io/2025/10/26/streaming-agenticgokit/</guid>
      <description>&lt;p&gt;Streaming makes AI feel alive—tokens show up instantly, long tasks feel responsive, and multi‑step workflows become explainable as they run. In this post, we&amp;rsquo;ll build two streaming experiences with AgenticGoKit:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A minimal &amp;ldquo;simple-streaming&amp;rdquo; chat&lt;/li&gt;&#xA;&lt;li&gt;A sequential multi‑agent &amp;ldquo;streaming_workflow&amp;rdquo; with step-by-step progress&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;We&amp;rsquo;ll also cover when to use streaming, why it helps, and a few gotchas and tips.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-streaming-and-why-it-matters&#34;&gt;What is streaming and why it matters&lt;/h2&gt;&#xA;&lt;p&gt;Instead of waiting for the full response, streaming lets you consume output as it&amp;rsquo;s generated (token‑by‑token or chunk‑by‑chunk). That enables:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
