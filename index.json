
[{"content":"","date":"16 November 2025","externalUrl":null,"permalink":"/tags/agenticgokit/","section":"Tags","summary":"","title":"Agenticgokit","type":"tags"},{"content":"","date":"16 November 2025","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"Ai","type":"tags"},{"content":"","date":"16 November 2025","externalUrl":null,"permalink":"/tags/golang/","section":"Tags","summary":"","title":"Golang","type":"tags"},{"content":"","date":"16 November 2025","externalUrl":null,"permalink":"/","section":"Kunal Kushwaha","summary":"","title":"Kunal Kushwaha","type":"page"},{"content":"","date":"16 November 2025","externalUrl":null,"permalink":"/tags/multi-agent/","section":"Tags","summary":"","title":"Multi-Agent","type":"tags"},{"content":"","date":"16 November 2025","externalUrl":null,"permalink":"/post/","section":"Posts","summary":"","title":"Posts","type":"post"},{"content":"\rWhat if your AI agents could argue about Oxford commas? üé≠\r#\rPicture this: You ask for a bedtime story. A Writer agent drafts it in seconds‚Äîintentionally making a few typos. An Editor agent reads it, spots \u0026ldquo;tyme\u0026rdquo; instead of \u0026ldquo;time,\u0026rdquo; and fires back with corrections. The Writer fixes the issues and resubmits. The Editor approves. A Publisher agent then polishes everything into a beautiful final piece.\nAll of this happens automatically, and you watch it unfold in real-time.\nThis isn\u0026rsquo;t science fiction‚Äîit\u0026rsquo;s what you can build in an afternoon with AgenticGoKit, a pure Go framework for orchestrating multi-agent AI workflows. No Python required. No heavyweight dependencies. Just clean, idiomatic Go that lets you compose AI agents like LEGO bricks.\nWhy Golang developers will love this\r#\rIf you\u0026rsquo;ve been envious of Python\u0026rsquo;s AI ecosystem, here\u0026rsquo;s your revenge:\nNative Go: No wrappers, no Python subprocesses. Pure, compiled Go performance. Channels everywhere: Streaming responses feel natural when your language was built for concurrency. Type safety: Your agents are strongly typed. Your workflow graph catches errors at compile time. Deploy anywhere: Single binary. No pip dependencies breaking in production. Ship it as a container, a Lambda, or bare metal. The best part? The code actually makes sense. No magic decorators or hidden state‚Äîjust explicit agent definitions and workflow composition.\nThe demo: A mini creative studio in ~200 lines\r#\rHere\u0026rsquo;s what the story-writer-chat-v2 example does:\nWriter Agent drafts a story (we seed a couple typos to make the loop visible) Editor Agent reviews and responds with either: FIX: tyme‚Üítime, happyness‚Üíhappiness APPROVED: [story text] Loop continues until approved (configurable max iterations; the example uses 3) Publisher Agent formats the final story with title and clean paragraphs What you see in the UI:\r#\rAgents \u0026ldquo;talk\u0026rdquo; to each other through streaming messages. It feels alive because the UI updates as each chunk arrives‚Äîjust like watching ChatGPT type.\nUnder the hood: How AgenticGoKit makes this trivial\r#\r1. Agents are just functions with personality\r#\r// Example agent creation (match the demo\u0026#39;s shape in `workflow/agents.go`) writer, _ := vnext.QuickChatAgentWithConfig(\u0026#34;Writer\u0026#34;, \u0026amp;vnext.Config{ Name: \u0026#34;writer\u0026#34;, SystemPrompt: WriterSystemPrompt, Timeout: 90 * time.Second, Streaming: \u0026amp;vnext.StreamingConfig{Enabled: true, BufferSize: 50, FlushInterval: 50}, LLM: vnext.LLMConfig{ Provider: cfg.Provider, Model: cfg.Model, Temperature: 0.8, MaxTokens: 500, APIKey: cfg.APIKey, }, }) That\u0026rsquo;s it. An agent is an LLM + config + optional memory. No inheritance hierarchies.\n2. Loops are first-class workflows\r#\rloopWorkflow, _ := vnext.NewLoopWorkflowWithCondition(\u0026amp;vnext.WorkflowConfig{ Mode: vnext.Loop, Timeout: 300 * time.Second, MaxIterations: 3, }, vnext.Conditions.OutputContains(\u0026#34;APPROVED\u0026#34;)) loopWorkflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;write\u0026#34;, Agent: writer}) loopWorkflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;edit\u0026#34;, Agent: editor}) The framework handles the cycling. You just define the exit condition.\n3. Workflows compose into bigger workflows\r#\r// Wrap the loop as a single agent revisionLoop := vnext.NewSubWorkflowAgent(\u0026#34;revision_loop\u0026#34;, loopWorkflow, ...) // Build the main pipeline mainWorkflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;revisions\u0026#34;, Agent: revisionLoop}) mainWorkflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;publish\u0026#34;, Agent: publisher}) // Run it with streaming stream, _ := storyPipeline.RunStream(ctx, userPrompt) for chunk := range stream.Chunks() { // Handle start/text/complete/metadata/error } Nesting workflows is trivial‚Äîthey\u0026rsquo;re just agents with extra steps. This is how you build complex orchestrations without losing your mind.\n4. Transforms enforce contracts\r#\rvnext.WorkflowStep{ Name: \u0026#34;edit\u0026#34;, Agent: editor, // In the demo transforms are simple string-\u0026gt;string helpers (see `workflow/transforms.go`). Transform: func(input string) string { return fmt.Sprintf(`Review this draft. Output either: - FIX: word1‚Üícorrection1, word2‚Üícorrection2 - APPROVED: [story text] Draft: %s`, input) }, } Transforms let you massage inputs/outputs between steps. The Editor always gets instructions‚Äîthe Writer never sees them. Clean separation of concerns.\nThe visual flow\r#\rflowchart TD\rU[User Prompt] --\u003e SP[Story Pipeline]\rSP --\u003e RL[Revision Loop]\rsubgraph RevisionLoop[Writer ‚Üî Editor Loop]\rdirection TB\rW[Writerdrafts story] --\u003e E[Editorreviews draft]\rE -- FIX: typos --\u003e W\rE -- APPROVED --\u003e OUT[Exit Loop]\rend\rRL --\u003e P[Publisherformats final story]\rP --\u003e FINAL[Beautiful Story ‚ú®]\rstyle RevisionLoop fill:#f0f0ff\rstyle W fill:#ffe0e0\rstyle E fill:#e0f0ff\rstyle P fill:#e0ffe0\rReal-time streaming: The secret sauce\r#\rThe demo includes a WebSocket server that converts agent events into UI updates:\n// Example (illustrative) stream handler that matches how the demo emits chunks. type StreamHandler struct { wsConn *websocket.Conn } func (h *StreamHandler) OnChunk(chunk *vnext.StreamChunk) { switch chunk.Type { case vnext.ChunkTypeAgentStart: h.wsConn.WriteJSON(map[string]interface{}{\u0026#34;type\u0026#34;: \u0026#34;agent_start\u0026#34;, \u0026#34;agent\u0026#34;: chunk.Metadata[\u0026#34;step_name\u0026#34;]}) case vnext.ChunkTypeText, vnext.ChunkTypeDelta: text := chunk.Content if chunk.Type == vnext.ChunkTypeDelta { text = chunk.Delta } h.wsConn.WriteJSON(map[string]interface{}{\u0026#34;type\u0026#34;: \u0026#34;agent_chunk\u0026#34;, \u0026#34;agent\u0026#34;: chunk.Metadata[\u0026#34;step_name\u0026#34;], \u0026#34;text\u0026#34;: text}) case vnext.ChunkTypeAgentComplete: h.wsConn.WriteJSON(map[string]interface{}{\u0026#34;type\u0026#34;: \u0026#34;agent_complete\u0026#34;, \u0026#34;agent\u0026#34;: chunk.Metadata[\u0026#34;step_name\u0026#34;], \u0026#34;metadata\u0026#34;: chunk.Metadata}) case vnext.ChunkTypeMetadata: h.wsConn.WriteJSON(map[string]interface{}{\u0026#34;type\u0026#34;: \u0026#34;workflow_info\u0026#34;, \u0026#34;meta\u0026#34;: chunk.Metadata}) case vnext.ChunkTypeError: h.wsConn.WriteJSON(map[string]interface{}{\u0026#34;type\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;error\u0026#34;: chunk.Error.Error()}) } } The frontend renders these as progressive chat messages. Users see agents \u0026ldquo;thinking\u0026rdquo; in real-time. It\u0026rsquo;s the UX polish that makes AI feel magical.\nRun it yourself in 60 seconds\r#\rgit clone https://github.com/AgenticGoKit/agentic-examples cd agentic-examples\\story-writer-chat-v2 # Copy example env and edit it to match your provider Copy-Item .env.example .env # Edit `.env`: set `LLM_PROVIDER` and the matching API key variable. # Examples: # - For OpenRouter: set `LLM_PROVIDER=openrouter` and `OPENROUTER_API_KEY=your-key` # - For OpenAI direct: set `LLM_PROVIDER=openai` and `OPENAI_API_KEY=your-key` # Start backend go mod tidy go run main.go # Runs on :8080 # In another terminal, start frontend cd frontend npm install npm run dev # Opens http://localhost:5173 Type: \u0026ldquo;Write a short bedtime story about a tiny star that lost its sky.\u0026rdquo;\nWatch the Writer draft, the Editor critique, and the Publisher polish. It takes ~10 seconds.\nExtend it in 5 minutes: Real ideas that work\r#\rIdea 1: Human-in-the-loop approval\r#\r// Pause before publishing, wait for user confirmation mainWorkflow.AddStep(vnext.WorkflowStep{ Name: \u0026#34;human_approval\u0026#34;, Agent: humanApprovalAgent, // Blocks until webhook/button press }) Idea 2: Add a tone coach\r#\rtoneCoach := vnext.QuickChatAgentWithConfig(\u0026#34;tone_coach\u0026#34;, llm, vnext.ChatConfig{ SystemPrompt: \u0026#34;Rate tone (1-5). If \u0026lt; 4, suggest improvements.\u0026#34;, }) loopWorkflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;tone_check\u0026#34;, Agent: toneCoach}) Idea 3: Parallel fact-checking\r#\rparallelWorkflow := vnext.NewParallelWorkflow(cfg) parallelWorkflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;spell_check\u0026#34;, Agent: spellChecker}) parallelWorkflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;fact_check\u0026#34;, Agent: factChecker}) All of these are ~10 lines of code. Seriously.\nWhy AgenticGoKit exists\r#\rPython has LangChain and CrewAI. But if you\u0026rsquo;re a Go shop:\nYou\u0026rsquo;re not rewriting your infra in Python just for AI workflows You want proper testing, not notebooks that break in production You need to deploy to Lambda/Cloud Run/K8s without containerizing Python runtimes You value explicitness over magic AgenticGoKit gives you the good parts of agentic frameworks (composition, streaming, memory) without the Python tax.\nWhy you should try\r#\rIf you\u0026rsquo;re building:\nContent pipelines: Automate drafts, reviews, SEO optimization Customer support: Route ‚Üí classify ‚Üí respond ‚Üí escalate workflows Data processing: Extract ‚Üí validate ‚Üí transform ‚Üí summarize chains Code review bots: Analyze ‚Üí suggest ‚Üí apply ‚Üí test loops \u0026hellip;you need orchestration that\u0026rsquo;s transparent, debuggable, and fast. AgenticGoKit delivers.\nThe story-writer demo is intentionally simple‚Äîit shows the core pattern without drowning you in complexity. Once you grok Writer ‚Üî Editor loops, you can build anything.\nGet started\r#\rGitHub: AgenticGoKit/agentic-examples Docs: https://docs.agenticgokit.com/ Discord: Join our community Clone the repo. Run the demo. Break it. Build something weird.\nThen show us what you made‚Äîwe love seeing creative hacks. üöÄ\nP.S. If you build a story-writer that uses an LLM to argue about serial commas, please tag us. We need to see this.\n","date":"16 November 2025","externalUrl":null,"permalink":"/post/real-time-ai-editorial-workflows-go/","section":"Posts","summary":"Build real-time AI editorial workflows in Go with AgenticGoKit ‚Äî loop revisions, stream progress to a live UI, and publish polished stories.","title":"Real-Time AI Editorial Workflows in Go: Loop, Revise, Publish with AgenticGoKit","type":"post"},{"content":"","date":"16 November 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"16 November 2025","externalUrl":null,"permalink":"/tags/tutorial/","section":"Tags","summary":"","title":"Tutorial","type":"tags"},{"content":"","date":"16 November 2025","externalUrl":null,"permalink":"/tags/workflows/","section":"Tags","summary":"","title":"Workflows","type":"tags"},{"content":"\rWe\u0026rsquo;re excited to announce the release of AgenticGoKit v0.4.6 ‚Äî a major update that expands how Go developers can build and experiment with agentic workflows.\nWith this release, AgenticGoKit now supports:\nHugging Face ‚Äî Access models through the new router API with OpenAI-compatible format OpenRouter ‚Äî Use a wide variety of LLMs (GPT, Claude, Gemini, Llama) through a unified API These integrations empower developers to build richer, more flexible AI agents without switching languages or managing complex SDKs.\nWhat\u0026rsquo;s New\r#\rHugging Face Integration with New Router API\r#\rAgenticGoKit now supports Hugging Face\u0026rsquo;s latest router-based architecture (router.huggingface.co), which provides:\nOpenAI-compatible format ‚Äî Seamless migration from other providers Multiple API types ‚Äî Inference API, Chat API, dedicated Endpoints, and self-hosted TGI Llama 3.2 models ‚Äî Access to the latest open-source models Streaming support ‚Äî Real-time token-by-token responses Advanced parameters ‚Äî Fine-tune with temperature, top-p, top-k, and more Quick Example\r#\rimport ( \u0026#34;context\u0026#34; \u0026#34;github.com/kunalkushwaha/agenticgokit/core/vnext\u0026#34; _ \u0026#34;github.com/kunalkushwaha/agenticgokit/plugins/llm/huggingface\u0026#34; ) config := \u0026amp;vnext.Config{ Name: \u0026#34;hf-assistant\u0026#34;, SystemPrompt: \u0026#34;You are a helpful AI assistant.\u0026#34;, LLM: vnext.LLMConfig{ Provider: \u0026#34;huggingface\u0026#34;, Model: \u0026#34;meta-llama/Llama-3.2-1B-Instruct\u0026#34;, APIKey: os.Getenv(\u0026#34;HUGGINGFACE_API_KEY\u0026#34;), Temperature: 0.7, MaxTokens: 500, }, } agent, _ := vnext.NewBuilder(\u0026#34;hf-assistant\u0026#34;).WithConfig(config).Build() result, _ := agent.Run(context.Background(), \u0026#34;Explain machine learning\u0026#34;) fmt.Println(result.Content) Available Models:\nmeta-llama/Llama-3.2-1B-Instruct ‚Äî Fast, efficient meta-llama/Llama-3.2-3B-Instruct ‚Äî Better quality deepseek-ai/DeepSeek-R1 ‚Äî Advanced reasoning Model routing with :fastest and :cheapest suffixes HuggingFace Examples | HuggingFace vNext Quickstart\nOpenRouter Integration\r#\rOpenRouter provides a single interface to dozens of leading LLMs from OpenAI, Anthropic, Google, Meta, and more. With AgenticGoKit v0.4.6, you can:\nAccess GPT-4, Claude 3, Gemini 2.0, Llama 3.1, and many others Compare models side-by-side for your use case Switch between providers with a single line of code Track usage with site analytics Quick Example\r#\rconfig := \u0026amp;vnext.Config{ Name: \u0026#34;openrouter-agent\u0026#34;, SystemPrompt: \u0026#34;You are a helpful assistant.\u0026#34;, LLM: vnext.LLMConfig{ Provider: \u0026#34;openrouter\u0026#34;, Model: \u0026#34;anthropic/claude-3-haiku\u0026#34;, APIKey: os.Getenv(\u0026#34;OPENROUTER_API_KEY\u0026#34;), Temperature: 0.7, MaxTokens: 500, }, } agent, _ := vnext.NewBuilder(\u0026#34;openrouter-agent\u0026#34;).WithConfig(config).Build() result, _ := agent.Run(ctx, \u0026#34;What\u0026#39;s the difference between REST and GraphQL?\u0026#34;) Popular Models Available:\nopenai/gpt-4-turbo ‚Äî Most capable GPT model anthropic/claude-3-haiku ‚Äî Fast, cost-effective google/gemini-2.0-flash-exp:free ‚Äî Free Google model meta-llama/llama-3.1-8b-instruct ‚Äî Open-source Llama OpenRouter Examples | OpenRouter vNext Quickstart\nWhy This Matters\r#\rBuilding agentic systems in Go is now easier and more powerful than ever. By connecting to Hugging Face and OpenRouter, AgenticGoKit developers can:\nRapidly prototype with different LLMs and compare performance Optimize costs by testing various model/provider combinations Build multi-agent systems using diverse model capabilities Access the latest models including Llama 3.2, Claude 3, Gemini 2.0 Keep the speed and reliability of Go with type safety Switch providers easily without rewriting application logic Key Features in v0.4.6\r#\rMultiple API Types for HuggingFace\r#\rInference API ‚Äî New router with OpenAI-compatible format Chat API ‚Äî Conversational models with legacy support Inference Endpoints ‚Äî Dedicated hosting for production Text Generation Inference (TGI) ‚Äî Self-hosted optimized inference Streaming Support\r#\rBoth HuggingFace and OpenRouter now support real-time streaming:\nstream, _ := agent.RunStream(ctx, \u0026#34;Write a story about AI\u0026#34;) for chunk := range stream.Chunks() { if chunk.Type == vnext.ChunkTypeDelta { fmt.Print(chunk.Delta) } } Advanced Configuration\r#\rFine-tune model behavior with provider-specific parameters:\nLLM: vnext.LLMConfig{ Provider: \u0026#34;huggingface\u0026#34;, Model: \u0026#34;meta-llama/Llama-3.2-1B-Instruct\u0026#34;, Temperature: 0.8, MaxTokens: 1000, TopP: 0.9, TopK: 50, RepetitionPenalty: 1.2, } Comprehensive Examples\r#\r9 HuggingFace examples covering all API types 8 OpenRouter examples demonstrating various models Simple quickstart examples for both providers Migration guides for the new HuggingFace router API Complete documentation with troubleshooting tips The Story: Breaking Free from LLM Lock-In\r#\rThe Challenge\r#\rYou\u0026rsquo;re building an AI agent in Go. You start with OpenAI\u0026rsquo;s GPT-4 because it\u0026rsquo;s the best. But then:\nCosts add up ‚Äî Your prototype\u0026rsquo;s API bills are growing faster than your user base You\u0026rsquo;re locked in ‚Äî What if you want to try Anthropic\u0026rsquo;s Claude? Or use open-source Llama models? Experimentation is hard ‚Äî Testing different models means rewriting integration code Enterprise needs ‚Äî Your client wants Azure OpenAI for compliance, but your code is OpenAI-specific You\u0026rsquo;re stuck. Switching providers means days of refactoring.\nThe Solution\r#\rAgenticGoKit v0.4.6 changes the game. With HuggingFace and OpenRouter support, you can now:\n// Start with HuggingFace for experimentation config.LLM.Provider = \u0026#34;huggingface\u0026#34; config.LLM.Model = \u0026#34;meta-llama/Llama-3.2-1B-Instruct\u0026#34; // Switch to OpenRouter to test Claude config.LLM.Provider = \u0026#34;openrouter\u0026#34; config.LLM.Model = \u0026#34;anthropic/claude-3-haiku\u0026#34; // Move to production with OpenAI config.LLM.Provider = \u0026#34;openai\u0026#34; config.LLM.Model = \u0026#34;gpt-4-turbo\u0026#34; Same code. Different providers. One line change.\nThe Impact\r#\rWith 5 supported providers (OpenAI, Azure OpenAI, HuggingFace, OpenRouter, Ollama), AgenticGoKit now gives you:\nProvider What You Get Why It Matters HuggingFace (NEW) Open-source models (Llama 3.2) + Router API Free tier, experimentation, self-hosting options OpenRouter (NEW) 50+ models (GPT, Claude, Gemini, Llama) Single API, compare models, optimize costs OpenAI GPT-4, GPT-3.5 Industry standard, highest quality Azure OpenAI Enterprise GPT models Compliance, SLAs, enterprise support Ollama Local models Privacy, offline, zero API costs Build once. Deploy anywhere. Switch anytime.\nTake Action: Try It Now\r#\rStep 1: Install (30 seconds)\r#\rgo get github.com/kunalkushwaha/agenticgokit@latest Step 2: Get an API Key (2 minutes)\r#\rPick one provider to start:\nHuggingFace (free tier): https://huggingface.co/settings/tokens OpenRouter (flexible): https://openrouter.ai/keys Step 3: Run Your First Agent (1 minute)\r#\r# HuggingFace - Free tier, open models export HUGGINGFACE_API_KEY=\u0026#34;hf_...\u0026#34; cd examples/vnext/huggingface-quickstart go run simple.go # OR OpenRouter - 50+ models to choose from export OPENROUTER_API_KEY=\u0026#34;sk-or-...\u0026#34; cd examples/vnext/openrouter-quickstart go run main.go That\u0026rsquo;s it. You\u0026rsquo;re running AI agents in Go.\nWhat You Can Build Today\r#\r1. Cost Optimizer Agent\r#\r// Start cheap with HuggingFace Llama config.LLM.Provider = \u0026#34;huggingface\u0026#34; config.LLM.Model = \u0026#34;meta-llama/Llama-3.2-1B-Instruct\u0026#34; // Upgrade to GPT-4 for complex queries if complexQuery { config.LLM.Provider = \u0026#34;openai\u0026#34; config.LLM.Model = \u0026#34;gpt-4-turbo\u0026#34; } 2. Multi-Model Research Assistant\r#\r// Use OpenRouter to query multiple models models := []string{ \u0026#34;openai/gpt-4-turbo\u0026#34;, \u0026#34;anthropic/claude-3-haiku\u0026#34;, \u0026#34;google/gemini-2.0-flash-exp:free\u0026#34;, } for _, model := range models { config.LLM.Model = model // Compare responses from different models } 3. Privacy-First Local Agent\r#\r// Start with Ollama for development config.LLM.Provider = \u0026#34;ollama\u0026#34; config.LLM.Model = \u0026#34;llama3.2\u0026#34; // Switch to HuggingFace TGI for production config.LLM.Provider = \u0026#34;huggingface\u0026#34; config.LLM.BaseURL = \u0026#34;http://your-tgi-server:8080\u0026#34; Learn More\r#\rQuick Links:\nHuggingFace Quickstart ‚Äî 9 examples OpenRouter Quickstart ‚Äî 8 examples HuggingFace Migration Guide ‚Äî New router API All Examples ‚Äî 20+ working examples GitHub Discussions ‚Äî Get help Join the Community\r#\rAgenticGoKit is open source and growing. We\u0026rsquo;d love your feedback:\nFound a bug? Open an issue Have an idea? Start a discussion Want to contribute? Check out our contributor guide Like what you see? Star us on GitHub The Bottom Line\r#\rAgenticGoKit v0.4.6 gives you freedom.\nStart with HuggingFace for free experimentation Switch to OpenRouter to test 50+ models Move to OpenAI for production quality Deploy to Azure OpenAI for enterprise compliance Or keep it local with Ollama One codebase. Five providers. Endless possibilities.\n# Get started now go get github.com/kunalkushwaha/agenticgokit@latest Happy building!\nQuestions? Join our community discussions or check out the documentation.\n","date":"8 November 2025","externalUrl":null,"permalink":"/post/agenticgokit-love-hf-and-openrouter/","section":"Posts","summary":"","title":"AgenticGoKit ‚ù§Ô∏è HuggingFace and OpenRouter","type":"post"},{"content":"","date":"8 November 2025","externalUrl":null,"permalink":"/tags/agents/","section":"Tags","summary":"","title":"Agents","type":"tags"},{"content":"","date":"8 November 2025","externalUrl":null,"permalink":"/tags/go/","section":"Tags","summary":"","title":"Go","type":"tags"},{"content":"","date":"8 November 2025","externalUrl":null,"permalink":"/tags/huggingface/","section":"Tags","summary":"","title":"HuggingFace","type":"tags"},{"content":"","date":"8 November 2025","externalUrl":null,"permalink":"/tags/llama/","section":"Tags","summary":"","title":"Llama","type":"tags"},{"content":"","date":"8 November 2025","externalUrl":null,"permalink":"/tags/llms/","section":"Tags","summary":"","title":"LLMs","type":"tags"},{"content":"","date":"8 November 2025","externalUrl":null,"permalink":"/tags/openrouter/","section":"Tags","summary":"","title":"OpenRouter","type":"tags"},{"content":"","date":"8 November 2025","externalUrl":null,"permalink":"/tags/release/","section":"Tags","summary":"","title":"Release","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/ai-agents/","section":"Tags","summary":"","title":"AI Agents","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/ai-tooling/","section":"Tags","summary":"","title":"AI Tooling","type":"tags"},{"content":"\rIf you\u0026rsquo;re new to building AI agents in Go, you might think integrating tools and external services is complex. It\u0026rsquo;s not. AgenticGoKit makes tools‚Äîincluding Model Context Protocol (MCP) tools‚Äîincredibly simple to use.\nIn this hands-on guide, you\u0026rsquo;ll discover how AgenticGoKit treats tools as first-class citizens, making your AI agents genuinely useful by connecting them to real-world capabilities. Whether you want to call APIs, run system commands, or integrate with external services, it\u0026rsquo;s just a few lines of Go code.\nWhat makes this different? No complex setup, no wrestling with schemas, no boilerplate. Just clean, idiomatic Go that gets your agent talking to the world around it.\nWhat you\u0026rsquo;ll build\r#\rAn agent with tools enabled MCP tools via explicit server or automatic discovery Direct tool calls from Go LLM‚Äëdriven tool calls using a simple TOOL_CALL JSON envelope Prerequisites\r#\rGo 1.24+ LLM provider plugin (examples use Ollama with gemma3:1b) Optional: An MCP server (e.g., HTTP SSE on localhost:8812) or use discovery Quick mental model\r#\rAgenticGoKit supports two tool sources:\nInternal tools: in-process functions you implement and register MCP tools: tools discovered from external MCP servers Enable tools on the Builder and they become available to your agent. You can call them directly or let the LLM trigger them.\nControl how tools are invoked\r#\rBuilder.WithTools wires tools (internal + MCP) into the agent‚Äôs capabilities. Choosing if/when to call tools is a policy‚ÄîvNext leaves that up to your handler:\nTool‚Äëaware wrapper (WithToolAugmentation) = automatic tool execution when the model requests it ToolsFirst = a heuristic that tries tools first, then falls back to the LLM Custom handler = complete control (e.g., parse a TOOL_CALL JSON envelope) Details below in ‚ÄúChoose your tool invocation policy.‚Äù\nDiagram: tools at a glance\r#\rflowchart LR\rsubgraph \"Tool sources\"\rA[\"Internal tools\"]\rB[\"MCP servers\"]\rend\rC[\"WithTools(...)\"] --\u003e D[\"Tool discovery/registry\"]\rA --\u003e D\rB --\u003e D\rD --\u003e E[\"Agent capabilities (tools)\"]\rUse internal tools (implement or reuse)\r#\rYou can implement your own tool by satisfying the vnext.Tool interface and registering it. There‚Äôs also a built-in echo tool you can use immediately.\nImplement and register your own tool\r#\r// 1) Implement the Tool interface type helloTool struct{} func (t *helloTool) Name() string { return \u0026#34;hello\u0026#34; } func (t *helloTool) Description() string { return \u0026#34;Greets a name you pass\u0026#34; } func (t *helloTool) Execute(ctx context.Context, args map[string]any) (*vnext.ToolResult, error) { name, _ := args[\u0026#34;name\u0026#34;].(string) if name == \u0026#34;\u0026#34; { name = \u0026#34;there\u0026#34; } return \u0026amp;vnext.ToolResult{ Success: true, Content: fmt.Sprintf(\u0026#34;Hello, %s!\u0026#34;, name) }, nil } // 2) Register it (e.g., in init() or main()) func init() { vnext.RegisterInternalTool(\u0026#34;hello\u0026#34;, func() vnext.Tool { return \u0026amp;helloTool{} }) } // 3) Discover and call it ctx := context.Background() tools, _ := vnext.DiscoverTools() // includes internal tools for _, t := range tools { fmt.Println(t.Name(), t.Description()) } res, err := vnext.ExecuteToolByName(ctx, \u0026#34;hello\u0026#34;, map[string]any{\u0026#34;name\u0026#34;: \u0026#34;Kunal\u0026#34;}) if err == nil { fmt.Println(res.Content) } Reuse an existing internal tool (echo)\r#\rAn echo tool ships out of the box. Call it directly:\nctx := context.Background() res, err := vnext.ExecuteToolByName(ctx, \u0026#34;echo\u0026#34;, map[string]any{\u0026#34;message\u0026#34;: \u0026#34;hello world\u0026#34;}) 1) Enable tools on an agent\r#\rExplicit MCP server (HTTP SSE shown; adjust for your transport):\nagent, err := vnext.NewBuilder(\u0026#34;mcp-agent\u0026#34;). WithConfig(\u0026amp;vnext.Config{ Name: \u0026#34;mcp-agent\u0026#34;, SystemPrompt: \u0026#34;You are a helpful assistant with access to tools.\u0026#34;, Timeout: 60 * time.Second, LLM: vnext.LLMConfig{ Provider: \u0026#34;ollama\u0026#34;, Model: \u0026#34;gemma3:1b\u0026#34; }, }). WithTools( vnext.WithMCP(vnext.MCPServer{ Name: \u0026#34;tools\u0026#34;, Type: \u0026#34;http_sse\u0026#34;, Address: \u0026#34;localhost\u0026#34;, Port: 8812, Enabled: true }), vnext.WithToolTimeout(30*time.Second), ). Build() Prefer auto‚Äëdiscovery? Scan common ports:\nagent, err := vnext.NewBuilder(\u0026#34;mcp-discovery\u0026#34;). WithConfig(\u0026amp;vnext.Config{ /* LLM + basics as above */ }). WithTools(vnext.WithMCPDiscovery(8080, 8081, 8090, 8100, 8811, 8812)). Build() Choose your tool invocation policy\r#\rBy default, tools don‚Äôt auto‚Äërun on agent.Run(). They‚Äôre available once configured, and you decide how they‚Äôre invoked. Pick one of these options:\nTool‚Äëaware wrapper (automatic; no custom parsing) // Wrap an LLM-only handler so it can call tools when the model requests it handler := vnext.WithToolAugmentation(vnext.LLMOnly(\u0026#34;You are a helpful assistant.\u0026#34;)) agent, err := vnext.NewBuilder(\u0026#34;tool-auto\u0026#34;). WithConfig(\u0026amp;vnext.Config{ /* LLM + basics */ }). WithTools(/* WithMCP or WithMCPDiscovery */). WithHandler(handler). Build() Alternatively, use a prebuilt strategy that tries tools first and falls back to the LLM:\nhandler := vnext.ToolsFirst(\u0026#34;You are a helpful assistant with tools.\u0026#34;) Manual TOOL_CALL parsing (complete control) handler := func(ctx context.Context, input string, caps *vnext.Capabilities) (string, error) { // Ask LLM to output TOOL_CALL{\u0026#34;name\u0026#34;:..., \u0026#34;args\u0026#34;:{...}} out, err := caps.LLM(\u0026#34;Use tools when helpful and emit TOOL_CALL JSON.\u0026#34;, input) if err != nil { return \u0026#34;\u0026#34;, err } // Parse and execute any tool calls results, _ := vnext.ExecuteToolsFromLLMResponse(ctx, out) if len(results) \u0026gt; 0 \u0026amp;\u0026amp; results[0].Success { return fmt.Sprint(results[0].Content), nil } return out, nil } Sequence: policy options\r#\rsequenceDiagram\ractor User\rparticipant Agent\rparticipant Handler\rparticipant LLM\rparticipant Tools as Tool Registry\rparticipant MCP as MCP Server\rparticipant Internal as Internal Tool\rUser-\u003e\u003eAgent: Run(input)\rAgent-\u003e\u003eHandler: Handle(input, caps)\ralt WithToolAugmentation/ToolsFirst\rHandler-\u003e\u003eLLM: Prompt + tools schema\rLLM--\u003e\u003eHandler: Text or TOOL_CALL\ropt TOOL_CALL\rHandler-\u003e\u003eTools: ExecuteToolByName(name,args)\rTools-\u003e\u003eMCP: Forward if MCP\rMCP--\u003e\u003eTools: ToolResult\rTools--\u003e\u003eHandler: ToolResult\rHandler--\u003e\u003eAgent: Combine result + answer\rend\ropt No TOOL_CALL\rHandler--\u003e\u003eAgent: Return LLM text\rend\relse Custom\rHandler-\u003e\u003eLLM: Prompt with TOOL_CALL instruction\rLLM--\u003e\u003eHandler: Output\rHandler-\u003e\u003eHandler: Parse TOOL_CALL JSON\rHandler-\u003e\u003eTools: Execute tools (if any)\rTools--\u003e\u003eHandler: ToolResult\rHandler--\u003e\u003eAgent: Final response\rend\rAgent--\u003e\u003eUser: Reply\rTip: In your main package, include blank imports for MCP transport/registry and an LLM provider (the examples already do this):\nimport ( _ \u0026#34;github.com/kunalkushwaha/agenticgokit/plugins/mcp/unified\u0026#34; _ \u0026#34;github.com/kunalkushwaha/agenticgokit/plugins/mcp/default\u0026#34; _ \u0026#34;github.com/kunalkushwaha/agenticgokit/plugins/llm/ollama\u0026#34; ) 2) List and call tools from code\r#\r// List all tools (internal + MCP) tools, _ := vnext.DiscoverTools() for _, t := range tools { fmt.Printf(\u0026#34;- %s: %s\\n\u0026#34;, t.Name(), t.Description()) } // Execute a tool directly by name res, err := vnext.ExecuteToolByName(ctx, \u0026#34;echo\u0026#34;, map[string]any{\u0026#34;message\u0026#34;: \u0026#34;hello\u0026#34;}) if err != nil { /* handle */ } fmt.Println(res.Success, res.Content) 3) Let the LLM call tools (TOOL_CALL)\r#\rAsk your model to emit this envelope when a tool helps:\nTOOL_CALL{\u0026#34;name\u0026#34;: \u0026#34;search\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;golang mcp\u0026#34;, \u0026#34;max_results\u0026#34;: 5}} Parse and execute in one step:\nllmOutput := `Here is what I will do.\\nTOOL_CALL{\u0026#34;name\u0026#34;: \u0026#34;echo\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;message\u0026#34;: \u0026#34;from llm\u0026#34;}}` results, _ := vnext.ExecuteToolsFromLLMResponse(ctx, llmOutput) Flow: TOOL_CALL execution\r#\rflowchart TD\rA[\"LLM output\"] --\u003e B{\"Contains TOOL_CALL?\"}\rB -- Yes --\u003e C[\"ExecuteToolsFromLLMResponse(ctx, output)\"]\rC --\u003e D[\"ToolResult(s)\"]\rD --\u003e E[\"Combine into final answer\"]\rB -- No --\u003e F[\"Return LLM text\"]\rF --\u003e E\rTry it now\r#\rTwo runnable examples are included in the repo:\nexamples/vnext/mcp-tools-blog-demo/ ‚Äî minimal, annotated demo examples/vnext/mcp-integration/ ‚Äî fuller example with both modes # From repo root \u0026#34;cd examples/vnext/mcp-tools-blog-demo; go run .\u0026#34; Practical tips\r#\rIf no tools appear: verify MCP server address/port or use discovery; ensure plugins are imported in your binary. When using TOOL_CALL, make sure argument names/types match the tool\u0026rsquo;s schema. Caching is available; enable it when tools are expensive or frequently called. Links\r#\rRepo examples: examples/vnext/mcp-tools-blog-demo examples/vnext/mcp-integration vNext APIs Documentation : vNext API Reference That‚Äôs it‚Äîtreat MCP as just another tool source. Enable it, list tools, and either call them directly or let your LLM trigger them to deliver better answers.\n","date":"6 November 2025","externalUrl":null,"permalink":"/post/mcp-tools-with-agenticgokit/","section":"Posts","summary":"","title":"Building AI Agents That Actually Do Things: Tools and MCP Made Simple in Go","type":"post"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/mcp/","section":"Tags","summary":"","title":"MCP","type":"tags"},{"content":"","date":"6 November 2025","externalUrl":null,"permalink":"/tags/tools/","section":"Tags","summary":"","title":"Tools","type":"tags"},{"content":"","date":"30 October 2025","externalUrl":null,"permalink":"/tags/aiagents/","section":"Tags","summary":"","title":"AIagents","type":"tags"},{"content":"\rOne of the biggest challenges in building AI agents is making them remember. Users expect conversational agents to recall previous interactions, maintain context across multiple turns, and provide personalized responses based on conversation history.\nAgenticGoKit\u0026rsquo;s memory system solves this elegantly with a unified interface that supports:\nConversation history - Sequential chat memory RAG (Retrieval Augmented Generation) - Semantic search over memories Memory tracking - Monitor memory usage and query performance Session management - Scope memories to conversation sessions In this post, we\u0026rsquo;ll build a real interactive chat agent that demonstrates these features.\nDiscover how to create conversational AI agents that remember past interactions and provide personalized, context-aware responses\nThe Memory Problem\r#\rConsider a simple conversation:\nUser: \u0026#34;My name is Sarah and I love hiking.\u0026#34; Assistant: \u0026#34;Nice to meet you, Sarah! Hiking is a wonderful activity.\u0026#34; User: \u0026#34;What do you know about me?\u0026#34; Assistant: \u0026#34;I don\u0026#39;t have any information about you.\u0026#34; Without memory, each interaction is isolated. The agent forgets everything immediately.\nThe AgenticGoKit Solution\r#\rWith AgenticGoKit\u0026rsquo;s memory system, the same conversation becomes:\nUser: \u0026#34;My name is Sarah and I love hiking.\u0026#34; Assistant: \u0026#34;Nice to meet you, Sarah! Hiking is a wonderful activity.\u0026#34; [Memory] 2 queries performed User: \u0026#34;What do you know about me?\u0026#34; Assistant: \u0026#34;Based on our conversation, I know your name is Sarah and you enjoy hiking!\u0026#34; [Memory] 2 queries performed The agent remembers! Let\u0026rsquo;s see how to build this.\nSetting Up Memory-Enabled Agent\r#\r1. Configuration\r#\rFirst, configure your agent with memory support:\nagent, err := vnext.NewBuilder(\u0026#34;chat-assistant\u0026#34;). WithConfig(\u0026amp;vnext.Config{ Name: \u0026#34;chat-assistant\u0026#34;, SystemPrompt: `You are a helpful and friendly chat assistant. You remember details from our conversation and provide personalized responses. Be conversational and engaging while being helpful.`, // LLM Configuration LLM: vnext.LLMConfig{ Provider: \u0026#34;ollama\u0026#34;, Model: \u0026#34;gpt-oss:120b-cloud\u0026#34;, Temperature: 0.7, MaxTokens: 2000, }, // Memory Configuration Memory: \u0026amp;vnext.MemoryConfig{ Provider: \u0026#34;memory\u0026#34;, // In-memory provider RAG: \u0026amp;vnext.RAGConfig{ MaxTokens: 1000, // Context window for memories PersonalWeight: 0.8, // Prioritize conversation history KnowledgeWeight: 0.2, // Lower weight for knowledge base HistoryLimit: 20, // Keep last 20 messages }, }, Timeout: 300 * time.Second, }). Build() if err != nil { log.Fatalf(\u0026#34;Failed to create agent: %v\u0026#34;, err) } // Initialize the agent if err := agent.Initialize(ctx); err != nil { log.Fatalf(\u0026#34;Failed to initialize agent: %v\u0026#34;, err) } defer agent.Cleanup(ctx) 2. Understanding Memory Configuration\r#\rLet\u0026rsquo;s break down the key memory settings:\nProvider Options:\n\u0026quot;memory\u0026quot; - In-memory storage (simple, great for demos) \u0026quot;pgvector\u0026quot; - PostgreSQL with vector embeddings (production-ready) \u0026quot;weaviate\u0026quot; - Weaviate vector database Custom providers via plugin system RAG Configuration:\nMaxTokens: Maximum tokens for retrieved context (affects prompt size) PersonalWeight: Priority for conversation history (0.0-1.0) KnowledgeWeight: Priority for knowledge base documents (0.0-1.0) HistoryLimit: Number of recent messages to include Pro Tip: For conversational agents, set PersonalWeight higher (0.7-0.9) to prioritize recent dialogue over general knowledge.\nBuilding an Interactive Chat Loop\r#\rBasic (Non-Streaming) Version\r#\rscanner := bufio.NewScanner(os.Stdin) conversationCount := 0 fmt.Println(\u0026#34;Start chatting! Type \u0026#39;quit\u0026#39; or \u0026#39;exit\u0026#39; to end.\u0026#34;) for { fmt.Print(\u0026#34;You: \u0026#34;) if !scanner.Scan() { break } userInput := strings.TrimSpace(scanner.Text()) if userInput == \u0026#34;\u0026#34; { continue } if strings.ToLower(userInput) == \u0026#34;quit\u0026#34; { fmt.Println(\u0026#34;Goodbye!\u0026#34;) break } conversationCount++ fmt.Printf(\u0026#34;\\nAssistant (Turn %d):\\n\u0026#34;, conversationCount) // Run agent with memory result, err := agent.Run(ctx, userInput) if err != nil { fmt.Printf(\u0026#34;Error: %v\\n\\n\u0026#34;, err) continue } // Display response fmt.Printf(\u0026#34;%s\\n\u0026#34;, result.Content) // Show memory usage if result.MemoryUsed { fmt.Printf(\u0026#34;\\n[Memory] Used (%d queries)\\n\u0026#34;, result.MemoryQueries) } fmt.Printf(\u0026#34;[Time] Response time: %v\\n\u0026#34;, result.Duration) fmt.Println(strings.Repeat(\u0026#34;-\u0026#34;, 60)) } Streaming Version (Real-time Token-by-Token)\r#\rFor a more interactive experience, use streaming:\nstream, err := agent.RunStream(ctx, userInput) if err != nil { fmt.Printf(\u0026#34;Error: %v\\n\u0026#34;, err) continue } // Process streaming chunks for chunk := range stream.Chunks() { switch chunk.Type { case vnext.ChunkTypeDelta: fmt.Print(chunk.Delta) // Print tokens as they arrive case vnext.ChunkTypeError: fmt.Printf(\u0026#34;\\nError: %v\\n\u0026#34;, chunk.Error) case vnext.ChunkTypeDone: fmt.Println() // New line after response } } // Get final result with memory stats result, err := stream.Wait() if err != nil { fmt.Printf(\u0026#34;Error: %v\\n\u0026#34;, err) continue } fmt.Printf(\u0026#34;\\n[Memory] %d queries | [Time] %v\\n\u0026#34;, result.MemoryQueries, result.Duration) How Memory Works Under the Hood\r#\rWhen you send a message to a memory-enabled agent, here\u0026rsquo;s what happens:\ngraph TD A[User Input] --\u003e B[Memory Query Phase] B --\u003e C{RAG Query} B --\u003e D{Chat History} C --\u003e E[Semantic Search] D --\u003e F[Sequential Retrieval] E --\u003e G[Relevant Memories] F --\u003e H[Recent Messages] G --\u003e I[Context Enrichment] H --\u003e I I --\u003e J[Enriched Prompt] J --\u003e K[LLM Generation] K --\u003e L[Response] L --\u003e M[Memory Storage] M --\u003e N[Update RAG Index] M --\u003e O[Update Chat History]\r1. Memory Query Phase (Before LLM call)\r#\rInput: \u0026#34;What did we discuss earlier?\u0026#34; Memory System performs: ‚îú‚îÄ RAG Query: Semantic search for relevant past memories ‚îÇ ‚îî‚îÄ Returns: Top-k similar conversation snippets ‚îÇ ‚îî‚îÄ History Fetch: Get recent sequential messages ‚îî‚îÄ Returns: Last N conversation turns Typically 2 queries per turn:\nOne RAG semantic query One chat history retrieval 2. Context Enrichment\r#\rThe agent uses specialized APIs to combine context into an enriched prompt:\nKey APIs Used:\nEnrichWithMemory() - Performs RAG semantic search and returns relevant memories BuildChatHistoryContext() - Retrieves recent conversation history BuildEnrichedPrompt() - Combines all context (system prompt, memories, history, user input) How it works in agent.Run():\n// Step 2: Enhance prompt with memory context if memory is enabled // Use the new BuildEnrichedPrompt utility for proper RAG integration memoryQueries := 0 if a.memoryProvider != nil \u0026amp;\u0026amp; a.config.Memory != nil { // Convert llm.Prompt to core.Prompt for enrichment var corePrompt core.Prompt corePrompt, memoryQueries = BuildEnrichedPrompt(ctx, prompt.System, prompt.User, a.memoryProvider, a.config.Memory) // Update the LLM prompt with enriched content prompt.System = corePrompt.System prompt.User = corePrompt.User } The BuildEnrichedPrompt function internally:\nCalls EnrichWithMemory() to get RAG context (counts as 1 query) Calls BuildChatHistoryContext() to get chat history (counts as 1 query if performed) Combines everything into the final enriched prompt The agent combines:\nYour system prompt Retrieved memories (RAG results from EnrichWithMemory) Recent chat history (from BuildChatHistoryContext) Current user input Into an enriched prompt sent to the LLM via BuildEnrichedPrompt.\n3. Response Generation\r#\rThe LLM generates a response using the enriched context.\n4. Memory Storage\r#\rAfter response generation:\nUser message is stored in memory Assistant response is stored in memory Both are available for future retrievals 5. Result Tracking\r#\rThe Result object reports:\nresult.MemoryUsed // true if memory was accessed result.MemoryQueries // Number of queries performed result.Duration // Total execution time result.TokensUsed // LLM tokens consumed Real-World Example Output\r#\rHere\u0026rsquo;s what a real conversation looks like:\nInteractive Chat Agent with Memory =================================== You: My name is Sarah and I work as a software engineer in San Francisco. Assistant (Turn 1): Nice to meet you, Sarah! It\u0026#39;s great to connect with a software engineer from San Francisco. The tech scene there is incredible! What kind of projects do you work on? [Memory] Used (2 queries) [Time] Response time: 3.2s ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ You: I mostly work on distributed systems and cloud infrastructure. Assistant (Turn 2): That sounds fascinating! Distributed systems and cloud infrastructure are critical areas in modern software engineering. Working on those systems in San Francisco must give you exposure to some cutting-edge technology. Are you working with any particular cloud providers or frameworks? [Memory] Used (2 queries) [Time] Response time: 3.5s ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ You: What do you know about me so far? Assistant (Turn 3): Based on our conversation, I know that: - Your name is Sarah - You work as a software engineer in San Francisco - You focus on distributed systems and cloud infrastructure These are really impressive areas of expertise! Is there anything specific about your work you\u0026#39;d like to discuss? [Memory] Used (2 queries) [Time] Response time: 3.1s ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Notice how the agent:\nRemembers the name \u0026ldquo;Sarah\u0026rdquo; Recalls the job (software engineer in San Francisco) References the technical specialization (distributed systems) Maintains conversational context across turns Memory Query Counter Deep Dive\r#\rThe memory query counter is particularly useful for:\nPerformance Monitoring\r#\rif result.MemoryQueries \u0026gt; 5 { log.Printf(\u0026#34;Warning: High memory query count: %d\u0026#34;, result.MemoryQueries) } Understanding Memory Behavior\r#\rTypical query counts:\n0 queries: Memory disabled or no memory configured 1 query: Only RAG or only history (unusual) 2 queries: Normal (1 RAG + 1 history fetch) 3+ queries: Multiple memory providers or custom implementations Cost Tracking\r#\rFor production systems using vector databases:\ntype ConversationMetrics struct { TotalTurns int TotalMemQueries int AvgQueriesPerTurn float64 } metrics.TotalMemQueries += result.MemoryQueries metrics.AvgQueriesPerTurn = float64(metrics.TotalMemQueries) / float64(metrics.TotalTurns) Advanced: RAG vs. Chat History\r#\rAgenticGoKit\u0026rsquo;s memory system uses two complementary retrieval mechanisms:\n1. RAG (Retrieval Augmented Generation)\r#\rHow it works:\nConverts text to embeddings Performs semantic similarity search Finds conceptually related memories Best for:\nFinding relevant facts from many turns ago Semantic understanding (\u0026ldquo;what hobbies\u0026rdquo; finds \u0026ldquo;I like hiking\u0026rdquo;) Knowledge base queries Example:\nTurn 1: \u0026#34;I love hiking in the mountains\u0026#34; Turn 20: \u0026#34;What outdoor activities do I enjoy?\u0026#34; ‚Üì RAG retrieves Turn 1 (semantically similar) 2. Chat History\r#\rHow it works:\nSequential retrieval Returns last N conversation turns Maintains dialogue flow Best for:\nRecent context Conversation continuity Referencing \u0026ldquo;earlier\u0026rdquo; or \u0026ldquo;just now\u0026rdquo; Example:\nTurn 18: \u0026#34;I work in San Francisco\u0026#34; Turn 19: \u0026#34;What city did I mention?\u0026#34; ‚Üì Chat history provides Turn 18 Why Both?\r#\rMemory: \u0026amp;vnext.MemoryConfig{ RAG: \u0026amp;vnext.RAGConfig{ PersonalWeight: 0.8, // Chat history priority KnowledgeWeight: 0.2, // RAG semantic search priority HistoryLimit: 20, // Last 20 messages }, } The weights balance:\nHigh PersonalWeight: Prioritizes recent dialogue context, ensuring conversational continuity and immediate relevance High KnowledgeWeight: Prioritizes semantic relevance over recency, better for knowledge-intensive tasks Why prioritize chat history for conversations? Recent dialogue provides immediate context, maintains conversation flow, and ensures the agent remembers what was just discussed. This creates more natural, coherent interactions compared to jumping between semantically similar but temporally distant memories.\nProduction Considerations\r#\r1. Choose the Right Provider\r#\rDevelopment/Testing:\nMemory: \u0026amp;vnext.MemoryConfig{ Provider: \u0026#34;memory\u0026#34;, // In-memory, lost on restart } Production:\nMemory: \u0026amp;vnext.MemoryConfig{ Provider: \u0026#34;pgvector\u0026#34;, ConnectionString: os.Getenv(\u0026#34;DATABASE_URL\u0026#34;), Embedding: \u0026amp;vnext.EmbeddingConfig{ Provider: \u0026#34;ollama\u0026#34;, Model: \u0026#34;nomic-embed-text:latest\u0026#34;, }, } 2. Session Management\r#\rScope memories to user sessions:\nsessionID := memory.NewSession() ctx = memory.SetSession(ctx, sessionID) // All subsequent operations use this session result, err := agent.Run(ctx, \u0026#34;Hello\u0026#34;) 3. Token Budget Management\r#\rRAG: \u0026amp;vnext.RAGConfig{ MaxTokens: 1000, // Limit context size HistoryLimit: 10, // Fewer turns = lower costs } Why this matters:\nLLM APIs charge per token Larger context = higher costs per request Balance context quality vs. cost 4. Error Handling\r#\rresult, err := agent.Run(ctx, input) if err != nil { log.Printf(\u0026#34;Agent error: %v\u0026#34;, err) continue } if !result.MemoryUsed { log.Println(\u0026#34;Warning: Memory was not utilized\u0026#34;) } if result.MemoryQueries == 0 { log.Println(\u0026#34;Warning: No memory queries performed\u0026#34;) } Comparing: Memory vs. No Memory\r#\rLet\u0026rsquo;s see the difference side-by-side:\nAspect Without Memory With Memory Turn 1: \u0026ldquo;My name is Alex\u0026rdquo; \u0026ldquo;Nice to meet you, Alex!\u0026rdquo; \u0026ldquo;Nice to meet you, Alex!\u0026quot;\n[Memory] 2 queries Turn 2: \u0026ldquo;What\u0026rsquo;s my name?\u0026rdquo; \u0026ldquo;I don\u0026rsquo;t know your name.\u0026rdquo; \u0026ldquo;Your name is Alex!\u0026quot;\n[Memory] 2 queries Turn 3: \u0026ldquo;What do I do?\u0026rdquo; \u0026ldquo;I don\u0026rsquo;t have information about your profession.\u0026rdquo; \u0026ldquo;I don\u0026rsquo;t recall you mentioning your profession yet. What do you do?\u0026quot;\n[Memory] 2 queries Turn 4: \u0026ldquo;I\u0026rsquo;m a teacher\u0026rdquo; \u0026ldquo;That\u0026rsquo;s interesting!\u0026rdquo; \u0026ldquo;That\u0026rsquo;s wonderful! As a teacher, you must have many interesting stories to share.\u0026quot;\n[Memory] 2 queries Turn 5: \u0026ldquo;Tell me about myself\u0026rdquo; \u0026ldquo;I don\u0026rsquo;t have personal information about you.\u0026rdquo; \u0026ldquo;Based on our conversation, I know you\u0026rsquo;re Alex and you work as a teacher. You seem passionate about education!\u0026quot;\n[Memory] 2 queries Without Memory (Code)\r#\ragent, _ := vnext.QuickChatAgent(\u0026#34;gpt-4o-mini\u0026#34;) // No memory configuration result1, _ := agent.Run(ctx, \u0026#34;My name is Alex\u0026#34;) result2, _ := agent.Run(ctx, \u0026#34;What\u0026#39;s my name?\u0026#34;) // Response: \u0026#34;I don\u0026#39;t know your name\u0026#34; With Memory (Code)\r#\ragent, _ := vnext.NewBuilder(\u0026#34;agent\u0026#34;). WithConfig(\u0026amp;vnext.Config{ LLM: vnext.LLMConfig{ Provider: \u0026#34;ollama\u0026#34;, Model: \u0026#34;gpt-oss:120b-cloud\u0026#34;, }, Memory: \u0026amp;vnext.MemoryConfig{ Provider: \u0026#34;memory\u0026#34;, RAG: \u0026amp;vnext.RAGConfig{ PersonalWeight: 0.8, HistoryLimit: 20, }, }, }). Build() result1, _ := agent.Run(ctx, \u0026#34;My name is Alex\u0026#34;) // [Memory] 2 queries result2, _ := agent.Run(ctx, \u0026#34;What\u0026#39;s my name?\u0026#34;) // Response: \u0026#34;Your name is Alex!\u0026#34; // [Memory] 2 queries Common Pitfalls \u0026amp; Best Practices\r#\rPrivacy \u0026amp; Data Security\r#\rPitfall: Storing sensitive information in memory without user consent.\nBest Practice: Always implement data retention policies and give users control over their data:\n// Clear user session data if err := memoryProvider.ClearSession(ctx, sessionID); err != nil { log.Printf(\u0026#34;Failed to clear session: %v\u0026#34;, err) } // Implement data retention limits Memory: \u0026amp;vnext.MemoryConfig{ RetentionPolicy: \u0026amp;vnext.RetentionPolicy{ MaxAge: 30 * 24 * time.Hour, // 30 days MaxEntries: 1000, // Per session }, } Token Cost Explosion\r#\rPitfall: Memory context grows unbounded, causing skyrocketing API costs.\nBest Practice: Set reasonable limits and monitor usage:\nRAG: \u0026amp;vnext.RAGConfig{ MaxTokens: 1000, // Limit context size HistoryLimit: 10, // Limit conversation turns } // Monitor costs if result.MemoryQueries \u0026gt; 5 { log.Printf(\u0026#34;High memory usage detected: %d queries\u0026#34;, result.MemoryQueries) } Context Drift \u0026amp; Hallucinations\r#\rPitfall: Outdated or irrelevant memories confuse the agent.\nBest Practice: Use session management and implement relevance scoring:\n// Start new conversation sessions sessionID := memory.NewSession() ctx = memory.SetSession(ctx, sessionID) // AgenticGoKit automatically handles session isolation result, err := agent.Run(ctx, \u0026#34;Hello, I\u0026#39;m starting a new conversation\u0026#34;) Performance Degradation\r#\rPitfall: Memory queries slow down response times.\nBest Practice: Choose appropriate providers and optimize configurations:\n// Development: Fast in-memory Memory: \u0026amp;vnext.MemoryConfig{Provider: \u0026#34;memory\u0026#34;} // Production: Optimized vector database Memory: \u0026amp;vnext.MemoryConfig{ Provider: \u0026#34;pgvector\u0026#34;, Embedding: \u0026amp;vnext.EmbeddingConfig{ Provider: \u0026#34;ollama\u0026#34;, Model: \u0026#34;nomic-embed-text:latest\u0026#34;, }, } Over-Reliance on Memory\r#\rPitfall: Agent becomes too dependent on past context, ignoring current instructions.\nBest Practice: Balance memory weight with current context importance:\nRAG: \u0026amp;vnext.RAGConfig{ PersonalWeight: 0.7, // Memory context KnowledgeWeight: 0.3, // Current/general knowledge } Complete Working Example\r#\rHere\u0026rsquo;s the full code for a production-ready memory-enabled chat agent:\npackage main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/kunalkushwaha/agenticgokit/core/vnext\u0026#34; _ \u0026#34;github.com/kunalkushwaha/agenticgokit/plugins/llm/ollama\u0026#34; _ \u0026#34;github.com/kunalkushwaha/agenticgokit/plugins/memory/memory\u0026#34; ) func main() { ctx := context.Background() // Create agent with memory agent, err := vnext.NewBuilder(\u0026#34;chat-assistant\u0026#34;). WithConfig(\u0026amp;vnext.Config{ Name: \u0026#34;chat-assistant\u0026#34;, SystemPrompt: `You are a helpful and friendly chat assistant. You remember details from our conversation and provide personalized responses.`, LLM: vnext.LLMConfig{ Provider: \u0026#34;ollama\u0026#34;, Model: \u0026#34;gpt-oss:120b-cloud\u0026#34;, Temperature: 0.7, MaxTokens: 2000, }, Memory: \u0026amp;vnext.MemoryConfig{ Provider: \u0026#34;memory\u0026#34;, RAG: \u0026amp;vnext.RAGConfig{ MaxTokens: 1000, PersonalWeight: 0.8, KnowledgeWeight: 0.2, HistoryLimit: 20, }, }, Timeout: 300 * time.Second, }). Build() if err != nil { log.Fatalf(\u0026#34;Failed to create agent: %v\u0026#34;, err) } if err := agent.Initialize(ctx); err != nil { log.Fatalf(\u0026#34;Failed to initialize agent: %v\u0026#34;, err) } defer agent.Cleanup(ctx) fmt.Println(\u0026#34;Chat Agent Ready!\u0026#34;) fmt.Println(\u0026#34;Try: \u0026#39;My name is [name]\u0026#39; then ask \u0026#39;What do you know about me?\u0026#39;\u0026#34;) fmt.Println() scanner := bufio.NewScanner(os.Stdin) conversationCount := 0 for { fmt.Print(\u0026#34;You: \u0026#34;) if !scanner.Scan() { break } userInput := strings.TrimSpace(scanner.Text()) if userInput == \u0026#34;\u0026#34; || strings.ToLower(userInput) == \u0026#34;quit\u0026#34; { break } conversationCount++ fmt.Printf(\u0026#34;\\nAssistant (Turn %d):\\n\u0026#34;, conversationCount) result, err := agent.Run(ctx, userInput) if err != nil { fmt.Printf(\u0026#34;Error: %v\\n\\n\u0026#34;, err) continue } fmt.Printf(\u0026#34;%s\\n\u0026#34;, result.Content) if result.MemoryUsed { fmt.Printf(\u0026#34;\\n[Memory] %d queries | [Time] %v\\n\u0026#34;, result.MemoryQueries, result.Duration) } fmt.Println(strings.Repeat(\u0026#34;-\u0026#34;, 60)) } fmt.Println(\u0026#34;\\nThanks for chatting!\u0026#34;) } Running the Examples\r#\rAgenticGoKit includes two complete demos:\n1. Basic Memory Demo\r#\rcd examples/vnext/conversation-memory-demo go run main.go Features:\nStandard request/response flow Memory tracking and statistics Simple to understand and modify 2. Streaming Memory Demo\r#\rcd examples/vnext/conversation-memory-stream-demo go run main.go Features:\nReal-time token-by-token streaming Live memory query feedback Enhanced user experience Key Takeaways\r#\rMemory makes agents conversational - Context awareness transforms isolated Q\u0026amp;A into natural dialogue\nTwo-tier retrieval is powerful - RAG + chat history provides both semantic understanding and conversational flow\nMemory queries are trackable - Monitor performance and costs with result.MemoryQueries\nConfiguration is flexible - Tune weights, limits, and providers for your use case\nProduction-ready options exist - Scale from in-memory prototypes to PostgreSQL/Weaviate production deployments\nNext Steps\r#\rExplore the Memory API documentation Try the streaming examples Check out workflow memory patterns Resources\r#\rAgenticGoKit Repository: github.com/kunalkushwaha/agenticgokit Documentation: docs/reference/api/vnext/ Memory Examples: examples/vnext/conversation-memory-demo/ Built with AgenticGoKit v0.4.5 - The Go framework for production-ready AI agents\nContribute to AgenticGoKit\r#\rOpen-source Agentic AI framework in Go for building, orchestrating, and deploying intelligent agents. LLM-agnostic, event-driven, with multi-agent workflows, MCP tool discovery, and production-grade observability.\nAgenticGoKit is an open-source project that welcomes contributions! Whether you\u0026rsquo;re interested in:\nAdding new LLM providers or memory backends Improving documentation and examples Implementing new agent patterns or workflows Fixing bugs or adding features Writing tests and improving code quality We\u0026rsquo;d love to have you join our community. Check out our contributing guide and GitHub repository to get started.\n","date":"30 October 2025","externalUrl":null,"permalink":"/post/context-aware-ai-agents-with-memory/","section":"Posts","summary":"","title":"Building Context-Aware AI Agents with Memory in AgenticGoKit","type":"post"},{"content":"","date":"30 October 2025","externalUrl":null,"permalink":"/tags/context/","section":"Tags","summary":"","title":"Context","type":"tags"},{"content":"","date":"30 October 2025","externalUrl":null,"permalink":"/tags/memoryagents/","section":"Tags","summary":"","title":"Memoryagents","type":"tags"},{"content":"","date":"26 October 2025","externalUrl":null,"permalink":"/tags/ollama/","section":"Tags","summary":"","title":"Ollama","type":"tags"},{"content":"","date":"26 October 2025","externalUrl":null,"permalink":"/tags/streaming/","section":"Tags","summary":"","title":"Streaming","type":"tags"},{"content":"Streaming makes AI feel alive‚Äîtokens show up instantly, long tasks feel responsive, and multi‚Äëstep workflows become explainable as they run. In this post, we\u0026rsquo;ll build two streaming experiences with AgenticGoKit:\nA minimal \u0026ldquo;simple-streaming\u0026rdquo; chat A sequential multi‚Äëagent \u0026ldquo;streaming_workflow\u0026rdquo; with step-by-step progress We\u0026rsquo;ll also cover when to use streaming, why it helps, and a few gotchas and tips.\nWhat is streaming and why it matters\r#\rInstead of waiting for the full response, streaming lets you consume output as it\u0026rsquo;s generated (token‚Äëby‚Äëtoken or chunk‚Äëby‚Äëchunk). That enables:\nReal‚Äëtime feedback: Users see progress immediately Better UX for long tasks: No ‚Äúblank screen‚Äù pause Step visibility in workflows: Know which agent/step is running Early assessment: Skim partial output and course‚Äëcorrect sooner Under the hood, AgenticGoKit vNext exposes a Stream you can iterate over, with multiple chunk types like text deltas, metadata, tool calls, and final completion signals.\nPrerequisites\r#\rGo installed and working in this repo Ollama running locally (default http://localhost:11434) Model: gemma3:1b Repo paths in this post are relative to the project root Pull the model if needed:\nollama pull gemma3:1b Alternatively, you can use OpenAI or Azure OpenAI instead of Ollama by setting API keys and pointing your agent config to those providers:\n# OpenAI $env:OPENAI_API_KEY = \u0026#34;\u0026lt;your-openai-api-key\u0026gt;\u0026#34; # Azure OpenAI $env:AZURE_OPENAI_KEY = \u0026#34;\u0026lt;your-azure-openai-key\u0026gt;\u0026#34; # Your Azure Base URL typically looks like: # https://\u0026lt;your-resource-name\u0026gt;.openai.azure.com/ For a deeper dive into APIs and options, see core/vnext/STREAMING_GUIDE.md.\nPart 1: Minimal simple-streaming\r#\rThe example lives here: examples/vnext/simple-streaming/main.go.\nIt creates a small chat agent and prints tokens as they arrive:\n// Start streaming ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second) defer cancel() stream, err := agent.RunStream(ctx, prompt) if err != nil { log.Fatalf(\u0026#34;Failed to start streaming: %v\u0026#34;, err) } // Print tokens as they arrive for chunk := range stream.Chunks() { if chunk.Error != nil { fmt.Printf(\u0026#34;\\n‚ùå Error: %v\\n\u0026#34;, chunk.Error); break } switch chunk.Type { case vnext.ChunkTypeDelta: fmt.Print(chunk.Delta) // token-by-token case vnext.ChunkTypeDone: fmt.Println(\u0026#34;\\n\\n‚úÖ Streaming completed!\u0026#34;) } } // Always check the final result _, _ = stream.Wait() Run it:\ncd examples/vnext/simple-streaming go run . You‚Äôll see something like:\nNotes:\nThe example uses Ollama with gemma3:1b. Adjust the model/provider in the config if needed. Always call stream.Wait() after consuming chunks to surface any trailing errors. Part 2: Multi‚Äëagent streaming workflow\r#\rNow, let‚Äôs level up with a sequential, two‚Äëagent workflow that streams each step in real time. Code: examples/vnext/streaming_workflow/main.go.\nWe‚Äôll create two specialized agents‚ÄîResearcher and Summarizer‚Äîand wire them into a vNext Workflow. Each step streams its own tokens and emits metadata so you know what‚Äôs happening.\nDefining agents\r#\rfunc CreateResearcherAgent() (vnext.Agent, error) { return vnext.QuickChatAgentWithConfig(\u0026#34;Researcher\u0026#34;, \u0026amp;vnext.Config{ Name: \u0026#34;researcher\u0026#34;, SystemPrompt: \u0026#34;You are a Research Agent...\u0026#34;, Timeout: 60 * time.Second, LLM: vnext.LLMConfig{ Provider: \u0026#34;ollama\u0026#34;, Model: \u0026#34;gemma3:1b\u0026#34;, Temperature: 0.2, MaxTokens: 300, BaseURL: \u0026#34;http://localhost:11434\u0026#34; }, }) } func CreateSummarizerAgent() (vnext.Agent, error) { return vnext.QuickChatAgentWithConfig(\u0026#34;Summarizer\u0026#34;, \u0026amp;vnext.Config{ Name: \u0026#34;summarizer\u0026#34;, SystemPrompt: \u0026#34;You are a Summarizer Agent...\u0026#34;, Timeout: 60 * time.Second, LLM: vnext.LLMConfig{ Provider: \u0026#34;ollama\u0026#34;, Model: \u0026#34;gemma3:1b\u0026#34;, Temperature: 0.3, MaxTokens: 150, BaseURL: \u0026#34;http://localhost:11434\u0026#34; }, }) } Using OpenAI or Azure OpenAI\r#\rWhere the Ollama LLM config is defined above, you can swap in OpenAI or Azure OpenAI with minimal changes:\n// OpenAI LLM: vnext.LLMConfig{ Provider: \u0026#34;openai\u0026#34;, Model: \u0026#34;gpt-4\u0026#34;, APIKey: os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;), } // Azure OpenAI LLM: vnext.LLMConfig{ Provider: \u0026#34;azure\u0026#34;, Model: \u0026#34;gpt-4\u0026#34;, BaseURL: \u0026#34;https://your-resource.openai.azure.com/\u0026#34;, APIKey: os.Getenv(\u0026#34;AZURE_OPENAI_KEY\u0026#34;), } Notes:\nKeep the rest of the streaming code exactly the same; provider selection is handled via LLMConfig. Ensure the appropriate environment variables are set in your shell before running the examples. Building a sequential workflow\r#\rworkflow, err := vnext.NewSequentialWorkflow(\u0026amp;vnext.WorkflowConfig{ Mode: vnext.Sequential, Timeout: 180 * time.Second }) if err != nil { log.Fatal(err) } _ = workflow.AddStep(vnext.WorkflowStep{ Name: \u0026#34;research\u0026#34;, Agent: researcher, Transform: func(input string) string { return fmt.Sprintf(\u0026#34;Research the topic: %s. Provide key information, benefits, and current applications.\u0026#34;, input) }, }) _ = workflow.AddStep(vnext.WorkflowStep{ Name: \u0026#34;summarize\u0026#34;, Agent: summarizer, Transform: func(input string) string { return fmt.Sprintf(\u0026#34;Please summarize this research into key points:\\n\\n%s\u0026#34;, input) }, }) Running with streaming\r#\rctx := context.Background() stream, err := workflow.RunStream(ctx, topic) if err != nil { log.Fatal(err) } for chunk := range stream.Chunks() { switch chunk.Type { case vnext.ChunkTypeMetadata: if stepName, ok := chunk.Metadata[\u0026#34;step_name\u0026#34;].(string); ok { fmt.Printf(\u0026#34;\\nüîÑ [STEP: %s] %s\\n\u0026#34;, strings.ToUpper(stepName), chunk.Content) fmt.Println(\u0026#34;‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u0026#34;) } case vnext.ChunkTypeDelta: fmt.Print(chunk.Delta) case vnext.ChunkTypeDone: fmt.Println(\u0026#34;\\n‚úÖ Workflow step completed!\u0026#34;) } } result, err := stream.Wait() // final success/error Run it:\ncd examples/vnext/streaming_workflow go run . What you‚Äôll see:\nüöÄ vnext.Workflow Streaming Showcase ==================================== üîç Testing Ollama connection... ‚úÖ Ollama connection successful üåü vnext.Workflow Sequential Streaming ===================================== üéØ Topic: Benefits of streaming in AI applications üí¨ Real-time Workflow Streaming: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üîÑ [STEP: RESEARCH] Step 1/2: research Streaming is a really cool way to access content... üîÑ [STEP: SUMMARIZE] Step 2/2: summarize Based on the research findings, here are the key points: üéâ vnext.WORKFLOW STREAMING COMPLETED! When to use streaming vs. non‚Äëstreaming\r#\rWithout streaming:\nUser: \u0026#34;Research AI streaming benefits\u0026#34; System: [Working... 60‚Äì90s of silence] System: [Full results appear all at once] With streaming:\nUser: \u0026#34;Research AI streaming benefits\u0026#34; System: üîÑ [STEP: RESEARCH] ‚Ä¶ tokens stream live ‚Ä¶ System: üîÑ [STEP: SUMMARIZE] ‚Ä¶ tokens stream live ‚Ä¶ System: ‚úÖ Workflow completed Streaming shines when:\nThe task takes more than ~1‚Äì2 seconds You want visibility into multi‚Äëstep progress You‚Äôre building chat UIs or CLIs where responsiveness matters Tips, options, and best practices\r#\rAlways use contexts with timeouts for cancellation: ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second) defer cancel() After consuming chunks, call stream.Wait() to catch final errors and access the final result Need only text? Use text‚Äëonly streaming to reduce noise For UIs, include metadata to display current step/agent Tune buffer size and flush intervals for your UX/perf needs See core/vnext/STREAMING_GUIDE.md for:\nChunk types (Text, Delta, Thought, ToolCall, ToolResult, Metadata, Error, Done) Stream options: buffer size, thoughts/tool calls, metadata, flush interval Utilities: CollectStream, PrintStream, StreamToChannel, AsReader Troubleshooting\r#\rStream hangs or never finishes: use a context with timeout and ensure you read all chunks Missing output: verify you‚Äôre handling ChunkTypeDelta (token deltas) and/or ChunkTypeText Slow UI updates: try a larger buffer or longer flush interval Provider issues: confirm Ollama is running and the model is pulled Wrap‚Äëup\r#\rYou now have two paths:\nStart simple with examples/vnext/simple-streaming to understand token streaming Build richer, explainable systems with examples/vnext/streaming_workflow Both rely on the same Stream primitives, so once you‚Äôre comfortable with one, the other feels natural.\nIf you want to go deeper, open core/vnext/STREAMING_GUIDE.md and explore advanced options like tool‚Äëcall streaming, thought visibility, and custom stream builders.\n","date":"26 October 2025","externalUrl":null,"permalink":"/post/streaming-agenticgokit/","section":"Posts","summary":"","title":"Streaming AI Responses in Go with AgenticGoKit","type":"post"},{"content":"TL;DR: Create a professional research-to-report pipeline with two AI agents in under 60 lines of Go code.\nThe Challenge\r#\rYou want to build an AI system that:\nResearches a topic thoroughly Synthesizes findings into a professional report Handles the orchestration automatically Traditionally, this means wrestling with:\nComplex state management Manual prompt chaining Error handling across multiple LLM calls Coordinating agent communication What if it could be as simple as this?\nresearcher := createAgent(\u0026#34;researcher\u0026#34;, \u0026#34;Gather key facts...\u0026#34;, 0.7) reporter := createAgent(\u0026#34;reporter\u0026#34;, \u0026#34;Create a report...\u0026#34;, 0.5) workflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;research\u0026#34;, Agent: researcher}) workflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;report\u0026#34;, Agent: reporter}) result := workflow.Run(ctx, \u0026#34;What is Kubernetes?\u0026#34;) Spoiler: It is.\nIntroducing AgenticGoKit vNext\r#\rAgenticGoKit is a Go framework for building AI agent systems. The new vNext API makes multi-agent workflows ridiculously simple.\nLet\u0026rsquo;s build a Researcher ‚Üí Reporter pipeline that takes any topic and produces a professional report.\nThe Complete Solution (Under 60 Lines!)\r#\rHere\u0026rsquo;s the entire program:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/kunalkushwaha/agenticgokit/core/vnext\u0026#34; _ \u0026#34;github.com/kunalkushwaha/agenticgokit/plugins/llm/ollama\u0026#34; ) func main() { ctx := context.Background() // Step 1: Create two specialized agents researcher := createAgent(\u0026#34;researcher\u0026#34;, \u0026#34;Gather key facts and information about the topic.\u0026#34;, 0.7) reporter := createAgent(\u0026#34;reporter\u0026#34;, \u0026#34;Create a concise report with Summary, Key Points, and Conclusion.\u0026#34;, 0.5) // Step 2: Create a sequential workflow workflow, _ := vnext.NewSequentialWorkflow(\u0026amp;vnext.WorkflowConfig{ Mode: vnext.Sequential, Timeout: 600 * time.Second, }) // Step 3: Add agents as workflow steps workflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;research\u0026#34;, Agent: researcher}) workflow.AddStep(vnext.WorkflowStep{ Name: \u0026#34;report\u0026#34;, Agent: reporter, Transform: func(input string) string { return \u0026#34;Based on this research, create a professional report:\\n\\n\u0026#34; + input }, }) // Step 4: Run the workflow! workflow.Initialize(ctx) defer workflow.Shutdown(ctx) result, _ := workflow.Run(ctx, \u0026#34;What is Kubernetes?\u0026#34;) // Display results fmt.Println(\u0026#34;üîç RESEARCH FINDINGS:\u0026#34;) fmt.Println(result.StepResults[0].Output) fmt.Println(\u0026#34;\\nüìÑ FINAL REPORT:\u0026#34;) fmt.Println(result.StepResults[1].Output) fmt.Printf(\u0026#34;\\n‚úÖ Completed in %v\\n\u0026#34;, result.Duration) } // Helper to create an agent func createAgent(name, prompt string, temperature float32) vnext.Agent { agent, err := vnext.NewBuilder(name). WithConfig(\u0026amp;vnext.Config{ Name: name, SystemPrompt: prompt, LLM: vnext.LLMConfig{ Provider: \u0026#34;ollama\u0026#34;, Model: \u0026#34;gemma3:1b\u0026#34;, Temperature: temperature, MaxTokens: 400, }, Timeout: 600 * time.Second, }). Build() if err != nil { log.Fatalf(\u0026#34;Failed to create %s: %v\u0026#34;, name, err) } agent.Initialize(context.Background()) return agent } That\u0026rsquo;s it. Under 60 lines. No complex orchestration. No manual state management.\nBreaking It Down\r#\rStep 1: Create Specialized Agents\r#\rresearcher := createAgent(\u0026#34;researcher\u0026#34;, \u0026#34;Gather key facts and information about the topic.\u0026#34;, 0.7) reporter := createAgent(\u0026#34;reporter\u0026#34;, \u0026#34;Create a concise report with Summary, Key Points, and Conclusion.\u0026#34;, 0.5) Each agent is configured for its specific role:\nResearcher: Higher temperature (0.7) for creative exploration Reporter: Lower temperature (0.5) for focused, structured output The createAgent() helper encapsulates all the configuration:\nConnects to your LLM provider (Ollama, OpenAI, Azure, etc.) Sets the system prompt to define the agent\u0026rsquo;s role Configures temperature, token limits, timeouts One function call. One agent. Done.\nStep 2: Create the Workflow\r#\rworkflow, _ := vnext.NewSequentialWorkflow(\u0026amp;vnext.WorkflowConfig{ Mode: vnext.Sequential, Timeout: 600 * time.Second, }) AgenticGoKit supports multiple workflow patterns:\nSequential: Steps run in order (what we\u0026rsquo;re using) Parallel: Steps run concurrently DAG: Complex dependency graphs Loop: Iterative processing We chose sequential because we want: Research ‚Üí Report\nStep 3: Add Your Agents as Steps\r#\rworkflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;research\u0026#34;, Agent: researcher}) workflow.AddStep(vnext.WorkflowStep{ Name: \u0026#34;report\u0026#34;, Agent: reporter, Transform: func(input string) string { return \u0026#34;Based on this research, create a professional report:\\n\\n\u0026#34; + input }, }) This is where the magic happens:\nFirst step: Researcher agent processes the input topic Transform function: Reformats the research output into a prompt for the reporter Second step: Reporter agent creates the final report The Transform function is your secret weapon for prompt engineering between steps. It takes the output from one agent and shapes it perfectly for the next.\nStep 4: Run It!\r#\rworkflow.Initialize(ctx) defer workflow.Shutdown(ctx) result, _ := workflow.Run(ctx, \u0026#34;What is Kubernetes?\u0026#34;) fmt.Println(result.StepResults[0].Output) // Research findings fmt.Println(result.StepResults[1].Output) // Final report The workflow:\n‚úÖ Manages execution order ‚úÖ Handles errors gracefully ‚úÖ Passes data between steps ‚úÖ Returns all results in one structure You get access to:\nIndividual step outputs Overall success/failure status Timing information Token usage What You Get\r#\rRunning this with the topic \u0026ldquo;What is Kubernetes and why is it important?\u0026rdquo; produces:\nüîç RESEARCH FINDINGS: Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It was originally developed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF). Key aspects of Kubernetes: ‚Ä¢ Automates container deployment across clusters ‚Ä¢ Provides self-healing capabilities ‚Ä¢ Enables horizontal scaling ‚Ä¢ Supports rolling updates and rollbacks ‚Ä¢ Manages service discovery and load balancing üìÑ FINAL REPORT: Summary: Kubernetes is a powerful container orchestration system that has become the industry standard for deploying and managing cloud-native applications. Key Points: ‚Ä¢ Automates deployment and scaling of containerized applications ‚Ä¢ Provides self-healing and resilience capabilities ‚Ä¢ Enables microservices architecture at scale ‚Ä¢ Supported by all major cloud providers ‚Ä¢ Open-source with strong community backing Conclusion: Kubernetes has become essential infrastructure for modern cloud applications, enabling teams to build scalable, resilient systems with reduced operational complexity. ‚úÖ Completed in 4.2s Professional results. Minimal code.\nExtending the Workflow\r#\rWant to add more sophistication? It\u0026rsquo;s just as simple:\nAdd a Third Agent (Editor)\r#\reditor := createAgent(\u0026#34;editor\u0026#34;, \u0026#34;Proofread and improve the report\u0026#34;, 0.3) workflow.AddStep(vnext.WorkflowStep{Name: \u0026#34;edit\u0026#34;, Agent: editor}) Add Memory\r#\rresearcher := vnext.NewBuilder(\u0026#34;researcher\u0026#34;). WithConfig(config). WithMemory(). // Now remembers past interactions! Build() Add Conditional Logic\r#\rworkflow.AddStep(vnext.WorkflowStep{ Name: \u0026#34;summary\u0026#34;, Agent: summarizer, Condition: func(ctx context.Context, wfCtx *vnext.WorkflowContext) bool { // Only summarize if research is lengthy return len(wfCtx.GetStepResult(\u0026#34;research\u0026#34;).Output) \u0026gt; 1000 }, }) Use Different Models per Agent\r#\r// Heavy research with large model researcher.LLM.Model = \u0026#34;llama3.1:8b\u0026#34; // Fast reporting with small model reporter.LLM.Model = \u0026#34;gemma3:1b\u0026#34; Getting Started\r#\rPrerequisites\r#\rInstall Go 1.23+\nInstall Ollama (or use OpenAI/Azure)\n# macOS/Linux curl https://ollama.ai/install.sh | sh # Pull a model ollama pull gemma3:1b Install AgenticGoKit\ngo get github.com/kunalkushwaha/agenticgokit Run the Example\r#\rgit clone https://github.com/kunalkushwaha/AgenticGoKit cd AgenticGoKit/examples/vnext/researcher-reporter go run main.go Customize for Your Use Case\r#\rChange the topic:\nresult, _ := workflow.Run(ctx, \u0026#34;Your topic here\u0026#34;) Change the agents:\nanalyst := createAgent(\u0026#34;analyst\u0026#34;, \u0026#34;Analyze market trends\u0026#34;, 0.6) strategist := createAgent(\u0026#34;strategist\u0026#34;, \u0026#34;Create strategy recommendations\u0026#34;, 0.5) Change the workflow:\nworkflow, _ := vnext.NewParallelWorkflow(config) // Run steps concurrently // or workflow, _ := vnext.NewDAGWorkflow(config) // Complex dependencies The Bottom Line\r#\rMulti-agent AI systems don\u0026rsquo;t have to be complex. With AgenticGoKit vNext:\n‚úÖ Under 60 lines of code instead of 200+\n‚úÖ 4 simple steps instead of complex orchestration\n‚úÖ Production-ready error handling and state management\n‚úÖ Flexible - extend with memory, tools, streaming, and more\n‚úÖ Type-safe Go with excellent IDE support\nThe Philosophy\r#\r\u0026ldquo;Simple things should be simple. Complex things should be possible.\u0026rdquo;\nAgenticGoKit makes the simple case trivial (as you\u0026rsquo;ve seen), while still supporting:\nRAG with vector databases MCP tool integration Custom memory providers Complex DAG workflows And much more\u0026hellip; Try It Yourself\r#\rThe complete example is available in the AgenticGoKit repository.\nWhat will you build?\nResearch assistants? Content pipelines? Data analysis workflows? Something entirely new? Share your creations! We\u0026rsquo;d love to see what you build.\nResources\r#\rGitHub: kunalkushwaha/AgenticGoKit Documentation: Full vNext Guide Examples: More vNext Examples Community: Discussions Questions? Open an issue or start a discussion on GitHub.\nWant to contribute? PRs are always welcome!\nAppendix: Quick Reference\r#\rAgent Creation\r#\ragent, err := vnext.NewBuilder(name). WithConfig(\u0026amp;vnext.Config{ Name: name, SystemPrompt: prompt, LLM: vnext.LLMConfig{ Provider: \u0026#34;ollama\u0026#34;, Model: \u0026#34;gemma3:1b\u0026#34;, Temperature: 0.7, MaxTokens: 400, }, Timeout: 600 * time.Second, }). Build() Workflow Types\r#\rvnext.NewSequentialWorkflow(config) // Steps in order vnext.NewParallelWorkflow(config) // Steps concurrently vnext.NewDAGWorkflow(config) // Dependency graph vnext.NewLoopWorkflow(config) // Iterative execution Workflow Step\r#\rvnext.WorkflowStep{ Name: \u0026#34;step-name\u0026#34;, Agent: agent, Transform: func(input string) string { return modified }, Condition: func(ctx, wfCtx) bool { return true }, } Result Access\r#\rresult.Success // bool result.FinalOutput // string result.StepResults[i] // Individual step results ([]StepResult) result.Duration // time.Duration result.TotalTokens // int result.ExecutionPath // []string - order of executed steps Happy building! üöÄ\n","date":"20 October 2025","externalUrl":null,"permalink":"/post/building-multi-agent-workflows/","section":"Posts","summary":"","title":"Building Multi-Agent Workflows in Go: Simpler Than You Think","type":"post"},{"content":"\rImagine you\u0026rsquo;re building an AI agent for the first time. You want it to translate text, summarize documents, or perhaps even plan tasks. But as you dive in, you realize that orchestrating these agents, managing their states, and ensuring smooth communication between them is more complex than anticipated.\nEnter AgentFlow‚Äîa lightweight, Go-native framework designed to simplify the creation of event-driven AI agent workflows. With AgentFlow, you can focus on defining your agents\u0026rsquo; behaviors, while the framework handles the orchestration, state management, and integration with large language models (LLMs) like OpenAI\u0026rsquo;s GPT-4.\nIn this blog post, I will introduce you to AgentFlow\u0026rsquo;s core concepts, demonstrate how to build a simple multi-agent workflow using LLMs, explore the built-in tracing functionality, and show how AgentFlow can streamline your AI development process.\nWhat is AgentFlow?\r#\rAgentFlow is a Go-based framework that enables developers to build AI agents as modular, event-driven components. Each agent is a simple Go function that processes an Event, updates a shared State, and returns a result. AgentFlow provides built-in orchestration patterns‚Äîsuch as sequential, parallel, and looped execution‚Äîto manage complex workflows with ease.\nKey Features\r#\rModular Agent Design: Define agents as individual Go functions, promoting code reusability and simplicity. Built-in Orchestration: Utilize predefined patterns like SequentialAgent, ParallelAgent, and LoopAgent to structure your workflows. LLM Integration: Seamlessly integrate with LLM providers like OpenAI, Azure OpenAI, and Ollama through a unified ModelProvider interface. Observability: Leverage the agentcli tool to trace and debug your workflows effectively. Extensibility: Enhance agent capabilities by registering custom tools and functions. Core Workflow Patterns\r#\rAgentFlow supports various orchestration patterns to manage how agents interact within a workflow. Let\u0026rsquo;s explore the primary patterns:\nSequential Execution\r#\rAgents are executed one after another, with each agent receiving the updated state from the previous one.\ngraph TD Start --\u003e Agent1 --\u003e Agent2 --\u003e Agent3 --\u003e End\rParallel Execution\r#\rMultiple agents are executed concurrently, and their outputs are merged into a single state.\ngraph TD Start --\u003e AgentA Start --\u003e AgentB Start --\u003e AgentC AgentA --\u003e Merge AgentB --\u003e Merge AgentC --\u003e Merge Merge --\u003e End\rLoop Execution\r#\rAn agent or group of agents is executed repeatedly until a specified condition is met or a maximum number of iterations is reached.\ngraph TD Start --\u003e LoopStart LoopStart --\u003e AgentX AgentX --\u003e ConditionCheck ConditionCheck -- Yes --\u003e End ConditionCheck -- No --\u003e LoopStart\rBuilding a Multi-Agent LLM Workflow\r#\rLet‚Äôs walk through creating a simple workflow that translates text into French and then summarizes it, using OpenAI\u0026rsquo;s GPT-4 model.\nPrerequisites\r#\rGo installed on your system. An OpenAI API key set as the OPENAI_API_KEY environment variable. Step 1: Define the Agents\r#\rWe‚Äôll create two agents: one for translation and another for summarization. AgentFlow includes built-in support for LLMs via a ModelProvider interface, and you can access it through the context in your handlers. Here\u0026rsquo;s an example inspired by opeanai example:\nfunc TranslateAgent(ctx context.Context, ev core.Event, st core.State) (core.AgentResult, error) { provider := llms.GetProviderFromContext(ctx) input := ev.GetData()[\u0026#34;text\u0026#34;].(string) prompt := fmt.Sprintf(\u0026#34;Translate the following text to French:\\n\\n%s\u0026#34;, input) translated, err := provider.Call(ctx, prompt) if err != nil { return core.AgentResult{}, err } st.Set(\u0026#34;translated\u0026#34;, translated) return core.AgentResult{OutputState: st}, nil } func SummarizeAgent(ctx context.Context, ev core.Event, st core.State) (core.AgentResult, error) { provider := llms.GetProviderFromContext(ctx) input := ev.GetData()[\u0026#34;text\u0026#34;].(string) prompt := fmt.Sprintf(\u0026#34;Summarize the following text:\\n\\n%s\u0026#34;, input) summary, err := provider.Call(ctx, prompt) if err != nil { return core.AgentResult{}, err } st.Set(\u0026#34;summary\u0026#34;, summary) return core.AgentResult{OutputState: st}, nil } Step 2: Set Up the Workflow\r#\rWe\u0026rsquo;ll use the ParallelAgent to execute both agents concurrently.\nhandlers := map[string]factory.AgentHandlerFunc{ \u0026#34;translate\u0026#34;: TranslateAgent, \u0026#34;summarize\u0026#34;: SummarizeAgent, } flow := agents.NewParallelAgent([]string{\u0026#34;translate\u0026#34;, \u0026#34;summarize\u0026#34;}) runner := factory.NewRunnerWithConfig(factory.RunnerConfig{ Agents: handlers, Flow: flow, }) Step 3: Execute the Workflow\r#\rEmit an event to start the workflow and retrieve the results.\nrunner.Start(context.Background()) session := runner.Emit(core.NewEvent(\u0026#34;parallel\u0026#34;, core.EventData{\u0026#34;text\u0026#34;: \u0026#34;Hello, AgentFlow!\u0026#34;}, nil)) trace := runner.DumpTrace(session) result := trace[len(trace)-1].State fmt.Println(\u0026#34;Translated Text:\u0026#34;, result.Get(\u0026#34;translated\u0026#34;)) fmt.Println(\u0026#34;Summary:\u0026#34;, result.Get(\u0026#34;summary\u0026#34;)) runner.Stop() Tracing Your Agent Workflow\r#\rAgentFlow includes built-in support for tracing, allowing you to inspect each step of your agent workflow‚Äîwhat data was passed, how state changed, and what each agent did.\nWhen you run a workflow using runner.Emit(), AgentFlow collects a trace of each step. You can dump and inspect this trace using runner.DumpTrace().\n// Emit the initial event session := runner.Emit(core.NewEvent(\u0026#34;parallel\u0026#34;, core.EventData{\u0026#34;text\u0026#34;: \u0026#34;Hello, AgentFlow!\u0026#34;}, nil)) // Dump and inspect the trace trace := runner.DumpTrace(session) for i, step := range trace { fmt.Printf(\u0026#34;Step %d:\\n\u0026#34;, i+1) fmt.Printf(\u0026#34; Agent ID: %s\\n\u0026#34;, step.AgentID) fmt.Printf(\u0026#34; Input Event: %+v\\n\u0026#34;, step.InputEvent.Data) fmt.Printf(\u0026#34; Output State: %+v\\n\u0026#34;, step.State.Data) } Example Output\r#\rStep 1: Agent ID: translate Input Event: map[text:Hello, AgentFlow!] Output State: map[translated:Bonjour, AgentFlow!] Step 2: Agent ID: summarize Input Event: map[text:Hello, AgentFlow!] Output State: map[summary:An overview of AgentFlow.] Visualizing Workflow with agentcli\r#\rYou can also use the agentcli tool to load and inspect trace files in a more visual or structured way:\nagentcli trace --load trace.json --pretty This makes it easier to debug workflows, compare outputs, and track down issues when agents don‚Äôt behave as expected.\nGetting Started\r#\rTo explore AgentFlow further:\nInstall AgentFlow:\ngo get github.com/kunalkushwaha/agentflow@latest Explore Examples:\nNavigate to the examples/multi_agent directory in the AgentFlow repository to see more complex workflows in action.\nConsult the Documentation:\nRefer to the docs directory for detailed guides and architectural overviews.\nUse the CLI Tool:\nLeverage agentcli to trace and debug your workflows effectively.\nAgentFlow simplifies the process of building AI agent workflows in Go, allowing you to focus on your application\u0026rsquo;s logic rather than the intricacies of orchestration and state management. Whether you\u0026rsquo;re just starting with AI agents or looking to streamline your existing workflows, AgentFlow provides the tools and patterns to accelerate your development.\nExplore AgentFlow today and experience a more straightforward approach to building AI-powered applications.\n","date":"12 May 2025","externalUrl":null,"permalink":"/post/building-ai-agents-with-agentflow/","section":"Posts","summary":"","title":"Building AI Agents with AgentFlow: A Beginner's Journey","type":"post"},{"content":"","date":"28 January 2020","externalUrl":null,"permalink":"/tags/configmap/","section":"Tags","summary":"","title":"Configmap","type":"tags"},{"content":"\rA Pod is collection of containers running in shared namespace. A container is created from Docker/OCI Image, which is immutable in nature i.e. data bundled into the image cannot be changed later in image.\n\u0026ldquo;While immutable image of application is great for security, but putting configurations \u0026amp; data like security keys is bad for security as well as portability\u0026rdquo;\nThere are various ways to pass data to application in containers, without changing the flow of application e.g. by setting environment variables or using volume mount to provide configuration file at the time of container creation.\nIn Kubernetes, ConfigMaps and Secrets are two objects using which configuration data can be passed to Pods and containers running within them.\nIn this blog, I will give you a walkthough of how to use ConfigMap and Secrets with a Pod.\nConfigMaps: \u0026ldquo;It allows you to decouple configuration artifacts from image content to keep containerized applications portable\u0026rdquo;\nIt stores the data as key-value pairs, where value can be a string or entire content of file. Creating ConfigMap\r#\rSince ConfigMap is standard object of Kubernetes, it can be created using command kubectl create configmap as well as kubectl apply with ConfigMap definition file. e.g. ConfigMap with two key-value pairs log=debug \u0026amp; environment=test\n$ kubectl create configmap testapp-pod-config --from-literal=log=debug --from-literal=environment=test configmap/testapp-pod-config created Literal values can be seen using with kubectl describe command\n$ kubectl describe configmap testapp-pod-config Name: testapp-pod-config Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== environment: ---- test log: ---- debug Events: \u0026lt;none\u0026gt; Simillarly, The ConfigMap can be created using files too.\ne.g. creating a config map to store ha-proxy config stored in ./ha-proxy.cfg file\n$ kubectl create configmap ha-proxy-cfg --from-file=ha-proxy-cfg=ha-proxy-config.cfg configmap/ha-proxy-cfg created You may go through various examples of ConfigMap creation in Kubernetes documentation\nHow to use it with Pod?\r#\rConfigMap data can be consumed in Pod broadly either as an environment variable or by mounting them as files in a container. Lets see in detail, how this is done.\n1. Import all data as environment variable:\r#\rThis is simplest form, where we will import all data present in a ConfigMap into the Pod as environment variable. Use envFrom \u0026amp; configMapRef to import all. In the below example, I am importing all the key-value pairs of ConfigMaptestapp-pod-config which we created above.\napiVersion: v1 kind: Pod metadata: name: testapp-pod spec: containers: - name: testapp-container image: docker.io/library/busybox:latest command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;env\u0026#34;] envFrom: - configMapRef: name: testapp-pod-config You can see below, both key-value pairs are now part of environment variables of Pod.\n$ kubectl apply -f testapp-pod.yaml pod/testapp-pod created $ kubectl logs testapp-pod | grep -E -i -w \u0026#39;log|environment\u0026#39; environment=test log=debug Note\nIf you are looking at container environment variables first time, don\u0026rsquo;t get intimidated with list of environment variables list not related to your pod/container.\nA list of all services that were running when a Container is created is available to that Container as environment variables.\n2. Import selected keys as environment variables:\r#\rSometimes, not all key-value pairs are required to be exposed to container especially when same ConfigMap is used by multiple Pods. In such cases, we can import selected keys to a container.\nThis can be done using env , valueFrom \u0026amp; configMapKeyRef. Below is example, where we import only log key from test-pod-config\napiVersion: v1 kind: Pod metadata: name: testapp-pod-single-key spec: containers: - name: testapp-container image: docker.io/library/busybox:latest command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;env\u0026#34;] env: - name: \u0026#34;log-level\u0026#34; valueFrom: configMapKeyRef: name: testapp-pod-config key: \u0026#34;log\u0026#34; You can see only log key is imported and its renamed as log-level as defined in above yaml file. Hence while importing key wise, we can even rename the environment variables too.\n$ kubectl apply -f example-single-config-pod.yaml pod/testapp-pod-single-key created $ kubectl logs testapp-pod-single-key | grep -E -i -w \u0026#39;log|environment\u0026#39; log-level=debug Note\nSince, env and envFrom are defined as list, environment list from multiple ConfigMaps can be imported.\n3. Import as a volume:\r#\rThis option is helpful, if the application bundled in the container needs to be configured using config file. We can mount the file to specific path within container.\nFor this, we need to first use ConfigMap as volume of Pod and then mount within the container.\nIn below example, a pod will mount the ConfigMap ha-proxy-cfg.\napiVersion: v1 kind: Pod metadata: name: testapp-pod-mnt spec: volumes: - name: ha-proxy-volume configMap: name: ha-proxy-cfg containers: - name: testapp-container image: docker.io/library/busybox:latest command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;ls -l /etc/config/\u0026#34;] volumeMounts: - name: ha-proxy-volume mountPath: \u0026#34;/etc/config\u0026#34; You can see below, the key became filename and value assigned to key will be contents of file(not shown) Using configMap \u0026amp; just name items, all the keys defined in ConfigMap will be mounted on provided mountPath\n$ kubectl get configmaps NAME DATA AGE ha-proxy-cfg 1 20h $ kubectl apply -f mount-file-example.yaml pod/testapp-pod-mnt created $ kubectl logs testapp-pod-mnt total 0 lrwxrwxrwx 1 root root 19 Jan 29 05:17 ha-proxy-cfg -\u0026gt; ..data/ha-proxy-cfg 4. Import selected keys to volume.\r#\rAgain similar to #2 case, selected keys can be imported within mounted volume. This can be done by putting keys in volumes section of Pod as shown below.\napiVersion: v1 kind: Pod metadata: name: testapp-pod-mnt-single spec: volumes: - name: ha-proxy-volume-single configMap: name: ha-proxy items: - key: \u0026#34;ha-proxy-cfg\u0026#34; path: ha-proxy.cfg containers: - name: testapp-container image: docker.io/library/busybox:latest command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;ls -l /etc/config/\u0026#34;] volumeMounts: - name: ha-proxy-volume-single mountPath: /etc/config As you can see, the filename can be chosen in this case.\n$ kubectl apply -f mount-single-file-example.yaml pod/testapp-pod-mnt-single created $ kubectl logs testapp-pod-mnt-single total 0 lrwxrwxrwx 1 root root 19 Jan 29 05:48 ha-proxy.cfg -\u0026gt; ..data/ha-proxy.cfg Note\nAny changes to ConfigMap will be refletcted in volumes too. So this is helpful in cases where application bundled in containers can read changes and you dont need to create new pods to propagate the changes in configuration files\nInteresting Facts: Q. One question normally arise how big data can be stored in configMap. A. Kubernetes do not put any hard limits on ConfigMap size, though it stores configMaps in etcd. Etcd has hard limit to store value upto 1MB Ref : https://github.com/kubernetes/kubernetes/issues/19781 That is all for using ConfigMaps in Pod.\nSecrets are another object similar to ConfigMaps that is used to store data in encrypted form. Since this blog became quite long, so shall post about using Secrets in Pods in next blog.\nFeel free to comment, if you find anything I missed, need to be corrected or can be added as Interesting Facts.\n","date":"28 January 2020","externalUrl":null,"permalink":"/post/configmaps-and-pods/","section":"Posts","summary":"","title":"ConfigMaps and Pods","type":"post"},{"content":"","date":"28 January 2020","externalUrl":null,"permalink":"/tags/k8s/","section":"Tags","summary":"","title":"K8s","type":"tags"},{"content":"","date":"28 January 2020","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"28 January 2020","externalUrl":null,"permalink":"/tags/pods/","section":"Tags","summary":"","title":"Pods","type":"tags"},{"content":"","date":"28 January 2020","externalUrl":null,"permalink":"/tags/secret/","section":"Tags","summary":"","title":"Secret","type":"tags"},{"content":"","date":"14 October 2016","externalUrl":null,"permalink":"/tags/chef/","section":"Tags","summary":"","title":"Chef","type":"tags"},{"content":"","date":"14 October 2016","externalUrl":null,"permalink":"/tags/docker-machine/","section":"Tags","summary":"","title":"Docker Machine","type":"tags"},{"content":"","date":"14 October 2016","externalUrl":null,"permalink":"/tags/infrakit/","section":"Tags","summary":"","title":"Infrakit","type":"tags"},{"content":"","date":"14 October 2016","externalUrl":null,"permalink":"/tags/puppet/","section":"Tags","summary":"","title":"Puppet","type":"tags"},{"content":"Docker recently open sourced new project Infrakit at ContainerCon 2016, Berlin.\nAs project describes itself as\n\u0026ldquo;A toolkit for creating and managing declarative, self-healing infrastructure\u0026rdquo;\nLets try to understand what it does and how it adds value to existing tools.\nInfrakit\r#\rProblem\r#\rManaging docker on different infrastructure is difficult and not portable.\nHistory\r#\rInitially Docker machine project was focused for building docker environment on various platforms, but there were lot of issues mainly due to unreliable third party drivers.\nUser experience was not great on developer platforms like Mac (VirtualBox issues) Idea of declarative infrastructure was thought in Docker Machine Project also.\nFew Proposals and PR were made for declarative infrastructure but never got merged due to user experience issues and Docker Inc\u0026rsquo;s long term Roadmap vision.\nNOTE:Docker Machine is still very much active project with great userbase.\nLater during Dockercon16, while interacting with docker developers \u0026amp; maintainer of Docker Machine, I understood, long term plan is to merge docker machine capabilities into docker engine.\nMeanwhile Docker had already released Docker For Mac. It is amazing product, as it just works and upgrade itself for new releases too.\nDocker for AWS/Azure were announced in Dockercon16. Infrakit is born out of these projects.\nOverview\r#\rUnlike most of docker projects, Infrakit is not about containers, it is about preparing \u0026amp; managing Infrastructure for distributed computing. Infrakit by design is build of active process(a.k.a. plugins) which collaborate with each other to analyze and take action to bring infrastructure in desired state. As other plugins in Docker Projects, Infrakit plugins also communicate over unix sockets and use HTTP protocol. This makes plugin implementation language agnostic and can be deployed separately as containers.\nThese Plugins are also building block of Infrakit to manage infrastructure.\nCurrently there are three kind of plugins supported by Infrakit which represents different layer of abstraction representing whole stack of infrastructure. Lets go through each and try to understand their roles.\nGroups\r#\rGroups plugins are for managing cluster of machine. These clusters can be represented by two set of machines.\nCattle or Replica : Machines with identical configuration. Pets or Quorum : Machine with slightly different properties like Ordering and Identity. The size of group can be increased and decreased. While changing configuration of machines, rolling updates by swarmkit ensures, no downtime and smooth up-gradation. Following actions can be done on group plugin.\nwatch/ unwatch a group (start / stop managing a group) inspect a group trigger an update the configuration of a group - like changing its size or underlying properties of instances. stop an update destroy a group Instance\r#\rInstance plugin represent, one instance of machine can be Cloud instance or just a VM. These Cloud instance or VMs have same specs. Instances define spec of VM or Cloud instance and also we can associate some tags to identify etc.\nFlavors\r#\rFlavor plugin give flavor to each instance. i.e. What exactly should one instance should contain and running. e.g. setting up software packages etc.\nWith these plugins all kind of infrastructure from machine instance point of view can be configured.\nI won\u0026rsquo;t go into implementation details in this blog, but if you want to play with Infrakit, they have very good introductory tutorial. You must try.\nHow Infrakit is different from existing products and solutions available?\r#\rConfiguration Management softwares(Puppet/Chef etc).\r#\rAll configuration management software works, once machine are up and running. They do not have capability to create and destroy the computing instances. Infrakit\u0026rsquo;s itself don\u0026rsquo;t have any defined syntax for configuring such as Puppet/Chef. So Puppet/Chef could to be used in Infakit\u0026rsquo;s flavor plugins, where computing node is configured. Terraform/CloudFormation/Heat.\r#\rInfrakit is similar to these, in terms of it also works on configuration plus state reconciliation feature i.e. Infrakit not only builds the infrastructure but also monitors them for changes and restore to expected state. i.e. If some instances in cluster stops, it will create new instances. Terraform/CloudFormation/Heat can be used as Instance Plugins. Example of Terraform plugin is provided by Infrakit Also, Infrakit is not polished product like Terraform, it is a framework which can be used by any existing/new product, to manage different infrastructure is similar fashion. It abstracts the details of different infrastructure\u0026rsquo;s to end user. Final Notes\r#\rInfrakit is amazing piece of code \u0026amp; concept for build self healing infrastructure, like Swarmkit to building distributed softwares. Though it is very early in development, so community contribution in ideas and code may bring more changes(in good ways) in Project.\nIssues like preparing docker platform with third party services/framework enabled for Storage/Networking/Security plugins should be far more smoother and I am looking forward to work on same.\n","date":"14 October 2016","externalUrl":null,"permalink":"/post/understanding-infrakit/","section":"Posts","summary":"","title":"Understanding Infrakit","type":"post"},{"content":"","date":"12 August 2016","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"12 August 2016","externalUrl":null,"permalink":"/tags/fixed-ip/","section":"Tags","summary":"","title":"Fixed-Ip","type":"tags"},{"content":"","date":"12 August 2016","externalUrl":null,"permalink":"/tags/legacy/","section":"Tags","summary":"","title":"Legacy","type":"tags"},{"content":"If title of this blog, attracted you towards this blog, Most likely, you will be one of us, who want to migrate old traditional application\u0026rsquo;s in container environment. This blog covers my experience of Migrating old traditional application into Docker.\nBefore beginning with migration procedure, Let me explain requirements I had to met with migrated system.\nProblems\r#\rOld apps running on servers/VMs, resource utilization not optimal. High maintenance cost. OS running is no longer supported by vendors, so in-house support required. Maintenance downtime high. Platform to adopt DevOps practice for fast refactoring. Challenges\r#\rMigrate Application with least changes in existing platform and infrastructure use. Use same OS distribution. No rewriting application for new OS. No changes in network infrastructure. Behavioral changes for accessing the application should be none. Use hardware load balancer. Use Fixed-IP with pre-approved MAC address. Solution\r#\rMigrating old applications into containers can be divided into two parts.\nApplication code/binary migration into image. Application deployment, as per requirements. The first part is specific to each application, as every application have different challenges for migration. I will not discuss details of that. Though few of things need be considered before migrating.\nDocker Host\r#\rWhich OS/distribution to be chosen for docker host. Ideally any distribution which have LTS should be fine.\nThough, sometime you may need to consider alternate from latest due compatibility with application. To check application compatibility, follow the below Checklist.\nApplication compatibility Checklist\r#\rCheck if any changes in ABI for linux kernel functionality and libraries used by application. If ABI breaks, either we need to choose old kernel, where you may need to work little bit to make docker work properly. OR, recompile application with new libraries. Any special Kernel settings required by application. Host kernel parameters may need to be tweaked and/or cgroups/namespaces need to be adjusted accordingly. After detailed analysis, we can decide and configure Docker Host.\nApplication migration\r#\rApplication migration in my case is simple. Just containerize the whole application. i.e. Build an image/rootfs with old libraries and application code. Typically old application requires more then one process to run, using Supervisord, it can be achieved.\nNow once, you successfully able build \u0026amp; test image for application, Lets focus on how it can be deployed with least changes to current setup.\nDeployment\r#\rAs started in challenges section, we are using hardware load-balancer and want to continue the same. Also, this application is bind to fixed-IP and specific MAC address.\nSince docker supports macvlan network driver, the above two goals can be easily achieved. Using macvlan driver, we can put a container on same network as old servers. Also macvlan user less CPU and slightly better throughput then bridge network.\nLets go through its details.\nOn docker host, lets create a macvlan network.\n$ docker network create -d macvlan --subnet=172.20.20.0/18 --gateway=172.20.0.1 -o parent=enp1s0 -o macvlan_mode=bridge maclan\r798b8a2ee7988dc0388c9985eaa7f0bc9373f11cf1be4a3b3a44abee162442fd\r$ docker network ls\rNETWORK ID NAME DRIVER SCOPE\r9a9ff4851e71 bridge bridge local ddbc211bcf8f docker_gwbridge bridge local a7211f1ae74d host host local 8aetfb2dtwmu ingress overlay swarm 798b8a2ee798 maclan macvlan local d43c2f84509b none null local NOTE: macvlan network is local to host, so this cannot be used over swarm.\nNow, I need to create an network interfaces for my containers. I need to deploy two containers as replacement of two Application servers. For security reason, I need to assign predefined MAC address to this interface. But with macvlan driver this is also possible.\nNow my network is ready, I can deploy my applications on same. Lets create containers on these network.\n$ docker run --net=maclan --ip=172.20.20.51 --mac-address 08:00:27:0B:1C:FE -itd alpine sh 1f0307c12ea7a666bffc4224665f304aae67056a7cbc6f037ec19d73b5a8a64d\r$ docker run --net=maclan --ip=172.20.20.52 --mac-address 08:00:27:3F:FE:8E -itd alpine sh 72849de4f1e23b2bd2394ef579c48045260b4f8f0026a601c5ac93514bc90e0d\r$ docker ps\rCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\r72849de4f1e2 alpine \u0026#34;sh\u0026#34; 5 seconds ago Up 2 seconds grave_pike\r1f0307c12ea7 alpine \u0026#34;sh\u0026#34; 49 seconds ago Up 46 seconds elegant_yalow Test it and containers are reachable from other machine on network.\n$ ping 172.20.20.51\rPING 172.20.20.51 (172.20.20.51): 56 data bytes\r64 bytes from 172.20.20.51: seq=0 ttl=64 time=0.090 ms\r64 bytes from 172.20.20.51: seq=1 ttl=64 time=0.057 ms\r64 bytes from 172.20.20.51: seq=2 ttl=64 time=0.058 ms\r^C\r--- 172.20.20.51 ping statistics ---\r3 packets transmitted, 3 packets received, 0% packet loss\rround-trip min/avg/max = 0.057/0.068/0.090 ms\r$ ping 172.20.20.52\rPING 172.20.20.52 (172.20.20.52): 56 data bytes\r64 bytes from 172.20.20.52: seq=0 ttl=64 time=0.075 ms\r64 bytes from 172.20.20.52: seq=1 ttl=64 time=0.058 ms\r64 bytes from 172.20.20.52: seq=2 ttl=64 time=0.066 ms\r^C\r--- 172.20.20.52 ping statistics ---\r3 packets transmitted, 3 packets received, 0% packet loss\rround-trip min/avg/max = 0.058/0.066/0.075 ms Now these containers have successfully replaced the two app servers, and for external load balancer, there is no changes.\nNOTE: Alternative to macvlan is IPVlan, but since it is not yet stable in docker, I choose macvlan\nSo what are the Benefits, I finally achieved through this migration.\nBenefits\r#\rPortability: Application,can be migrated to different machines easily. Flexibility: Image can be shared and experiment within team. Modularity: Provides a base to break application into further smaller services. Reuse: Reuse current infrastructure like network, load balancer. Least disturbance. Hope this blog will help you to find some answers, if you are looking for migrating legacy/old/monolithic applications to docker.\n","date":"12 August 2016","externalUrl":null,"permalink":"/post/migrating-traditional-apps-to-docker/","section":"Posts","summary":"","title":"Migrating traditional apps to docker","type":"post"},{"content":"","date":"12 August 2016","externalUrl":null,"permalink":"/tags/migration/","section":"Tags","summary":"","title":"Migration","type":"tags"},{"content":"Last friday (22nd July), I gave DockerCon16 Recap Talk and demo at Docker Meetup Tokyo\nThis blog post gives a walk through my Docker Meetup demonstration, and you can follow these steps to try at your machines.\nIn DockerCon16, docker 1.12 RC was announced. This release has an important \u0026amp; interesting feature called docker swarm mode\nGet Docker 1.12 RC.\r#\rIf you are using Docker for Mac or Docker for Windows, You already have docker 1.12RC installed on your machine.\nYou can check version\n$ docker version\rClient:\rVersion: 1.12.0-rc4\rAPI version: 1.24\rGo version: go1.6.2\rGit commit: e4a0dbc\rBuilt: Wed Jul 13 03:28:51 2016\rOS/Arch: darwin/amd64\rExperimental: true\rServer:\rVersion: 1.12.0-rc4\rAPI version: 1.24\rGo version: go1.6.2\rGit commit: e4a0dbc\rBuilt: Wed Jul 13 03:28:51 2016\rOS/Arch: linux/amd64\rExperimental: true In case you are using linux or not using above, you can get latest releases from https://github.com/docker/docker/releases\nCreate demo docker hosts.\r#\rTo create a swarm of docker hosts, you require more then one docker-host. I use docker-machine to create my docker hosts.\nWith following commands, my three docker hosts will be ready with docker 1.12 RC installed with default boot2docker image.\n$docker-machine -d virtualbox manager\r$docker-machine -d virtualbox worker1\r$docker-machine -d virtualbox worker2\r$ docker-machine ls\rNAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS\rmanager * virtualbox Running tcp://192.168.99.100:2376 v1.12.0-rc4\rworker1 - virtualbox Running tcp://192.168.99.101:2376 v1.12.0-rc4\rworker2 - virtualbox Running tcp://192.168.99.102:2376 v1.12.0-rc4 Note: If you work behind proxy, use command as docker-machine -d virtualbox -engine-env http_proxy=\u0026quot;example.com:port\u0026quot; manager\nOnce, all machines are up and running, we can create swarm.\nInitialize swarm.\r#\rTo run command on my manager node, I need to set my environment variables, so all docker commands gets executed at manager. This can be achieved by single command as follows.\n$ eval $(docker-machine env manager) Now initialize docker swarm mode.\n$ docker swarm init --listen-addr $(docker-machine ip manager):2377\rNo --secret provided. Generated random secret:\raj92ivakk282slwal0ujwaloj\rSwarm initialized: current node (2ksrqgk2x3vbjh3bw0aly5dwr) is now a manager.\rTo add a worker to this swarm, run the following command:\rdocker swarm join --secret aj92ivakk282slwal0ujwaloj \\\r--ca-hash sha256:1f7176d2474cf8dd3fa7a29e46ce42250c5a0aecaf07e40e014f039a7bf1e5ba \\\r192.168.99.100:2377 The above command initializes and generates relevant secrets and CA Key for swarm.\nCurrently swarm is created with only one node \u0026ldquo;master\u0026rdquo;\n$ docker node ls\rID HOSTNAME MEMBERSHIP STATUS AVAILABILITY MANAGER STATUS\r2ksrqgk2x3vbjh3bw0aly5dwr * manager Accepted Ready Active Leader Adding Workers node\r#\rTo add swarm workers docker swarm join command will be used.\n$ eval $(docker-machine env worker1)\r$ docker swarm join --secret aj92ivakk282slwal0ujwaloj \\\r\u0026gt; --ca-hash sha256:1f7176d2474cf8dd3fa7a29e46ce42250c5a0aecaf07e40e014f039a7bf1e5ba \\\r\u0026gt; 192.168.99.100:2377\rThis node joined a Swarm as a worker.\r$ eval $(docker-machine env worker2)\r$ docker swarm join --secret aj92ivakk282slwal0ujwaloj \\\r--ca-hash sha256:1f7176d2474cf8dd3fa7a29e46ce42250c5a0aecaf07e40e014f039a7bf1e5ba \\\r192.168.99.100:2377\rThis node joined a Swarm as a worker. To view complete list of nodes in swarm, we need to query swarm master.\nNOTE: In case of multiple masters, query can be made to any master\n$ eval $(docker-machine env manager)\r$ docker node ls\rID HOSTNAME MEMBERSHIP STATUS AVAILABILITY MANAGER STATUS\r0cs0e5phve5onp41pxfe9c1kj worker1 Accepted Ready Active\r2ksrqgk2x3vbjh3bw0aly5dwr * manager Accepted Ready Active Leader\rc6nvb1ljj9mntt1v8qlmxl2my worker2 Accepted Ready Active Thats it! two commands create whole docker swarm. No external KV Store, No tricky CA Key generation, It all just two commands\nMonitoring events of swarm\r#\rIn swarm mode all managers have consistent view that enables, the monitoring of whole swarm through any manager node. All you need to listen at /var/run/docker.sock of any manager node.\nCreating Visualizer for Swarm.\r#\rVisualizer is a simple nodejs app, which listen on /var/run/docker.sock and show nodes present in swarm and containers within as boxes.\n$ docker run -it -d -p 8080:8080 -e HOST=$(docker-machine ip manager) -v /var/run/docker.sock:/var/run/docker.sock manomarks/visualizer Open the visualizer in browser with http://192.168.99.100:8080, here \u0026ldquo;192.168.99.100\u0026rdquo; is manager node IP.\nLets deploy some Application in swarm\r#\rTill now applications can be deployed using docker run command. To support orchestration, docker 1.12 have introduced docker service command. Using service commands, we can define properties of an application, which docker swarm mode tries to reconcile in case of any application errors or node failure.\nFor demo, I have a simple application demoapp, which is a simple web-server listen on port 5000, prints the some message. Lets try to deploy this application.\nSince, I want my application to be spread all over swarm, and all instance should be discoverable, at first, I will create an overlay network.\nLets create overlay network using docker network create command.\n$ docker network create -d overlay mynet\r2xjpxwugr0glog7sqywdsnbn6\r$ docker network ls\rNETWORK ID NAME DRIVER SCOPE\r01e7e286803b bridge bridge local\r7a512b485351 docker_gwbridge bridge local\r6712d943241e host host local\rdyk8o6w5cddo ingress overlay swarm\r2xjpxwugr0gl mynet overlay swarm\rae968fa64609 none null local In above list of docker networks, there are two overlay networks.\ningress is default overlay network which uses IPVS and blazing fast due to kernel only data path. This is used for exposing services to external load balancers. Create service\r#\rLets create a service with 2 replicas(instance) attached to mynet from image kunalkushwaha/demoapp_image\n$ docker service create \\\r\u0026gt; --name demoapp \\\r\u0026gt; --replicas 2 \\\r\u0026gt; --network mynet \\\r\u0026gt; -p 5000:5000 \\\r\u0026gt; kunalkushwaha/demoapp_image:v1\r8fwyujxnc1rm0jwewx5tfeai1 Service create commands defines that service and docker swarm manager, picks up the definition to schedule it. You can monitor, service state by following commands.\ndocker service ls just lists the service and shows how many instance running. docker service tasks command give details like on which node service instance is running.\n$ docker service ls\rID NAME REPLICAS IMAGE COMMAND\r8fwyujxnc1rm demoapp 0/2 kunalkushwaha/demoapp_image:v1\r$ docker service tasks demoapp\rID NAME SERVICE IMAGE LAST STATE DESIRED STATE NODE\ra2yczjfyhyryg4pkg8m27ztd6 demoapp.1 demoapp kunalkushwaha/demoapp_image:v1 Running 21 seconds ago Running manager\raj080rydyghazbhco6h9dhaaw demoapp.2 demoapp kunalkushwaha/demoapp_image:v1 Running 21 seconds ago Running worker2 To read configuration of service, you can use docker service inspect command.\n$ docker service inspect demoapp --pretty\rID:\t8fwyujxnc1rm0jwewx5tfeai1\rName:\tdemoapp\rMode:\tReplicated\rReplicas:\t2\rPlacement:\rStrategy:\tSpread\rUpdateConfig:\rParallelism:\t0\rContainerSpec:\rImage:\tkunalkushwaha/demoapp_image:v1\rResources:\rReservations:\rLimits:\rNetworks: 2xjpxwugr0glog7sqywdsnbn6Ports:\rName =\rProtocol = tcp\rTargetPort = 5000\rPublishedPort = 5000 Now our service is deployed successfully, lets try to access and see if, it works properly.\n$ curl 192.168.99.100:5000\rThis is DemoApp v1 So far all good! Now lets try to scale this service by 6 instances.\n$ docker service scale demoapp=6\rdemoapp scaled to 6\r$ docker service ls\rID NAME REPLICAS IMAGE COMMAND\r8fwyujxnc1rm demoapp 4/6 kunalkushwaha/demoapp_image:v1\r$ docker service ps demoapp\rID NAME SERVICE IMAGE LAST STATE DESIRED STATE NODE\ra2yczjfyhyryg4pkg8m27ztd6 demoapp.1 demoapp kunalkushwaha/demoapp_image:v1 Running 13 minutes ago Running manager\raj080rydyghazbhco6h9dhaaw demoapp.2 demoapp kunalkushwaha/demoapp_image:v1 Running 13 minutes ago Running worker2\r7c0dbomrbljlqdq9gitthjg1f demoapp.3 demoapp kunalkushwaha/demoapp_image:v1 Running 10 seconds ago Running manager\rdzooellh3a2vsbsr2soesfhir demoapp.4 demoapp kunalkushwaha/demoapp_image:v1 Running 10 seconds ago Running worker2\r5r114sf8i0n5o53s0hwv911g4 demoapp.5 demoapp kunalkushwaha/demoapp_image:v1 Preparing 10 seconds ago Running worker1\r85fbs7z0ntazachrxw98xnjfo demoapp.6 demoapp kunalkushwaha/demoapp_image:v1 Preparing 10 seconds ago Running worker1 If you observe carefully, column of LAST STATE \u0026amp; DESIRED STATE shows demoapp.5 and demoapp.6 as \u0026ldquo;Preparing\u0026rdquo; and \u0026ldquo;Running\u0026rdquo; This is same as explained above i.e. docker scale command changes the configuration of service. Now docker swarm mode is trying to achieve desired state.\nNode failure.\r#\rLet try to delete one of worker node and see how swarm it tries to reconcile the configuration of service.\n$ docker-machine rm worker2\rAbout to remove worker2\rAre you sure? (y/n): y\rSuccessfully removed worker2\r$ docker service ps demoapp\rID NAME SERVICE IMAGE LAST STATE DESIRED STATE NODE\ra2yczjfyhyryg4pkg8m27ztd6 demoapp.1 demoapp kunalkushwaha/demoapp_image:v1 Running 27 minutes ago Running manager\r5afbnsfl5p3xyygbk1b6aawc6 demoapp.2 demoapp kunalkushwaha/demoapp_image:v1 Accepted 3 seconds ago Accepted worker1\r7c0dbomrbljlqdq9gitthjg1f demoapp.3 demoapp kunalkushwaha/demoapp_image:v1 Running 13 minutes ago Running manager\r8lgqtsewc0vbgh318fx27sm0m demoapp.4 demoapp kunalkushwaha/demoapp_image:v1 Accepted 3 seconds ago Accepted manager\r5r114sf8i0n5o53s0hwv911g4 demoapp.5 demoapp kunalkushwaha/demoapp_image:v1 Running 13 minutes ago Running worker1\r85fbs7z0ntazachrxw98xnjfo demoapp.6 demoapp kunalkushwaha/demoapp_image:v1 Running 13 minutes ago Running worker1 You can see now, worker2\u0026rsquo;s instances (demoapp.2 \u0026amp; demoapp.4) got rescheduled on manager and worker1. All by its own :).\nRolling updates.\r#\rI have v2 of demoapp, which appends IP address of container to message. Lets try to upgrade the application. Here we can define how many instances should get upgraded at one point --update-parallelism and also delay between two updates --update-delay.\nThese both are important feature, which helps to upgrade the application without any downtime. Also, while upgrading if some error occurs, you can rollback the service.\nIn this example, I will try to upgrade 2 instances at a time and with delay of 30s.\ndocker service update demoapp --update-parallelism=2 --update-delay 10s --image kunalkushwaha/demoapp_image:v2 Now try to get the output of app.\n$ curl 192.168.99.100:5000\rThis is DemoApp v2 @ IP: 10.255.0.13\r$ curl 192.168.99.100:5000\rThis is DemoApp v2 @ IP: 10.255.0.9\r$ curl 192.168.99.100:5000\rThis is DemoApp v1\r$ curl 192.168.99.100:5000\rThis is DemoApp v1\r$ curl 192.168.99.100:5000\rThis is DemoApp v1 You can see output is mixed from v1 and v2 application instance. This is due to docker\u0026rsquo;s internal load-balancer. Docker has embedded DNS, which is used for service discovery and load balancing. By default its round-robin.\nRouting Mesh.\r#\rWith routing mesh, service discovery of services in swarm can be done through any node of swarm. i.e. even if service instance not running on any particular node, still if request comes of application/service port, it will be redirected to one of running instance of service. This is achieved by ingress network.\nTo demonstrate, lets add one more node into swarm.\n$ docker-machine create -d virtualbox worker3\r$ eval $(docker-machine env worker3)\r$ docker swarm join --secret aj92ivakk282slwal0ujwaloj \\\r--ca-hash sha256:1f7176d2474cf8dd3fa7a29e46ce42250c5a0aecaf07e40e014f039a7bf1e5ba \\\r192.168.99.100:2377\rThis node joined a Swarm as a worker.\r$ eval $(docker-machine env manager)\r$ docker node ls\rID HOSTNAME MEMBERSHIP STATUS AVAILABILITY MANAGER STATUS\r0cs0e5phve5onp41pxfe9c1kj worker1 Accepted Ready Active\r2ksrqgk2x3vbjh3bw0aly5dwr * manager Accepted Ready Active Leader\r5gfj06su4l885zg88qyze7imu worker3 Accepted Ready Active\rc6nvb1ljj9mntt1v8qlmxl2my worker2 Accepted Down Active Now, demoapp is running only on worker1 and manager nodes.\n$ docker service ps demoapp\rID NAME SERVICE IMAGE LAST STATE DESIRED STATE NODE\r9ucohwuw91e8uuquhxidhplsu demoapp.1 demoapp kunalkushwaha/demoapp_image:v2 Running 15 minutes ago Running manager\ra1odej6m4pz1k4lowzt2l48rz demoapp.2 demoapp kunalkushwaha/demoapp_image:v2 Running 16 minutes ago Running worker1\rcrm4zjjb1hjxdr59zraef4yj9 demoapp.3 demoapp kunalkushwaha/demoapp_image:v2 Running 15 minutes ago Running worker1\r9q84itlob65mbzzvpb1dyjtij demoapp.4 demoapp kunalkushwaha/demoapp_image:v2 Running 15 minutes ago Running worker1\r7bg8r15bq617ghupei8yegqdi demoapp.5 demoapp kunalkushwaha/demoapp_image:v2 Running 16 minutes ago Running worker1\r5uu9qppsh7ohopc7td1tjs9lg demoapp.6 demoapp kunalkushwaha/demoapp_image:v2 Running 15 minutes ago Running manager If I try to send request on worker3, still I will be able to get demoapp output and that too with load balancing.\n$ docker-machine ip worker3\r192.168.99.103\r$ curl --noproxy 192.168.99.103 192.168.99.103:5000\rThis is DemoApp v2 @ IP: 10.255.0.13\r$ curl --noproxy 192.168.99.103 192.168.99.103:5000\rThis is DemoApp v2 @ IP: 10.255.0.14\r$ curl --noproxy 192.168.99.103 192.168.99.103:5000\rThis is DemoApp v2 @ IP: 10.0.0.7 Isnt\u0026rsquo;t this cool!\nDocker swarm mode is about making orchestration simple, so anyone without deep understanding of distributed computing, clustering, security should be able to create a robust, scalable and super secure cluster and still focus on his main work.\nAlso Docker swarm mode do no expect you to change your workflow and application deployment. It just adapts to you.\nIf you find docker swarm mode interesting, You should look at SwarmKit. This project does all magic for docker swarm mode and you can use to build your own distributed application.\nHope this blog will help you to explore docker 1.12 RC.\n","date":"25 July 2016","externalUrl":null,"permalink":"/post/docker-1.12rc-demo/","section":"Posts","summary":"","title":"docker 1.12RC demo","type":"post"},{"content":"","date":"25 July 2016","externalUrl":null,"permalink":"/tags/docker1.12/","section":"Tags","summary":"","title":"Docker1.12","type":"tags"},{"content":"","date":"25 July 2016","externalUrl":null,"permalink":"/tags/orchestration/","section":"Tags","summary":"","title":"Orchestration","type":"tags"},{"content":"","date":"25 July 2016","externalUrl":null,"permalink":"/tags/swarm-mode/","section":"Tags","summary":"","title":"Swarm Mode","type":"tags"},{"content":"Docker-machine is tool to create Docker hosts on computer, on cloud providers, and inside data center. It creates Linux based server, and installs and configures docker. It is also capable of configuring docker-swarm nodes.\nThis blog, will explain stepwise walkthrough for docker host creation using docker-machine.\nInstallation.\r#\rIf you use Windows or Mac, Docker has already made awesome packaged installer for you Docker-toolbox. Download and Install it. For Linux users, you can download docker-machine binaries from here.\nNOTE: Linux users, do remember, you need to install docker client also on your machine.\nCreation of docker host with machine.\r#\rI will be using VirtualBox in this demo, but you can explore cloud options too.\nDocker host can be created with command docker-machine create. Here -d flags specifies driver-name. So command looks like as follows.\n$ docker-machine create -d \u0026lt;driver-name\u0026gt; \u0026lt;name-of-machine\u0026gt;\nThis command does couple of things at backend such as\nIt downloads latest boot2docker image, if not locally available. Create a machine with name-of-machine Creates ssh keys and copies to machine. Installs docker Configures docker daemon at port 2376, so that docker daemon accessible at tcp://\u0026lt;HostIP\u0026gt;:2376 Lets create our first host testhost.\n$ docker-machine create -d virtualbox testhost\rRunning pre-create checks...\rCreating machine...\r(testhost) Copying C:\\Users\\kunal\\.docker\\machine\\cache\\boot2docker.iso to C:\\Users\\kunal\\.docker\\machine\\machines\\testhost\\boot2docker.iso...\r(testhost) Creating VirtualBox VM...\r(testhost) Creating SSH key...\r(testhost) Starting VM...\rWaiting for machine to be running, this may take a few minutes...\rMachine is running, waiting for SSH to be available...\rDetecting operating system of created instance...\rDetecting the provisioner...\rProvisioning with boot2docker...\rCopying certs to the local machine directory...\rCopying certs to the remote machine...\rSetting Docker configuration on the remote daemon...\rChecking connection to Docker...\rDocker is up and running!\rTo see how to connect Docker to this machine, run: C:\\Program Files\\Docker Toolbox\\docker-machine.exe env testhost Here our docker host is ready for use. But wait! host is not same machine as your machine right? It is running inside Virtual Machine. So how docker daemon is accessed remotely?\nDocker works on client-server model. Docker daemon is server and it can communicates though REST API\u0026rsquo;s\nNOTE: docker-machine create is most important command of docker-machine. Understanding various flags help you to get best of docker-machine. You must spend some time in understanding all flags To access docker host with docker client binary remotely, we need to export some environment variables. Docker-machine provides a handy command for printing these variables and their values for us as shown below.\n$ docker-machine env testhost\rexport DOCKER_TLS_VERIFY=\u0026#34;1\u0026#34;\rexport DOCKER_HOST=\u0026#34;tcp://192.168.99.110:2376\u0026#34;\rexport DOCKER_CERT_PATH=\u0026#34;C:\\Users\\kunal\\.docker\\machine\\machines\\testhost\u0026#34;\rexport DOCKER_MACHINE_NAME=\u0026#34;testhost\u0026#34;\r# Run this command to configure your shell:\r# eval $(\u0026#34;C:\\Program Files\\Docker Toolbox\\docker-machine.exe\u0026#34; env testhost) To export all environment variable at once, simply run\n$ eval $(docker-machine env testhost) Now all docker commands will communicates with docker daemon of testhost\n$ docker info\rContainers: 0\rImages: 0\rServer Version: 1.9.1\rStorage Driver: aufs\rRoot Dir: /mnt/sda1/var/lib/docker/aufs\rBacking Filesystem: extfs\rDirs: 0\rDirperm1 Supported: true\rExecution Driver: native-0.2\rLogging Driver: json-file\rKernel Version: 4.1.13-boot2docker\rOperating System: Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015\rCPUs: 1\rTotal Memory: 996.2 MiB\rName: testhost\rID: 26MF:TVKB:JI7Q:TL4S:7RXM:U5CD:VHIR:W2NH:CUNZ:RGB3:C6GC:POBS\rDebug mode (server): true\rFile Descriptors: 14\rGoroutines: 21\rSystem Time: 2016-01-07T07:09:51.081394779Z\rEventsListeners: 0\rInit SHA1:\rInit Path: /usr/local/bin/docker\rDocker Root Dir: /mnt/sda1/var/lib/docker\rLabels:\rprovider=virtualbox Lets create our first docker container on this host.\n$ docker run busybox sh\rUnable to find image \u0026#39;busybox:latest\u0026#39; locally\rlatest: Pulling from library/busybox\rc00ef186408b: Pulling fs layer\rac6a7980c6c2: Pulling fs layer\rac6a7980c6c2: Verifying Checksum\rac6a7980c6c2: Download complete\rc00ef186408b: Verifying Checksum\rc00ef186408b: Download complete\rc00ef186408b: Pull complete\rac6a7980c6c2: Pull complete\rDigest: sha256:e4f93f6ed15a0cdd342f5aae387886fba0ab98af0a102da6276eaf24d6e6ade0\rStatus: Downloaded newer image for busybox:latest NOTE: If you are one of user, who works behind proxy, then you may face problem like, your newly created docker host may not communicate with docker-hub.\nTo Fix that, you need to pass few environment flags at creation time as below.\n$ docker-machine create -d virtualbox \\\r--engine-env HTTP_PROXY=\u0026#34;http://example.com:8080\u0026#34; \\\r--engine-env HTTPS_PROXY=\u0026#34;http://example.com:8080\u0026#34; \\\rtesthost So now we learnt how to create a docker host and run docker containers using docker-machine. But Docker-machine not only lets you to create independent hosts, but also can put these host in one cluster!\nIsn\u0026rsquo;t it interesting?\nLet\u0026rsquo;s see how we can build whole cluster of docker-hosts using docker-machine.\nDocker Swarm is native clustering solution for docker. Here with native means, Swarm understands and exports all(almost) docker API\u0026rsquo;s. i.e. API for docker and docker swarm are same. If one product/scripts works with docker, it will automatically work with swarm :)\nDocker Swarm\r#\rDocker swarm has three components.\nDiscovery Backend : Swarm requires a discovery backend, which is used by each node(agent) to discover the master. Default discovery is provided by Docker Hub. (Not for production usage) Master : Swarm master takes care of all scheduling logic and HA. Agent : These runs on each node and communicates with Swarm master. All agent node must listen to the same network interface (TCP port). Each node runs a node agent that registers the referenced Docker daemon, monitors it, and updates the discovery backend with the node‚Äôs status In this demo, We will create a docker swarm consisting one Master and three agents including master. Also, Consul will be used for discovery backend.\nDiscovery Backend using consul.\r#\rWe will create a dedicated machine for consul, but running consul using docker is just one command task.\n$ docker-machine create -d virtualbox \\\r--engine-env HTTP_PROXY=\u0026#34;http://example.com:8080\u0026#34; \\\r--engine-env HTTPS_PROXY=\u0026#34;http://example.com:8080\u0026#34; \\\rswl-consul\r$ docker $(docker-machine config swl-consul) run \\\r-e \u0026#34;http_proxy=http://example.com:8080\u0026#34; \\\r-e \u0026#34;https_proxy=http://example.com:8080\u0026#34; \\\r--restart=\u0026#34;always\u0026#34; \\\r-d -p \u0026#34;8500:8500\u0026#34; \\\r-h \u0026#34;consul\u0026#34; \\\rprogrium/consul -server -bootstrap Check if consul container is created as expected.\n$ docker-machine ls\rNAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS\rswl-consul - virtualbox Running tcp://192.168.99.100:2376 v1.9.1\r$ eval $(docker-machine env swl-consul)\r$ docker ps\rCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\r5357e85abcb5 progrium/consul \u0026#34;/bin/start -server -\u0026#34; 16 minutes ago Up 16 minutes 53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 0.0.0.0:8500-\u0026gt;8500/tcp goofy_aryabhata Creation of Swarm Master.\r#\rIn cluster, we will have one master and 2 agents. But master will also have agent, so practically we will have 3 agents.\nDocker machine helps to setup docker host with swarm in single command.\nFew highlights of docker machine provisioned swarm enabled host.\nSwarm master and agent runs as docker containers.\nSwarm manager bind on \u0026ldquo;3376\u0026rdquo; port, So to communicate with master tcp://\u0026lt;IP:3376\u0026gt;\u0026gt; should be used.\n$ docker-machine create \\\r\u0026gt; -d virtualbox \\\r\u0026gt; --swarm \\\r\u0026gt; --swarm-master \\\r\u0026gt; --swarm-discovery=consul://$(docker-machine ip swl-consul):8500 \\\r\u0026gt; --engine-env HTTP_PROXY=http://example.com:8080 \\\r\u0026gt; --engine-env HTTPS_PROXY=http://example.com:8080 \\\r\u0026gt; --engine-env NO_PROXY=192.168.99.100 \\\r\u0026gt; --engine-opt=\u0026#34;cluster-store=consul://$(docker-machine ip swl-consul):8500\u0026#34; \\\r\u0026gt; --engine-opt=\u0026#34;cluster-advertise=eth1:3376\u0026#34; \\\r\u0026gt; node1\rRunning pre-create checks...\r.\r.\r(node1) Starting VM...\rWaiting for machine to be running, this may take a few minutes...\rMachine is running, waiting for SSH to be available...\rDetecting operating system of created instance...\rDetecting the provisioner...\rProvisioning with boot2docker...\rCopying certs to the local machine directory...\rCopying certs to the remote machine...\rSetting Docker configuration on the remote daemon...\rConfiguring swarm...\rChecking connection to Docker...\rDocker is up and running!\rTo see how to connect Docker to this machine, run: C:\\Program Files\\Docker Toolbox\\docker-machine.exe env node1\r$ eval $(docker-machine env node1)\r$ docker ps\rCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\r7e644b71f606 swarm:latest \u0026#34;/swarm join --advert\u0026#34; 19 seconds ago Up 16 seconds swarm-agent\r58f2a4da438b swarm:latest \u0026#34;/swarm manage --tlsv\u0026#34; 22 seconds ago Up 19 seconds swarm-agent-master NOTE: You may have seen NO_PROXY is also set as environment variable.\nThis is to skip proxy for communicating with consul server Here,\n--swarm indicates, swarm agent will be configured. --swarm-master indicates, swarm master will be configured. --swarm-discovery sets the backend discovery for swarm. Here you go, we have our swarm master node up and running. Before communicating to swarm, let me remind you again.\nSwarm and docker talks in same API signature. But on our host, we have both docker daemon and swarm running. Our docker client can communicate with both, but we need to choose whom it has to talk. To talk with docker daemon use eval $(docker-machine env \u0026lt;host-name\u0026gt;) To talk with swarm use eval $(docker-machine env --swarm \u0026lt;host-name\u0026gt; So, since we want our docker client should communicate with swarm, we will add --swarm\n$ eval $(docker-machine env --swarm node1)\r$ docker info\rContainers: 2\rImages: 1\rRole: primary\rStrategy: spread\rFilters: health, port, dependency, affinity, constraint\rNodes: 1\rnode1: 192.168.99.102:2376\r‚îî Status: Healthy\r‚îî Containers: 2\r‚îî Reserved CPUs: 0 / 1\r‚îî Reserved Memory: 0 B / 1.021 GiB\r‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs\rCPUs: 1\rTotal Memory: 1.021 GiB\rName: node1 Now add two nodes with only agent.\n$ docker-machine create \\\r\u0026gt; -d virtualbox \\\r\u0026gt; --swarm \\\r\u0026gt; --swarm-discovery=\u0026#34;consul://$(docker-machine ip swl-consul):8500\u0026#34; \\\r\u0026gt; --engine-env HTTP_PROXY=\u0026#34;http://example.com:8080\u0026#34; \\\r\u0026gt; --engine-env HTTPS_PROXY=\u0026#34;http://example.com:8080\u0026#34; \\\r\u0026gt; --engine-env NO_PROXY=\u0026#34;192.168.99.100\u0026#34; \\\r\u0026gt; --engine-opt=\u0026#34;cluster-store=consul://$(docker-machine ip swl-consul):8500\u0026#34; \\\r\u0026gt; --engine-opt=\u0026#34;cluster-advertise=eth1:3376\u0026#34; \\ \u0026gt; node3 Command for adding agent doesn\u0026rsquo;t requires --swarm-master. cluster-advertise and cluster-store are required for overlay networking also, Will be explained later. So after adding two more machines with swarm agent, we have 4 machines running as below.\n$ docker-machine ls\rNAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS\rnode1 - virtualbox Running tcp://192.168.99.102:2376 node1 (master) v1.9.1\rnode2 - virtualbox Running tcp://192.168.99.103:2376 node1 v1.9.1\rnode3 - virtualbox Running tcp://192.168.99.104:2376 node1 v1.9.1\rswl-consul - virtualbox Running tcp://192.168.99.100:2376 v1.9.1 Lets see information of our cluster.\n$ eval $(docker-machine env --swarm node1)\r$ docker info\rContainers: 4\rImages: 3\rRole: primary\rStrategy: spread\rFilters: health, port, dependency, affinity, constraint\rNodes: 3\rnode1: 192.168.99.102:2376\r‚îî Status: Healthy\r‚îî Containers: 2\r‚îî Reserved CPUs: 0 / 1\r‚îî Reserved Memory: 0 B / 1.021 GiB\r‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs\rnode2: 192.168.99.103:2376\r‚îî Status: Healthy\r‚îî Containers: 1\r‚îî Reserved CPUs: 0 / 1\r‚îî Reserved Memory: 0 B / 1.021 GiB\r‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs\rnode3: 192.168.99.104:2376\r‚îî Status: Healthy\r‚îî Containers: 1\r‚îî Reserved CPUs: 0 / 1\r‚îî Reserved Memory: 0 B / 1.021 GiB\r‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Nov 20 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs\rCPUs: 3\rTotal Memory: 3.064 GiB\rName: node1 Superb! we have three nodes running in our cluster. This completes the swarm setup!!\nUnderstanding few more Swarm functionality.\r#\rStrategy Currently \u0026ldquo;Spread\u0026rdquo; strategy is set. So the containers will be spread over all the hosts in cluster.\ne.g.\n$ docker run -d -P -m 1G -e MYSQL_ROOT_PASSWORD=test123 --name db mysql\r57a1af46d72117306b833a28a219d3523e1531c98a19ef25cba00a7bc94c9645\r$ docker run -d -P -m 1G --name frontend nginx\rf40776d7c9f7583965263340c1542a09eb7cb881c53bcf18a290df0d6ce2fd51\r$ docker ps\rCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\rf40776d7c9f7 nginx \u0026#34;nginx -g \u0026#39;daemon off\u0026#34; 4 seconds ago Up 4 seconds 192.168.99.103:32770-\u0026gt;80/tcp, 192.168.99.103:32769-\u0026gt;443/tcp node2/frontend\r57a1af46d721 mysql \u0026#34;/entrypoint.sh mysql\u0026#34; 3 minutes ago Up 3 minutes 192.168.99.104:32769-\u0026gt;3306/tcp node3/db Here node3 have db and node2 have frontend container. If two nodes have the same amount of available RAM and CPUs, the spread strategy prefers the node with least containers. Other available strategies are BinPack and Random.\nBinPack tries to pack all possible containers on single node first and so on. If two nodes have the same amount of available RAM and CPUs, the binpack strategy prefers the node with most containers. Things to Try:\nCreate swarm with BinPack and create multiple containers to see the behavior. Filters\r#\rFilters tell Docker Swarm scheduler which nodes to use when creating and running a container.\nFilters are divided into two categories, node filters and container configuration filters.\nNode filters operate on characteristics of the Docker host or on the configuration of the Docker daemon.\nContainer configuration filters operate on characteristics of containers, or on the availability of images on a host.\nEach filter has a name that identifies it.\nThe node filters are:\nconstraint health The container configuration filters are:\naffinity dependency port When you start a Swarm manager with the swarm manage command, all the filters are enabled. If you want to limit the filters available to your Swarm, specify a subset of filters by passing the --filter flag and the name:\n$ swarm manage --filter=health --filter=dependency In case of docker machine, while provisioning --swarm-opt can be used to set filters.\nUse a constraint filter\r#\rNode constraints can refer to Docker‚Äôs default tags or to custom labels. Default tags are sourced from docker info. Often, they relate to properties of the Docker host. Currently, the dafult tags include:\nnode to refer to the node by ID or name storagedriver executiondriver kernelversion operatingsystem Custom node labels can be applied while provisioning docker machine. --engine-label can be used for setting custom label. custom label like environment, storage etc are helpful. Since we have all labels same except node name, we will try creating new container using node-name constraint.\n$ docker run -itd -e constraint:node==node3 --name test1 ubuntu\rae4013d014931e43cd5342a625f6b549a8c920cb146b1371a7226359a4bcf014\r$ docker run -itd -e constraint:node==node3 --name test2 ubuntu\ra6b371773e1a2609122ca68df00d1e03ee274ba3db504d3942c301f8e2c45504\r$ docker ps\rCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\ra6b371773e1a ubuntu \u0026#34;/bin/bash\u0026#34; 3 seconds ago Up 2 seconds node3/test2\rae4013d01493 ubuntu \u0026#34;/bin/bash\u0026#34; 19 seconds ago Up 18 seconds node3/test1\r934501aed30f ubuntu \u0026#34;/bin/bash\u0026#34; 19 hours ago Up 2 hours node3/n4\r30358292be89 ubuntu \u0026#34;/bin/bash\u0026#34; 19 hours ago Up 19 hours node1/n3\r6a13aa3eca8b ubuntu \u0026#34;/bin/bash\u0026#34; 21 hours ago Up 21 hours node2/n1 Similarly other filters can be used. While using filters the syntax to use in docker run is as below.\n- ``-e \u0026lt;filter-name\u0026gt;:\u0026lt;key\u0026gt;\u0026lt;operator\u0026gt;\u0026lt;value\u0026gt;``\r- Here operator used are ``==`` , ``!=`` \u0026amp; ``~``\r- For ``==`` \u0026amp; ``!=``, exact match is found.\r- If nothing satisfies the condition, it does schedule container.\r- ``~`` is for soft condition. i.e. if nothing matches, it ignores the condition and use default scheduler.\r- Here value can contain any valid regex (https://github.com/google/re2/wiki/Syntax)\rHope this article will be helpful in getting started with docker-machine and swarm :)\n","date":"14 January 2016","externalUrl":null,"permalink":"/post/tutorial-docker-swarm-with-multihost-networking/","section":"Posts","summary":"","title":"Tutorial setting up docker-swarm with multihost networking using docker-machine.","type":"post"},{"content":"Even after writing Go code for a while, there have been couple of time, when I get confused about Interfaces in golang. So I think, it may help people, who have started with Golang, or don\u0026rsquo;t use much Interfaces in there code.\nInterfaces\r#\rBy definition, Interfaces are named collections of method signatures. But usage of Interfaces in go is little confusing as it is used a in context of data type also and defining the behavior of methods also.\nLets try to understand each of them.\n1. interface{} , interface as data-type\r#\rThis is commonly mistaken as ( void type of C/C++), But it\u0026rsquo;s not! It still holds its property from the above definition. Since, interface{} is empty interface i.e. No methods associated with it, all data type satisfy the behavior and hence, all data types can be passed/assigned to interface{} type.\nLets take a example,\ntype Stack struct { data []interface{} } func NewStack() *Stack { s := Stack{data: make([]interface{},0)} return \u0026amp;s } func (s *Stack) Push(data interface{}) { s.data = append(s.data, data) } func (s *Stack) Pop() interface{} { if len(s.data) == 0 { return nil } data := s.data[len(s.data) -1] s.data = s.data[0:len(s.data)-1] return data } To use the above stack, a simple example shows retrieval of values from interface type.\ns := NewStack() s.Push(10) s.Push(\u0026#34;Hello\u0026#34;) obj1 := s.Pop() obj2 = s.Pop() // To retrieve int value, use type assertion. elem1 := obj.(string) elem2 := obj.(int) Here are few things needs to be understood about interfaces to be used as type.\nWhenever variables of any datatype is assigned to interface type, it is converted into interface type and stored. So properties of original data-type cannot be retrieved until, it is converted again back to original data-type. conversion to data-type from interfaces cannot be achieved using typecasting, Here it required, type assertion. So use interface{}, wherever you wish to add generic data-type and remember at retrieval time, use type assertion!\n2. Interface as set of methods.\r#\rUnlike above type, here interface have some function prototype.\nIf you come from \u0026ldquo;C/C++\u0026rdquo; background, you can relate it with function pointers or abstract functions/pure virtual functions. These functions don\u0026rsquo;t have any implementation.\nBut, to implement these functions, no need to inherit in structs or assign to function pointers.\nIn Go, if some structs or functions have same behavior as of interface, it automatically can be used in interface. Here behavior mean, all functions in interface should be implemented with same signature of functions as of interface.\nLets take a simple example.\ntype Plugin interface { Execute(string, int) bool } Here, a Plugin interface is defined with one function. Any Structure, which have this one function, will automatically implements the interface.\ni.e. Any structure, which has same behavior means the Plugin interface can represent that structure. No explicitly need to show via any keyword like implement or inherit.\nI feel, it is very simple approach to build OOP design. Such simple approaches, to create design patterns, make Go even more interesting.\nLets see examples which explains the behavior of interfaces\n1. Example #1\r#\rtype aPlugin struct { name string } func (p aPlugin) Execute(name string, count int) bool { fmt.Println(\u0026#34;aPlugin executing\u0026#34;, name, count) return 0 } Here, structure aPlugin have exactly same function as of Plugin interface, so this aPlugin can be assigned to Plugin interface.\nSo main function will look like this.\nfunc main() { // Create a Plugin array obj := []Plugin{aPlugin{}, bPlugin{}} obj[0].Execute(\u0026#34;Hello\u0026#34;, 1) obj[1].Execute(\u0026#34;World\u0026#34;, 2) } $ go run main.go aPlugin executing Hello 1 bPLugin executing World 2 2. Example #2\r#\rtype bPlugin struct { name string } func (p bPlugin) Execute(name string, count int) bool { fmt.Println(\u0026#34;bPlugin executing\u0026#34;, name, count) return 0 } func (p bPlugin) DumpInfo() { fmt.Println(p.name) } Here, another structure bPlugin have one additional function then of Plugin interface. So this bPlugin can be assigned to Plugin interface.\nOnly, if it is assigned to Plugin object, the additional function DumpInfo() cannot be invoked from Plugin object. - So a Structure can have additional functions, data members and still it can assigned to interface object.\n3. Example #3\r#\rtype cPlugin struct { name string } func (p cPlugin) Execute() bool { fmt.Println(\u0026#34;cPlugin executing\u0026#34;) return 0 } Here, structure cPlugin have function with same name Execute, but with different signatures. So cPlugin object cannot be assigned to Plugin object.\ngo compiler will raise error if it is tried to do so.\nobj := []Plugin{aPlugin{}, bPlugin{}, cPlugin{}} $ go run main.go\r./main.go:22: cannot use cPlugin literal (type cPlugin) as type Plugin in array element:\rcPlugin does not implement Plugin (wrong type for Execute method)\rhave Execute() bool\rwant Execute(string, int) bool How should these interfaces should be used.\r#\rInterfaces are one of most important building blocks of golang. Also, its usage make golang as language very simpler to use. for example, you can look into \u0026ldquo;io\u0026rdquo; package.\nIf you implement any kind of framework or as simple as muti-platform program, where you wish functionalities, differ for each OS, interfaces are perfect candidates to use.\nI hope this blog would have helped to understand the interfaces in Golang. Please comment in case you have any feedback.\nHappy Coding :)\n","date":"11 September 2015","externalUrl":null,"permalink":"/post/understanding-golang-interfaces/","section":"Posts","summary":"","title":"Understanding Golang Interfaces","type":"post"},{"content":"\rConditional compiling in golang.\r#\rWhile hacking around experimental builds of docker, I learned about conditional compiling in golang. As any project grew or ported to multiple architecture/platforms the need of conditional compiling is obvious.\nUnlike C golang, do not have macros. Golang make use of build tags to achieve this.\nBuild Tags:\r#\rBuild tags are basically annotation in source code. generally it is first line of file, where build tags are defined.\n// +build \u0026lt;tags\u0026gt;\nThese tags are searched by go build tool before passing to go compiler. These tags can be passed while go build command like\n$ go build -tags \u0026lt;tags\u0026gt;\nHere are few points to remember regarding tags.\nTags specifies architecture and platforms are special. i.e. No need to pass such tags, go build is smart enough to pass them. e.g. linux, windows, darwin, 386, arch etc Tags can be passed with negation i.e. ! A blank line must be present between build tags and code, else that line will be ignored. OR condition for tags can be specified with space. AND condition can be specified with , If some tag is defined in file and is not passed in `go build`` command, it will be ignored.\nLets see this with example with custom tag.\nHere we will take three files main.go include.go exclude.go ‚óã cat main.go package main import ( \u0026#34;fmt\u0026#34; ) func init() { fmt.Println(\u0026#34;Hello from Main init()\u0026#34;) } func main() { fmt.Println(\u0026#34;Hello from main\u0026#34;) } ‚óã cat include.go package main import \u0026#34;fmt\u0026#34; func init() { fmt.Println(\u0026#34;Hello from IncludeInit()\u0026#34;) } ‚óã cat exclude.go // +build exclude package main import \u0026#34;fmt\u0026#34; func init(){ fmt.Println(\u0026#34;Hello from exclude\u0026#34;) } As you can see, here exclude file has build tag of exclude.\nNow if do go build, It will not compile exclude.go.\n‚óã go build ‚óã ./temp Hello from IncludeInit() Hello from Main init() Hello from main Now let try using buildtag exclude.\n‚óã go build -tags exclude ‚óã ./temp Hello from exclude Hello from IncludeInit() Hello from Main init() Hello from main You can see now, the exclude.go is part of binary.\nOne more interesting thing with golang is go list tool. Using the go list, without compiling, you can see which files will be compiled, with given gotags or default.\nI found go list --help not great, but the summary of this tool is a go template can be used to check the list of files that will be included in the build. A simple example as below will help you to understand better.\n‚óã go list -f \u0026#39;{{.GoFiles}}\u0026#39; --tags exclude [exclude.go include.go main.go] ‚óã go list -f \u0026#39;{{.GoFiles}}\u0026#39; [include.go main.go] As you can see, if exclude tag is not used, only include.go and main.go files are used.\nSo it that all about conditional compiling in golang?\r#\rNo, There is even simpler method available too :-) and it is file-name suffixes.\nIn golang, there are few go environment variables defined. GOOS and GOARCH are two of such.\nGOOS defines the current OS GOARCH defines the current machine architecture. go build matches value of these two environment variables in filename and if it matches then only include for compiling.\nSo by naming files like exclude_linux.go, it will be included only if compiled in linux system. exclude_linux_amd64.go will ensure, it will be inlcuded only if OS is linux and architecture of system in 64 bit.\nYou may try these combination.\nWhich one should be used?\r#\rFor custom tags build tags are the only options. For platform and architecture specific, if you plan to use cross build on one systems, build tags will be preferable. Also, if same file does work for multiple platforms, build tags will be better choice. For rest, file sufixes can be used.\nHope this info will be useful.\n","date":"24 July 2015","externalUrl":null,"permalink":"/post/conditional-compilation-in-golang/","section":"Posts","summary":"","title":"Conditional compilation in golang","type":"post"},{"content":"Since 2014, Linux containers have become buzz word in Cloud Infrastructure. Almost all, from Big corporations to startups, all have started using it. Huge credit goes to Docker for making using containers so easy to use.\nLinux Containers are there in Linux systems for alomst decade old, But making them work, was not so easy, and generally required linux admin experts for doing same. Few Solution as linux containers like FreeBSD Jails, LXC, openVZ, Solaris Zones etc exists for quite some time.\nThese are also known as OS level Virtualization. To understand other type of virtualization please read Layman guide to Platform virtualization\nOperating System level Virtualization\r#\rQuoting below from Wikipedia, I it explains beautifully in technical and yet not too complex.\nOS level Virtualization is a server virtualization method where the kernel of an operating system allows for multiple isolated user space instances, instead of just one. Such instances (often called containers, virtualization engines (VE), virtual private servers (VPS), or jails) may look and feel like a real server from the point of view of its owners and users.\nIn simple words, it allows to run multiple rootfs (user-space) simultaneously and all running rootfs have their own view of filesystem and devices. So they are not aware of each others and resource usage can be configured.\nSounds similar to virtual Machines? Yes it is!\nHow this isolation is achieved?\r#\rThis isolation is achieved using linux features like namespaces, cgroups and chroot. To understand details we need to first understand each of them.\nNamespaces\r#\rNamespace wraps a particular global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource.\nCurrently, Linux implements six different types of namespaces\nMount namespaces (CLONE_NEWNS)\r#\rThis isolate the set of filesystem mount points seen by a group of processes. Thus, processes in different mount namespaces can have different views of the filesystem hierarchy. With the addition of mount namespaces, the mount() and umount() system calls ceased operating on a global set of mount points visible to all processes on the system and instead performed operations that affected just the mount namespace associated with the calling process.\nThis is also an alternative to chroot system call. This is supported since Linux 2.4.19 UTS namespaces (CLONE_NEWUTS)\r#\rThis isolate two system identifiers‚Äînodename and domainname‚Äîreturned by the uname() system call; the names are set using the sethostname() and setdomainname() system calls.\nIn the context of containers, the UTS namespaces feature allows each container to have its own hostname and NIS domain name. This can be useful for initialization and configuration scripts that tailor their actions based on these names. This is supported in Linux kernel since Linux 2.6.19. IPC namespaces (CLONE_NEWIPC)\r#\rThis isolate certain interprocess communication (IPC) resources, namely, System V IPC objects and (since Linux 2.6.30) POSIX message queues.\nThe common characteristic of these IPC mechanisms is that IPC objects are identified by mechanisms other than filesystem pathnames. Each IPC namespace has its own set of System V IPC identifiers and its own POSIX message queue filesystem. This is supported in Linux kernel since Linux 2.6.19 PID namespaces (CLONE_NEWPID)\r#\rThis isolate the process ID number space. In other words, processes in different PID namespaces can have the same PID. This helps migrating containers between hosts while keeping the same process IDs for the processes inside the container. PID namespaces also allow each container to have its own init (PID 1), the \u0026ldquo;ancestor of all processes\u0026rdquo; that manages various system initialization tasks and reaps orphaned child processes when they terminate. This is supported since Linux 2.6.24 Network namespaces (CLONE_NEWNET)\r#\rThis provide isolation of the system resources associated with networking. Thus, each network namespace has its own network devices, IP addresses, IP routing tables, /proc/net directory, port numbers, and so on.\nNetwork namespaces make containers useful from a networking perspective: each container can have its own (virtual) network device and its own applications that bind to the per-namespace port number space. started in Linux 2.4.19 2.6.24 and largely completed by about Linux 2.6.29 User namespaces (CLONE_NEWUSER)\r#\rThis isolate the user and group ID number spaces. In other words, a process\u0026rsquo;s user and group IDs can be different inside and outside a user namespace.\nThe most interesting case here is that a process can have a normal unprivileged user ID outside a user namespace while at the same time having a user ID of 0 inside the namespace. This means that the process has full root privileges for operations inside the user namespace, but is unprivileged for operations outside the namespace. This was partially supported since Linux 2.6.23 and completed in Linux 3.8. Control groups a.k.a. cgroups\r#\rCgroups allow you to allocate resources‚Äîsuch as CPU time, system memory, network bandwidth, or combinations of these resources‚Äîamong user-defined groups of tasks (processes) running on a system.\nOne can configure cgroups, deny cgroups access to certain resources, and even reconfigure cgroups dynamically on a running system. The cgconfig (control group config) service can be configured to start up at boot time and reestablish your predefined cgroups, thus making them persistent across reboots. By using cgroups, we gain fine-grained control over allocating, prioritizing, denying, managing, and monitoring system resources. Hardware resources can be appropriately divided up among tasks and users, increasing overall efficiency. These are like process, hierarchical in nature i.e. child cgroups inherit certain attributes from their parent cgroup. Follwing resources are supported currently in cgroups.\nblkio ‚Äî this subsystem sets limits on input/output access to and from block devices such as physical drives (disk, solid state, USB, etc.).\ncpu ‚Äî this subsystem uses the scheduler to provide cgroup tasks access to the CPU.\ncpuacct ‚Äî this subsystem generates automatic reports on CPU resources used by tasks in a cgroup.\ncpuset ‚Äî this subsystem assigns individual CPUs (on a multicore system) and memory nodes to tasks in a cgroup.\ndevices ‚Äî this subsystem allows or denies access to devices by tasks in a cgroup.\nfreezer ‚Äî this subsystem suspends or resumes tasks in a cgroup.\nmemory ‚Äî this subsystem sets limits on memory use by tasks in a cgroup, and generates automatic reports on memory resources used by those tasks.\nnet_cls ‚Äî this subsystem tags network packets with a class identifier (classid) that allows the Linux traffic controller (tc) to identify packets originating from a particular cgroup task.\nnet_prio ‚Äî this subsystem provides a way to dynamically set the priority of network traffic per network interface.\nns ‚Äî the namespace subsystem.\nFor details you may refer : https://www.kernel.org/doc/Documentation/cgroups/cgroups.txt\nSo how these helps in containers?\r#\rBy now, you must have understood, namespaces and cgroups help to create isolated environment.\nNamespaces provides isolation of filesystem view, devices, network and processes. Cgroups helps to allocate, devices accessibility, and allocate quota to use the devices. Is this all sufficient for Virtualization?\r#\rNo, still security is left. To create secure containers features like Capablities, secomp , SELinux and Apparmor are used for that. These all are integrated with new container solutions like Docker, CoreOS rocket etc.\nFew Container projects worth wating are\nLXC - Linux containers : This is default container hypervisior on all linux based systems. https://linuxcontainers.org/\nDocker libcontainer: This is now donated by Docker to Linux foundation and new name will is OpenContainers. http://www.opencontainers.org/\nCoreOS - Rocket - https://github.com/coreos/rkt\nRedhat\u0026rsquo;s systemd-nspawn. http://www.freedesktop.org/software/systemd/man/systemd-nspawn.html\nI hope this blog would have help you to understand Linux Containers.\nI will be writing more on current status of linux containers projects in my next blog so stay tuned :)\n","date":"4 July 2015","externalUrl":null,"permalink":"/post/container-virtualization/","section":"Posts","summary":"","title":"Container Virtualization and its building blocks","type":"post"},{"content":"Setting up golang environment is quite simple good docs are already present in golang.org. But I couldn\u0026rsquo;t find any simple doc, where complete setup with GOROOT and GOPATH along with github is explained.\nSo I thought it might be helpful to others too. My dev environment is ubuntu based ElementryOS \u0026ldquo;Freya\u0026rdquo;, So it would work on all Ubuntu based distros.\nGolang Installer\r#\rGolang comes with single tar file setup can be downloaded from here extract the tar file to your /usr/local using below command\ntar -C /usr/local -xzf go$VERSION.$OS-$ARCH.tar.gz this will install your all go binaries under following dir structure.\nbase folder where go is installed.\n/usr/local/go All standard go binaries are in\n/usr/local/go/bin Golang environment setup.\r#\rNow next step is to define golang related environment variable. There are two important golang Env variables that need to be defined.\nGOROOT : This variable have value, where golang is installed. GOPATH : This is like workspace. This folder will be root of all go getable packages and projects. So you need to create a folder for GOPATH. My fav is ~/go folder.\nkunal@kunal-Aspire-5670:~$ mkdir ~/go So now define GOROOT, GOPATH and append PATH variables.\nexport GOROOT=/usr/local/go export GOPATH=$HOME/go export PATH=$PATH:$GOROOT/bin:$GOPATH/bin Its better to append your ~/.profile file with these 3 lines. So no need to export these variables every time you restart machine.\nAfter setting environment variables check if everything is set as per expected or not! Use \u0026ldquo;go env\u0026rdquo; command.\nkunal@kunal-Aspire-5670:~$ go env GOARCH=\u0026#34;386\u0026#34; GOBIN=\u0026#34;\u0026#34; GOCHAR=\u0026#34;8\u0026#34; GOEXE=\u0026#34;\u0026#34; GOHOSTARCH=\u0026#34;386\u0026#34; GOHOSTOS=\u0026#34;linux\u0026#34; GOOS=\u0026#34;linux\u0026#34; GOPATH=\u0026#34;/home/kunal/go\u0026#34; GORACE=\u0026#34;\u0026#34; GOROOT=\u0026#34;/usr/local/go\u0026#34; GOTOOLDIR=\u0026#34;/usr/local/go/pkg/tool/linux_386\u0026#34; CC=\u0026#34;gcc\u0026#34; GOGCCFLAGS=\u0026#34;-fPIC -m32 -pthread -fmessage-length=0\u0026#34; CXX=\u0026#34;g++\u0026#34; CGO_ENABLED=\u0026#34;1\u0026#34; verify GOPATH and GOROOT.\ngo getable.\r#\rOne of beauty of golang is \u0026ldquo;go get\u0026rdquo; command. It automatically downloads the all required packages along with your program from git repo or mercurial.\nPrerequisite to this is you should install git and mercurial both on your machine. mercurial is required as lot of official libraries of golang is still hosted at google code and it hosted with mercurial.\nkunal@kunal-Aspire-5670:~$ sudo apt-get install mercurial So we are set with all basics, now ready for go getable your code!\nafter your go get github.com/, You will see your ~/go folder have sum folder structure created.\nkunal@kunal-Aspire-5670:~$ ll go total 32 drwxrwxr-x 6 kunal kunal 4096 Apr 20 20:28 ./ drwx------ 24 kunal kunal 12288 Apr 27 22:45 ../ drwxrwxr-x 2 kunal kunal 4096 Apr 16 03:27 bin/ drwxrwxr-x 3 kunal kunal 4096 Apr 12 21:26 pkg/ drwxrwxr-x 8 kunal kunal 4096 Apr 16 03:24 src/ These are folder which will have all your go getable code/packages and binaries. *bin : folder will have binaries build after you build go project using \u0026ldquo;go build\u0026rdquo; *pkg : this will have all pakages downloaded due to dependencies of your program. These packages compiled packages. *src : this will have your source code under folder of source of code like github.com , bitbucket.com, code.google.com etc e.g\nkunal@kunal-Aspire-5670:~$ ll go/src/ total 32 drwxrwxr-x 8 kunal kunal 4096 Apr 16 03:24 ./ drwxrwxr-x 6 kunal kunal 4096 Apr 20 20:28 ../ drwxrwxr-x 3 kunal kunal 4096 Apr 16 03:24 bitbucket.org/ drwxrwxr-x 3 kunal kunal 4096 Apr 12 21:28 code.google.com/ drwxrwxr-x 38 kunal kunal 4096 Apr 20 20:05 github.com/ drwxrwxr-x 3 kunal kunal 4096 Apr 12 21:29 golang.org/ drwxrwxr-x 3 kunal kunal 4096 Apr 12 21:29 google.golang.org/ drwxrwxr-x 6 kunal kunal 4096 Apr 16 03:26 gopkg.in/ So you are now all set for code/test/ship in golang. Happy Coding :)\n","date":"27 April 2015","externalUrl":null,"permalink":"/post/golang-dev-environment/","section":"Posts","summary":"","title":"Golang Development Environment","type":"post"},{"content":"\rPlatform Virtualization!\r#\rVirtualization is not new in Computer World today. But when I am asked what I am working on, I say \u0026ldquo;Cloud Infrastructure and trying to build a private Cloud Platform!\u0026rdquo;.\nExplaining cloud service is easy but Cloud Infrastructure becomes little difficult. So I am writing a blog series to explain Cloud Infrastructure and it building blocks. Hope it will help many others too :)\nVirtualization.\r#\rThough simple meaning of Virtualization is creating virtual version of anything. But in computer software world everything is already Virtual :). But computer hardware devices, Network cable etc are real!\nWhen all these things are created virtually i.e. software simulation of all such hardware its called virtualization!\nBut why that is required? I heard Hardware is becoming cheap day by day :-| These are genuine doubts! indeed hardware is becoming cheap and also performance of capacity wise is improving at much faster rate then ever.\nAlso, with increase of hardware capacity, most of hardware are underutilized :O Yes and this becomes huge concern for business owner, How to utilize best of your hardware. Second Management of hardware resources is also a addition of cost.\nUsing Virtualization both issues can be addressed. Multiple systems workload is executed on same hardware which result into better utilization, and Managing multiple systems with software also makes more efficient and easy.\nHow is Virtualization is achieved?\r#\rThere are broadly three types of Virtualization Techniques. Wikipedia has short and crisp definition for all three.\nHere Host OS is the OS which runs on Actual Hardware and Guest OS is running on Virtual Hardware/Machine.\nFull virtualization ‚Äì almost complete simulation of the actual hardware to allow software, which typically consists of a guest operating system, to run unmodified. e.g. VirtualBox, VMWare Workstations, Parallel for MAC works on full virtualization, No change is required in guest OS\nPartial virtualization ‚Äì some but not all of the target environment attributes are simulated. As a result, some guest programs may need modifications to run in such virtual environments. Probably this is first approach to virtualization which lead to full virtualization. e.g. IBM mainfraim system [IBM M44/44X] (http://en.wikipedia.org/wiki/IBM_M44/44X) was one of such experimental machine.\nParavirtualization ‚Äì a hardware environment is not simulated; however, the guest programs are executed in their own isolated domains, as if they are running on a separate system. Guest programs need to be specifically modified to run in this environment. Paravirtulization is lightweight as compared to full virtualization. e.g. XEN is based on paravirtualization.\nSo what we understood!\r#\rWell, We must have realized with all these techniques, all that is achieved here is Isolation! Isolation of Hardware, so OS running in Virtual Machine gets a feel all hardware is available for its use. That is the way all OS are implemented! Exclusive access to hardware.\nBut above three methods are not that efficient with Hardware support for Virtualization. So, CPU also have extra core and support to run the Virtualization code efficently. More you can find on [here] (http://en.wikipedia.org/wiki/X86_virtualization)\nSince Isolation is the key to virtulization, Unix \u0026amp; Linux based OS have few features, which provides Isolation to process. Features like namespaces, cgroups and chroot. Utilzing these features, OS level virtualization can be achieved. In linux these are called Containers.\nI will be writing details of containers in my next blog. So stay tuned :)\n","date":"25 April 2015","externalUrl":null,"permalink":"/post/layman-guide-to-platform-virtualization/","section":"Posts","summary":"","title":"Layman guide to Platform Virtualization","type":"post"},{"content":"Started blogging today after few years of gap. Had stopped blogging few years back on wordpress .\nBlogging is always fun and great way to share your experiences and findings with others.\nLot of things have changed in last 3-5 years in world of Web Technology, So is the blogging platform too. While exploring Web Technologies and Golang, I came across the static site genrator and Hugo.\nI found it really powerful tool for blogging as well as Product Documentations.\nWhy not to continue with wordpress!\r#\rI work most of times on linux terminal and Hugo with github is seamlessly integrated with git workflow. Also, I have complete control over theme and customization. This learning is helpful to me for product documentation too :)\nGoing ahead I am hopeful, will write regularly and share intresting findings from work and side projects.\n","date":"4 March 2015","externalUrl":null,"permalink":"/post/moved-to-github/","section":"Posts","summary":"","title":"Moving to Hugo \u0026 github","type":"post"},{"content":"Hello! I‚Äôm Kunal Kushwaha, a technical architect with over 20 years of experience building system‚Äëlevel products and cloud‚Äënative solutions. My core passions lie at the intersection of AI and cloud technologies, where I design scalable architectures, leverage container ecosystems, and integrate machine learning workflows to solve complex challenges.\nWhat I Do\r#\rAI \u0026amp; Cloud Architecture I architect and deploy AI‚Äëdriven applications on public and private clouds, harnessing tools like Kubernetes, Docker, and serverless frameworks to deliver robust, high‚Äëavailability systems. Container \u0026amp; Infrastructure Engineering With 8+ years in container technologies (LXC, Docker, Kubernetes) and nearly 7 years in Linux kernel and embedded development, I streamline CI/CD pipelines, optimize resource utilization, and contribute to open‚Äësource runtimes such as Podman and Containerd. Developer Ecosystems \u0026amp; Community I love empowering teams through hands‚Äëon training, public speaking, and building vibrant communities. I‚Äôve organized meetups, delivered workshops on AI/ML integration, and written tutorials to help engineers adopt best practices in cloud‚Äënative development. Professional Journey\r#\rOver two decades, I‚Äôve:\nLed global R\u0026amp;D teams in designing RAID firmwares, storage drivers, and data‚Äëpipeline tools. Partnered with enterprise customers to tailor AI/ML solutions, deploy analytics platforms, and implement secure identity integrations. Contributed code and design reviews to major open‚Äësource projects, fostering collaboration and innovation. Outside Tech\r#\rWhen I‚Äôm not architecting cloud solutions or experimenting with new AI frameworks, I pursue photography and filmmaking. Capturing stories through my lens keeps my creativity sharp‚Äîand you can follow my visual journeys on Instagram.\nFeel free to explore my work on GitHub or connect on LinkedIn. I‚Äôm always eager to discuss AI innovations, cloud strategies, or creative media collaborations.\n","date":"3 March 2015","externalUrl":null,"permalink":"/about/","section":"Kunal Kushwaha","summary":"","title":"About Me","type":"page"},{"content":"Hello! I‚Äôm Kunal Kushwaha, a technical architect with over 20 years of experience in building system-level products and cloud-native solutions. My current focus lies in the realms of AI and cloud technologies, where I design scalable architectures, leverage container ecosystems, and integrate machine learning workflows to address complex challenges.\nWhat I Do\r#\rAI \u0026amp; Cloud Architecture I architect and deploy AI-driven applications on both public and private clouds, utilizing tools like Kubernetes, Docker, and serverless frameworks to deliver robust, high-availability systems.\nContainer \u0026amp; Infrastructure Engineering With over 8 years in container technologies (LXC, Docker, Kubernetes) and nearly 7 years in Linux kernel and embedded development, I streamline CI/CD pipelines, optimize resource utilization, and contribute to open-source runtimes such as Podman and Containerd.\nDeveloper Ecosystems \u0026amp; Community I am passionate about empowering teams through hands-on training, public speaking, and building vibrant communities. I\u0026rsquo;ve organized meetups and delivered workshops on Docker and Kubernetes, aiming to help engineers adopt best practices in cloud-native development.\nProfessional Journey\r#\rOver the past two decades, I‚Äôve:\nLed global R\u0026amp;D teams in designing RAID firmware, storage drivers, and data-pipeline tools.\nPartnered with enterprise customers to tailor AI/ML solutions, deploy analytics platforms, and implement secure identity integrations.\nContributed code and design reviews to major open-source projects, fostering collaboration and innovation.\nOutside Tech\r#\rWhen I‚Äôm not architecting cloud solutions or exploring new AI frameworks, I indulge in photography and filmmaking. Capturing stories through my lens keeps my creativity sharp‚Äîyou can follow my visual journeys on Instagram.\nFeel free to explore my work on GitHub or connect with me on LinkedIn. I‚Äôm always eager to discuss AI innovations, cloud strategies, or creative media collaborations.\n","date":"3 March 2015","externalUrl":null,"permalink":"/about/about/","section":"About Me","summary":"","title":"About Me","type":"about"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]